{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06078a8d-460d-47af-bd0e-7922bc86c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "\n",
      "Training on sequence length: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 280\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Update PPO agent after each episode\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Optional: Print progress every N episodes\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episodes \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[17], line 223\u001b[0m, in \u001b[0;36mPPOAgent.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Take gradient step with gradient clipping\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 223\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_network\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Wrapper to limit the number of steps per episode (Gymnasium API)\n",
    "class LimitEpisodeSteps(gym.Wrapper):\n",
    "    def __init__(self, env, max_steps):\n",
    "        super(LimitEpisodeSteps, self).__init__(env)\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.current_step = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.current_step += 1\n",
    "        done = terminated or truncated\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True  # Force done when max steps are reached\n",
    "            terminated = False  # Indicate truncation\n",
    "            truncated = True\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# Heuristic for determining the correct action based on pole angle\n",
    "def heuristic_action(state):\n",
    "    pole_angle = state[2]\n",
    "    return 1 if pole_angle > 0 else 0\n",
    "\n",
    "# Function to evaluate the model's accuracy and mean reward\n",
    "def evaluate_model(agent, eval_env, num_episodes=5):\n",
    "    total_rewards = []\n",
    "    correct_actions = 0\n",
    "    total_actions = 0\n",
    "    agent.policy_network.eval()\n",
    "    with torch.no_grad():\n",
    "        for episode in range(num_episodes):\n",
    "            obs, _ = eval_env.reset()\n",
    "            terminated, truncated = False, False\n",
    "            episode_reward = 0\n",
    "            while not (terminated or truncated):\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "                action_probs = agent.policy_network(obs_tensor)\n",
    "                action = torch.argmax(action_probs).item()\n",
    "                heuristic = heuristic_action(obs)\n",
    "                if action == heuristic:\n",
    "                    correct_actions += 1\n",
    "                total_actions += 1\n",
    "                obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "                episode_reward += reward\n",
    "            total_rewards.append(episode_reward)\n",
    "    mean_reward = np.mean(total_rewards)\n",
    "    accuracy = (correct_actions / total_actions) * 100 if total_actions > 0 else 0.0\n",
    "    agent.policy_network.train()\n",
    "    return mean_reward, accuracy\n",
    "\n",
    "# Function to create and wrap the environment with sequence length\n",
    "def create_env(env_id, sequence_length):\n",
    "    env = gym.make(env_id)\n",
    "    env = LimitEpisodeSteps(env, max_steps=sequence_length)\n",
    "    return env\n",
    "\n",
    "# Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.action_head = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        action_logits = self.action_head(x)\n",
    "        action_probs = F.softmax(action_logits, dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "# Value Network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=64):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        state_value = self.value_head(x)\n",
    "        return state_value\n",
    "\n",
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, K_epochs=4, eps_clip=0.2):\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.policy_network = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.value_network = ValueNetwork(state_dim).to(device)\n",
    "        self.optimizer = optim.Adam([\n",
    "            {'params': self.policy_network.parameters(), 'lr': lr},\n",
    "            {'params': self.value_network.parameters(), 'lr': lr}\n",
    "        ])\n",
    "\n",
    "        self.policy_network.train()\n",
    "        self.value_network.train()\n",
    "\n",
    "        # Memory\n",
    "        self.memory = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'log_probs': [],\n",
    "            'rewards': [],\n",
    "            'is_terminals': []\n",
    "        }\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        action_probs = self.policy_network(state)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action).item()\n",
    "\n",
    "    def store_transition(self, state, action, log_prob, reward, is_terminal):\n",
    "        self.memory['states'].append(state)\n",
    "        self.memory['actions'].append(action)\n",
    "        self.memory['log_probs'].append(log_prob)\n",
    "        self.memory['rewards'].append(reward)\n",
    "        self.memory['is_terminals'].append(is_terminal)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        for key in self.memory:\n",
    "            self.memory[key] = []\n",
    "\n",
    "    def compute_returns_and_advantages(self, next_value, dones):\n",
    "        rewards = self.memory['rewards']\n",
    "        returns = []\n",
    "        Gt = next_value\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            Gt = reward + self.gamma * Gt * (1 - done)\n",
    "            returns.insert(0, Gt)\n",
    "        # Normalize returns\n",
    "        returns = np.array(returns)\n",
    "        if returns.std() != 0:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "        else:\n",
    "            returns = returns - returns.mean()\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "        return returns\n",
    "\n",
    "    def update(self):\n",
    "        # Convert lists to tensors using numpy array for efficiency\n",
    "        if len(self.memory['states']) == 0:\n",
    "            print(\"No transitions to update.\")\n",
    "            return\n",
    "\n",
    "        states = torch.from_numpy(np.array(self.memory['states'], dtype=np.float32)).to(device)\n",
    "        actions = torch.tensor(self.memory['actions'], dtype=torch.long).to(device)\n",
    "        old_log_probs = torch.tensor(self.memory['log_probs'], dtype=torch.float32).to(device)\n",
    "        rewards = self.memory['rewards']\n",
    "        is_terminals = self.memory['is_terminals']\n",
    "\n",
    "        # Compute returns\n",
    "        with torch.no_grad():\n",
    "            if len(self.memory['states']) > 0:\n",
    "                last_state = torch.tensor(self.memory['states'][-1], dtype=torch.float32).to(device)\n",
    "                next_value = self.value_network(last_state).item()\n",
    "            else:\n",
    "                next_value = 0\n",
    "            returns = self.compute_returns_and_advantages(next_value, is_terminals)\n",
    "\n",
    "        # Compute advantages\n",
    "        values = self.value_network(states).squeeze(-1)  # [batch_size]\n",
    "        advantages = returns - values  # [batch_size]\n",
    "\n",
    "        # Ensure no NaNs in advantages\n",
    "        if torch.isnan(advantages).any():\n",
    "            print(\"NaNs detected in advantages. Skipping update.\")\n",
    "            self.clear_memory()\n",
    "            return\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Get action probabilities\n",
    "            action_probs = self.policy_network(states)  # [batch_size, action_dim]\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(actions)  # [batch_size]\n",
    "\n",
    "            # Ratio for clipping\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)  # [batch_size]\n",
    "\n",
    "            # Surrogate loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = F.mse_loss(values, returns)\n",
    "            loss = policy_loss + value_loss - 0.01 * entropy\n",
    "\n",
    "            # Check for NaNs in loss\n",
    "            if torch.isnan(loss):\n",
    "                print(\"NaNs detected in loss. Skipping update.\")\n",
    "                self.clear_memory()\n",
    "                return\n",
    "\n",
    "            # Take gradient step with gradient clipping\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=0.5)\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Clear memory\n",
    "        self.clear_memory()\n",
    "\n",
    "# CartPole environment setup\n",
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "# Training parameters\n",
    "initial_sequence_length = 1  # Start with 1 step\n",
    "max_sequence_length = 500  # Maximum sequence length to avoid infinite loop\n",
    "timesteps_per_sequence = 1000  # Timesteps to train per sequence length\n",
    "num_eval_episodes = 5  # Number of episodes to evaluate after each training\n",
    "accuracy_threshold = 95.0  # Percentage accuracy required to increase sequence length\n",
    "\n",
    "# Initialize environment\n",
    "env = create_env(env_id, initial_sequence_length)\n",
    "\n",
    "# Get state and action dimensions\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Initialize PPO Agent\n",
    "agent = PPOAgent(state_dim, action_dim)\n",
    "\n",
    "# Training loop with strict continual learning\n",
    "sequence_length = initial_sequence_length\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while sequence_length <= max_sequence_length:\n",
    "    print(f\"\\nTraining on sequence length: {sequence_length}\")\n",
    "    timesteps = 0\n",
    "    episodes = 0\n",
    "\n",
    "    while timesteps < timesteps_per_sequence:\n",
    "        obs, _ = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        episode_reward = 0\n",
    "        episodes += 1\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            action, log_prob = agent.select_action(obs)\n",
    "            heuristic = heuristic_action(obs)\n",
    "            reward = 1.0 if action == heuristic else 0.0  # Modify reward based on action correctness\n",
    "            obs, reward_env, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            agent.store_transition(obs, action, log_prob, reward, done)\n",
    "            episode_reward += reward\n",
    "            timesteps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update PPO agent after each episode\n",
    "        agent.update()\n",
    "\n",
    "        # Optional: Print progress every N episodes\n",
    "        if episodes % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  Episodes: {episodes}, Timesteps: {timesteps}, Elapsed Time: {elapsed:.2f}s\")\n",
    "\n",
    "    # Create evaluation environment\n",
    "    eval_env = create_env(env_id, sequence_length)\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    mean_reward, accuracy = evaluate_model(agent, eval_env, num_eval_episodes)\n",
    "\n",
    "    # Print the required metrics on a single line\n",
    "    print(f\"Seq Length: {sequence_length}, Mean Reward: {mean_reward:.2f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Check if accuracy threshold is met to increase sequence length\n",
    "    if accuracy >= accuracy_threshold:\n",
    "        sequence_length += 1\n",
    "        print(f\"Reward threshold achieved! Increasing sequence length to {sequence_length}\")\n",
    "\n",
    "        if sequence_length > max_sequence_length:\n",
    "            break\n",
    "\n",
    "        # Update the training environment with increased sequence length\n",
    "        env = create_env(env_id, sequence_length)\n",
    "    else:\n",
    "        print(\"Accuracy threshold not met, continuing training on the same sequence length.\")\n",
    "\n",
    "    # Clean up evaluation environment\n",
    "    eval_env.close()\n",
    "\n",
    "# Save the final model parameters\n",
    "torch.save({\n",
    "    'policy_network_state_dict': agent.policy_network.state_dict(),\n",
    "    'value_network_state_dict': agent.value_network.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "}, \"ppo_cartpole_cl.pth\")\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(\"ppo_cartpole_cl.pth\", map_location=device)\n",
    "agent.policy_network.load_state_dict(checkpoint['policy_network_state_dict'])\n",
    "agent.value_network.load_state_dict(checkpoint['value_network_state_dict'])\n",
    "agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "agent.policy_network.eval()\n",
    "agent.value_network.eval()\n",
    "\n",
    "# Test the model with printouts\n",
    "test_env = create_env(env_id, sequence_length)\n",
    "obs, _ = test_env.reset()\n",
    "test_episodes = 0\n",
    "\n",
    "print(\"\\nStarting Testing Phase...\\n\")\n",
    "\n",
    "for i in range(1000):\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        action_probs = agent.policy_network(obs_tensor)\n",
    "    action = torch.argmax(action_probs).item()\n",
    "    heuristic = heuristic_action(obs)\n",
    "    accuracy_step = \"Correct\" if action == heuristic else \"Incorrect\"\n",
    "    print(f\"Step: {i}, Seq Length: {sequence_length}, Model Action: {action}, Heuristic Action: {heuristic}, Accuracy: {accuracy_step}\")\n",
    "    obs, _, terminated, truncated, _ = test_env.step(action)\n",
    "    test_episodes += 1\n",
    "    if terminated or truncated:\n",
    "        obs, _ = test_env.reset()\n",
    "\n",
    "test_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96997f-8464-4ae8-b5b0-099d20eff4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

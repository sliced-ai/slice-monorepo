{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225e162-4e87-4b2f-b544-98fa1fac6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai sentence-transformers scikit-learn numpy matplotlib hdbscan umap-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddce106-3920-4209-ba65-c8a68ac8cc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825bdaf7-7f23-4002-a661-f02a353ef633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=1000, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=10, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "def extract_token_probs(data):\n",
    "    # Initialize an empty list to store the token probabilities\n",
    "    token_probs_list = []\n",
    "    \n",
    "    # Iterate over each entry in the data list\n",
    "    for entry in data:\n",
    "        # Check if 'token_probs' key exists in the dictionary\n",
    "        if 'token_probs' in entry:\n",
    "            # Append the numpy array of token probabilities to the list\n",
    "            token_probs_list.append(entry['token_probs'])\n",
    "    \n",
    "    # Return the list of token probabilities\n",
    "    return token_probs_list\n",
    "    \n",
    "def pad_token_probs(probabilities, pad_value=0):\n",
    "    # Find the maximum length of the token probabilities list\n",
    "    max_length = max(len(probs) for probs in probabilities)\n",
    "    # Pad each probabilities list to the maximum length\n",
    "    padded_probabilities = np.array([np.pad(probs, (0, max_length - len(probs)), 'constant', constant_values=pad_value) for probs in probabilities])\n",
    "    return padded_probabilities\n",
    "\n",
    "\n",
    "prompt = \"I want a script that generates an analysis of some text data. Pull the text data from any source of text it doesn't matter what.\"\n",
    "n = 20\n",
    "responses, log_probs = get_responses(prompt, n)\n",
    "embeddings = get_embeddings(responses)\n",
    "\n",
    "probability_map = analyze_log_probs(log_probs)\n",
    "token_probs_list = extract_token_probs(probability_map)\n",
    "\n",
    "token_probabilities = pad_token_probs([entry['token_probs'] for entry in probability_map])\n",
    "\n",
    "# Combine embeddings and token probabilities\n",
    "combined_input = np.hstack((embeddings, token_probabilities))\n",
    "\n",
    "grid_size = 30\n",
    "latent_dim = grid_size * grid_size\n",
    "encoder = train_autoencoder(combined_input, latent_dim)\n",
    "\n",
    "encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093deb9e-9491-4c15-8d48-2ae831e7a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad the token probabilities to the same length\n",
    "def pad_token_probs(probabilities, pad_value=0):\n",
    "    # Find the maximum length of the token probabilities list\n",
    "    max_length = max(len(probs) for probs in probabilities)\n",
    "    # Pad each probabilities list to the maximum length\n",
    "    padded_probabilities = np.array([np.pad(probs, (0, max_length - len(probs)), 'constant', constant_values=pad_value) for probs in probabilities])\n",
    "    return padded_probabilities\n",
    "\n",
    "# Usage in your main workflow\n",
    "token_probabilities = pad_token_probs([entry['token_probs'] for entry in probability_map])\n",
    "\n",
    "combined_input = np.hstack((embeddings, token_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b3087-e335-4c31-8041-8d62202260bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=50, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=5) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "grid_size = 100\n",
    "latent_dim = grid_size * grid_size\n",
    "encoder = train_autoencoder(combined_input, latent_dim)\n",
    "encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a79e1-10e2-4efb-ba53-bf96679356ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Set output_hidden_states to True\n",
    "outputs = model(**batch_dict, output_hidden_states=True)\n",
    "# Use the new pooling method\n",
    "embeddings = multi_layer_pool(outputs.hidden_states, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Convert the embeddings to lists\n",
    "embedding_lists = embeddings.cpu().tolist()\n",
    "for i, embedding in enumerate(embedding_lists):\n",
    "    print(f'Embedding for input {i+1}: {embedding}')\n",
    "\n",
    "# To print the similarity scores\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(\"Similarity scores:\", scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba15aee8-d7c8-46c5-9f7c-86d749800f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b09c93b311c40d3ae7d1bb5e64aff6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "# Each query must come with a one-sentence instruction that describes the task\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "queries = [\n",
    "    get_detailed_instruct(task, 'how much protein should a female eat'),\n",
    "    get_detailed_instruct(task, 'summit define')\n",
    "]\n",
    "# No need to add instruction for retrieval documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n",
    "]\n",
    "input_texts = queries + documents\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n",
    "model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9adc899c-6a9f-41c6-88ff-cc60defcee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "[0.017814435064792633, 0.0037355837412178516, -0.007492876145988703, -0.00359846162609756, -0.009139152243733406, 0.008220024406909943, -0.008680135011672974, 0.01465494092553854, 0.02927706204354763, -0.009534928947687149, 0.019864309579133987, -0.014393788762390614, 0.028043238446116447, 0.015878915786743164, 0.005144068971276283, -0.0019143394893035293, 0.01968141458928585, 0.006150744389742613, -0.010869795456528664, -0.02459058165550232, -0.01935139112174511, -0.014834835194051266, 0.009098413400352001, 0.01934366673231125, -0.007110074628144503, -0.0024360318202525377, -0.01654447615146637, -0.010716388002038002, 0.0037316083908081055, 0.013736183755099773, 0.009002281352877617, -0.0011430300073698163, 0.009495102800428867, -0.0011326521635055542, -0.0034316235687583685, -0.011325624771416187, 0.005067669786512852, 0.00690497737377882, 0.008022170513868332, 0.0028627472929656506, -0.014525653794407845, 0.016094934195280075, -0.004652791190892458, 0.0005038662347942591, 0.004865825641900301, 0.010460663586854935, 0.0013061511563137174, 0.01060456596314907, 0.0019133695168420672, -0.015911662951111794, 1.4035054846317507e-05, -0.001488224253989756, -0.0012800900731235743, 0.016473881900310516, -0.022771628573536873, -0.007801698055118322, 0.018205540254712105, -0.02099478244781494, 0.007866578176617622, -0.015490081161260605, 0.010507802478969097, -0.013188485987484455, -0.012109971605241299, -0.02077138051390648, 0.02133808098733425, 0.008033553138375282, -0.0005843761609867215, -0.030907191336154938, -0.010611395351588726, -0.010196048766374588, -0.010333742946386337, -0.017692044377326965, -0.02403143048286438, 0.0018399059772491455, 0.016263479366898537, -0.03277909755706787, 0.0014518897514790297, 0.0061210086569190025, -0.007758164778351784, 0.006943443324416876, 0.00013737643894273788, -0.017876315861940384, 0.0024640345945954323, -0.000529667770024389, -0.009393390268087387, 0.027689466252923012, -0.007033866830170155, -0.008342870511114597, 0.0059315781109035015, -0.006199324503540993, 0.0002682809717953205, -0.006562728900462389, -0.006006902549415827, 0.024266354739665985, -0.003987669479101896, -0.004926803521811962, -0.0174579918384552, -0.001621393021196127, 0.008868023753166199, -0.01734757050871849]\n",
      "Similarity scores: [[82.91368103027344, 47.97158432006836], [46.95111083984375, 81.74347686767578]]\n"
     ]
    }
   ],
   "source": [
    "def multi_layer_pool(hidden_states: list, attention_mask: Tensor, num_layers: int = 1) -> Tensor:\n",
    "    \"\"\"\n",
    "    Extract embeddings from the last 'num_layers' layers and concatenate them.\n",
    "    \"\"\"\n",
    "    # Get the last num_layers layers\n",
    "    all_layers = hidden_states[-num_layers:]  \n",
    "    # Concatenate them on the embedding dimension\n",
    "    concatenated_layers = torch.cat(all_layers, dim=-1)  \n",
    "\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return concatenated_layers[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = concatenated_layers.shape[0]\n",
    "        return concatenated_layers[torch.arange(batch_size, device=concatenated_layers.device), sequence_lengths]\n",
    "\n",
    "max_length = 4096\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Set output_hidden_states to True\n",
    "outputs = model(**batch_dict, output_hidden_states=True)\n",
    "# Use the new pooling method\n",
    "embeddings = multi_layer_pool(outputs.hidden_states, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Convert the embeddings to lists\n",
    "embedding_lists = embeddings.cpu().tolist()\n",
    "\n",
    "print(len(embedding_lists[0]))\n",
    "print(embedding_lists[0][0:100])\n",
    "\n",
    "# To print the similarity scores\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(\"Similarity scores:\", scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7939c53-084c-4ccb-a858-ca34c4a0b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "def get_full_response(prompt, n=1, max_tokens=1000, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  # Replace with your model ID as needed\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True,  # Enable logprobs\n",
    "            top_logprobs=10  # Specify number of top log probabilities to return\n",
    "        )\n",
    "\n",
    "        # Print the entire response object\n",
    "        return response\n",
    "\n",
    "# Example usage\n",
    "response = get_full_response(\"Tell me a joke\", n=1)\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_response_info(response):\n",
    "    # Extract the conversation response\n",
    "    response_match = re.search(r'content=\"(.*?)\"', str(response), re.DOTALL)\n",
    "    response_text = response_match.group(1) if response_match else \"\"\n",
    "\n",
    "    # Extract the logprobs of the response\n",
    "    logprobs_match = re.findall(r'logprob=(-?\\d+\\.\\d+)', str(response))\n",
    "    logprobs = [float(logprob) for logprob in logprobs_match]\n",
    "\n",
    "    return response_text, logprobs\n",
    "    \n",
    "reply, logprobs= extract_response_info(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c33261ab-5276-4b73-944a-e9e19416773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n",
      "ChatCompletion(id='chatcmpl-9PwwgS4fzojyGkpz75tBbDTOda3sO', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='Why', bytes=[87, 104, 121], logprob=-0.12435661, top_logprobs=[TopLogprob(token='Why', bytes=[87, 104, 121], logprob=-0.12435661), TopLogprob(token='Sure', bytes=[83, 117, 114, 101], logprob=-2.3840039), TopLogprob(token='What', bytes=[87, 104, 97, 116], logprob=-4.0270743), TopLogprob(token='How', bytes=[72, 111, 119], logprob=-5.474174), TopLogprob(token='Did', bytes=[68, 105, 100], logprob=-6.390657), TopLogprob(token='I', bytes=[73], logprob=-7.5147605), TopLogprob(token='Certainly', bytes=[67, 101, 114, 116, 97, 105, 110, 108, 121], logprob=-9.238273), TopLogprob(token='Here', bytes=[72, 101, 114, 101], logprob=-9.42575), TopLogprob(token='Of', bytes=[79, 102], logprob=-9.471295), TopLogprob(token='Absolutely', bytes=[65, 98, 115, 111, 108, 117, 116, 101, 108, 121], logprob=-9.5682535)]), ChatCompletionTokenLogprob(token=' don', bytes=[32, 100, 111, 110], logprob=-0.1671742, top_logprobs=[TopLogprob(token=' don', bytes=[32, 100, 111, 110], logprob=-0.1671742), TopLogprob(token=' did', bytes=[32, 100, 105, 100], logprob=-2.2936466), TopLogprob(token=' couldn', bytes=[32, 99, 111, 117, 108, 100, 110], logprob=-3.585204), TopLogprob(token=' can', bytes=[32, 99, 97, 110], logprob=-4.172778), TopLogprob(token=' do', bytes=[32, 100, 111], logprob=-5.206788), TopLogprob(token=' was', bytes=[32, 119, 97, 115], logprob=-5.7459726), TopLogprob(token=' didn', bytes=[32, 100, 105, 100, 110], logprob=-6.847909), TopLogprob(token=' shouldn', bytes=[32, 115, 104, 111, 117, 108, 100, 110], logprob=-9.309106), TopLogprob(token=' are', bytes=[32, 97, 114, 101], logprob=-10.2750635), TopLogprob(token=' should', bytes=[32, 115, 104, 111, 117, 108, 100], logprob=-10.911479)]), ChatCompletionTokenLogprob(token=\"'t\", bytes=[39, 116], logprob=-0.020108163, top_logprobs=[TopLogprob(token=\"'t\", bytes=[39, 116], logprob=-0.020108163), TopLogprob(token='’t', bytes=[226, 128, 153, 116], logprob=-3.9168482), TopLogprob(token=\"'\", bytes=[39], logprob=-13.076961), TopLogprob(token='‘', bytes=[226, 128, 152], logprob=-14.646047), TopLogprob(token='\"', bytes=[34], logprob=-15.704122), TopLogprob(token=\"'s\", bytes=[39, 115], logprob=-15.988806), TopLogprob(token='´t', bytes=[194, 180, 116], logprob=-16.201094), TopLogprob(token='\\\\xca', bytes=[202], logprob=-16.220417), TopLogprob(token='’', bytes=[226, 128, 153], logprob=-16.62414), TopLogprob(token=\"'y\", bytes=[39, 121], logprob=-16.722715)]), ChatCompletionTokenLogprob(token=' skeletons', bytes=[32, 115, 107, 101, 108, 101, 116, 111, 110, 115], logprob=-0.063186556, top_logprobs=[TopLogprob(token=' skeletons', bytes=[32, 115, 107, 101, 108, 101, 116, 111, 110, 115], logprob=-0.063186556), TopLogprob(token=' scientists', bytes=[32, 115, 99, 105, 101, 110, 116, 105, 115, 116, 115], logprob=-3.2634964), TopLogprob(token=' eggs', bytes=[32, 101, 103, 103, 115], logprob=-3.9045181), TopLogprob(token=' o', bytes=[32, 111], logprob=-6.0089645), TopLogprob(token=' some', bytes=[32, 115, 111, 109, 101], logprob=-8.179813), TopLogprob(token=' skeleton', bytes=[32, 115, 107, 101, 108, 101, 116, 111, 110], logprob=-10.555836), TopLogprob(token=' science', bytes=[32, 115, 99, 105, 101, 110, 99, 101], logprob=-11.280941), TopLogprob(token=' dinosaurs', bytes=[32, 100, 105, 110, 111, 115, 97, 117, 114, 115], logprob=-11.413376), TopLogprob(token=' se', bytes=[32, 115, 101], logprob=-11.573328), TopLogprob(token=' scientist', bytes=[32, 115, 99, 105, 101, 110, 116, 105, 115, 116], logprob=-11.896111)]), ChatCompletionTokenLogprob(token=' fight', bytes=[32, 102, 105, 103, 104, 116], logprob=-5.9795446e-05, top_logprobs=[TopLogprob(token=' fight', bytes=[32, 102, 105, 103, 104, 116], logprob=-5.9795446e-05), TopLogprob(token=' ever', bytes=[32, 101, 118, 101, 114], logprob=-9.7516775), TopLogprob(token=' watch', bytes=[32, 119, 97, 116, 99, 104], logprob=-14.158694), TopLogprob(token=' play', bytes=[32, 112, 108, 97, 121], logprob=-14.480372), TopLogprob(token=' go', bytes=[32, 103, 111], logprob=-15.336719), TopLogprob(token=' use', bytes=[32, 117, 115, 101], logprob=-17.09947), TopLogprob(token=' ride', bytes=[32, 114, 105, 100, 101], logprob=-18.018038), TopLogprob(token=' like', bytes=[32, 108, 105, 107, 101], logprob=-18.455118), TopLogprob(token=' **', bytes=[32, 42, 42], logprob=-18.678438), TopLogprob(token='fight', bytes=[102, 105, 103, 104, 116], logprob=-18.973143)]), ChatCompletionTokenLogprob(token=' each', bytes=[32, 101, 97, 99, 104], logprob=0.0, top_logprobs=[TopLogprob(token=' each', bytes=[32, 101, 97, 99, 104], logprob=0.0), TopLogprob(token='?', bytes=[63], logprob=-17.178476), TopLogprob(token='?\\n\\n', bytes=[63, 10, 10], logprob=-17.321285), TopLogprob(token=' other', bytes=[32, 111, 116, 104, 101, 114], logprob=-19.32624), TopLogprob(token=' one', bytes=[32, 111, 110, 101], logprob=-19.377651), TopLogprob(token=' with', bytes=[32, 119, 105, 116, 104], logprob=-19.507286), TopLogprob(token='?\\n', bytes=[63, 10], logprob=-19.571552), TopLogprob(token=' in', bytes=[32, 105, 110], logprob=-19.851938), TopLogprob(token='each', bytes=[101, 97, 99, 104], logprob=-22.35489), TopLogprob(token=' ', bytes=[32], logprob=-22.515017)]), ChatCompletionTokenLogprob(token=' other', bytes=[32, 111, 116, 104, 101, 114], logprob=0.0, top_logprobs=[TopLogprob(token=' other', bytes=[32, 111, 116, 104, 101, 114], logprob=0.0), TopLogprob(token='other', bytes=[111, 116, 104, 101, 114], logprob=-17.77557), TopLogprob(token=' another', bytes=[32, 97, 110, 111, 116, 104, 101, 114], logprob=-17.92706), TopLogprob(token=' o', bytes=[32, 111], logprob=-19.17227), TopLogprob(token=' others', bytes=[32, 111, 116, 104, 101, 114, 115], logprob=-19.481651), TopLogprob(token=' ot', bytes=[32, 111, 116], logprob=-19.590126), TopLogprob(token=' ', bytes=[32], logprob=-20.046608), TopLogprob(token='-other', bytes=[45, 111, 116, 104, 101, 114], logprob=-20.145893), TopLogprob(token=' *', bytes=[32, 42], logprob=-22.026167), TopLogprob(token=' Other', bytes=[32, 79, 116, 104, 101, 114], logprob=-22.399275)]), ChatCompletionTokenLogprob(token='?', bytes=[63], logprob=-1.0460736, top_logprobs=[TopLogprob(token='?\\n\\n', bytes=[63, 10, 10], logprob=-0.43515745), TopLogprob(token='?', bytes=[63], logprob=-1.0460736), TopLogprob(token='?\\n', bytes=[63, 10], logprob=-6.4882045), TopLogprob(token='?\\n\\n\\n', bytes=[63, 10, 10, 10], logprob=-13.689981), TopLogprob(token='?\\n\\n\\n\\n', bytes=[63, 10, 10, 10, 10], logprob=-15.3081665), TopLogprob(token='?\\\\', bytes=[63, 92], logprob=-17.13432), TopLogprob(token=' in', bytes=[32, 105, 110], logprob=-17.70962), TopLogprob(token='?<', bytes=[63, 60], logprob=-18.174059), TopLogprob(token=' at', bytes=[32, 97, 116], logprob=-18.598429), TopLogprob(token='?\\r\\n', bytes=[63, 13, 10], logprob=-18.680979)]), ChatCompletionTokenLogprob(token=' They', bytes=[32, 84, 104, 101, 121], logprob=-0.12531306, top_logprobs=[TopLogprob(token=' They', bytes=[32, 84, 104, 101, 121], logprob=-0.12531306), TopLogprob(token=' \\n\\n', bytes=[32, 10, 10], logprob=-2.156727), TopLogprob(token=' \\n', bytes=[32, 10], logprob=-6.8250165), TopLogprob(token=' Because', bytes=[32, 66, 101, 99, 97, 117, 115, 101], logprob=-7.120873), TopLogprob(token='  \\n', bytes=[32, 32, 10], logprob=-9.2667675), TopLogprob(token='  \\n\\n', bytes=[32, 32, 10, 10], logprob=-9.571899), TopLogprob(token=' ', bytes=[32], logprob=-11.582386), TopLogprob(token='   \\n', bytes=[32, 32, 32, 10], logprob=-12.870388), TopLogprob(token='   \\n\\n', bytes=[32, 32, 32, 10, 10], logprob=-13.326384), TopLogprob(token=' \\n  \\n', bytes=[32, 10, 32, 32, 10], logprob=-13.8826275)]), ChatCompletionTokenLogprob(token=' don', bytes=[32, 100, 111, 110], logprob=-0.00020795093, top_logprobs=[TopLogprob(token=' don', bytes=[32, 100, 111, 110], logprob=-0.00020795093), TopLogprob(token=' just', bytes=[32, 106, 117, 115, 116], logprob=-8.481646), TopLogprob(token=' simply', bytes=[32, 115, 105, 109, 112, 108, 121], logprob=-14.636984), TopLogprob(token=' *', bytes=[32, 42], logprob=-16.545908), TopLogprob(token=' haven', bytes=[32, 104, 97, 118, 101, 110], logprob=-18.618372), TopLogprob(token=' **', bytes=[32, 42, 42], logprob=-18.619328), TopLogprob(token=\"'d\", bytes=[39, 100], logprob=-18.727057), TopLogprob(token=' \\u200b', bytes=[32, 226, 128, 139], logprob=-19.538984), TopLogprob(token=' wouldn', bytes=[32, 119, 111, 117, 108, 100, 110], logprob=-19.888308), TopLogprob(token=' ', bytes=[32], logprob=-19.929272)]), ChatCompletionTokenLogprob(token=\"'t\", bytes=[39, 116], logprob=-0.021902354, top_logprobs=[TopLogprob(token=\"'t\", bytes=[39, 116], logprob=-0.021902354), TopLogprob(token='’t', bytes=[226, 128, 153, 116], logprob=-3.8320906), TopLogprob(token='‘', bytes=[226, 128, 152], logprob=-18.505167), TopLogprob(token=\"'\", bytes=[39], logprob=-18.609701), TopLogprob(token='`t', bytes=[96, 116], logprob=-19.70302), TopLogprob(token='\\\\xca', bytes=[202], logprob=-19.784256), TopLogprob(token='´t', bytes=[194, 180, 116], logprob=-20.518385), TopLogprob(token='�t', bytes=[239, 191, 189, 116], logprob=-20.612457), TopLogprob(token=';t', bytes=[59, 116], logprob=-21.146574), TopLogprob(token=\"''\", bytes=[39, 39], logprob=-21.224083)]), ChatCompletionTokenLogprob(token=' have', bytes=[32, 104, 97, 118, 101], logprob=-3.1281633e-07, top_logprobs=[TopLogprob(token=' have', bytes=[32, 104, 97, 118, 101], logprob=-3.1281633e-07), TopLogprob(token=' want', bytes=[32, 119, 97, 110, 116], logprob=-15.625641), TopLogprob(token=' *', bytes=[32, 42], logprob=-16.59742), TopLogprob(token=' ', bytes=[32], logprob=-18.412844), TopLogprob(token='\\u200b', bytes=[226, 128, 139], logprob=-18.950409), TopLogprob(token=' **', bytes=[32, 42, 42], logprob=-19.15398), TopLogprob(token=\"'\", bytes=[39], logprob=-19.654428), TopLogprob(token=' hav', bytes=[32, 104, 97, 118], logprob=-20.05206), TopLogprob(token=' got', bytes=[32, 103, 111, 116], logprob=-20.11689), TopLogprob(token=' like', bytes=[32, 108, 105, 107, 101], logprob=-20.187271)]), ChatCompletionTokenLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=0.0, top_logprobs=[TopLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=0.0), TopLogprob(token=' ', bytes=[32], logprob=-18.848112), TopLogprob(token=' enough', bytes=[32, 101, 110, 111, 117, 103, 104], logprob=-18.937794), TopLogprob(token=' any', bytes=[32, 97, 110, 121], logprob=-18.955513), TopLogprob(token=' *', bytes=[32, 42], logprob=-19.205376), TopLogprob(token=' to', bytes=[32, 116, 111], logprob=-19.480103), TopLogprob(token=' a', bytes=[32, 97], logprob=-19.596024), TopLogprob(token=' guts', bytes=[32, 103, 117, 116, 115], logprob=-19.719492), TopLogprob(token=' **', bytes=[32, 42, 42], logprob=-20.569735), TopLogprob(token=' th', bytes=[32, 116, 104], logprob=-20.921513)]), ChatCompletionTokenLogprob(token=' guts', bytes=[32, 103, 117, 116, 115], logprob=0.0, top_logprobs=[TopLogprob(token=' guts', bytes=[32, 103, 117, 116, 115], logprob=0.0), TopLogprob(token=' stomach', bytes=[32, 115, 116, 111, 109, 97, 99, 104], logprob=-16.956444), TopLogprob(token=' gut', bytes=[32, 103, 117, 116], logprob=-20.161278), TopLogprob(token=' *', bytes=[32, 42], logprob=-22.291851), TopLogprob(token=' heart', bytes=[32, 104, 101, 97, 114, 116], logprob=-23.531113), TopLogprob(token=' \"', bytes=[32, 34], logprob=-25.78061), TopLogprob(token=' **', bytes=[32, 42, 42], logprob=-25.933327), TopLogprob(token=' _', bytes=[32, 95], logprob=-26.183369), TopLogprob(token=\" '\", bytes=[32, 39], logprob=-27.524174), TopLogprob(token=' G', bytes=[32, 71], logprob=-28.25705)]), ChatCompletionTokenLogprob(token='.', bytes=[46], logprob=-0.26414776, top_logprobs=[TopLogprob(token='.', bytes=[46], logprob=-0.26414776), TopLogprob(token='!', bytes=[33], logprob=-1.4610777), TopLogprob(token='!\\n', bytes=[33, 10], logprob=-9.265269), TopLogprob(token='.\\n', bytes=[46, 10], logprob=-9.756629), TopLogprob(token='<|end|>', bytes=None, logprob=-14.137578), TopLogprob(token=' to', bytes=[32, 116, 111], logprob=-15.326579), TopLogprob(token='.\\n\\n', bytes=[46, 10, 10], logprob=-16.09179), TopLogprob(token=' for', bytes=[32, 102, 111, 114], logprob=-16.727707), TopLogprob(token='!\\n\\n', bytes=[33, 10, 10], logprob=-16.734253), TopLogprob(token='!\"', bytes=[33, 34], logprob=-18.073544)])]), message=ChatCompletionMessage(content=\"Why don't skeletons fight each other? They don't have the guts.\", role='assistant', function_call=None, tool_calls=None))], created=1715971814, model='gpt-4-turbo-2024-04-09', object='chat.completion', system_fingerprint='fp_294de9593d', usage=CompletionUsage(completion_tokens=15, prompt_tokens=21, total_tokens=36))\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8eda05-7d2d-45ae-a5a8-3c04c69eb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91f16e-7de6-4886-b912-f06499a700db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "        token_probs = softmax(token_logprobs)\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs\n",
    "        })\n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "    autoencoder.fit(input_data, input_data, epochs=20, batch_size=8, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 20\n",
    "    responses, log_probs = get_responses(prompt, n)\n",
    "    embeddings = get_embeddings(responses)\n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    token_probs_list = np.array([entry['token_probs'] for entry in probability_map])\n",
    "    combined_input = np.hstack((embeddings, token_probs_list))\n",
    "    grid_size = 30\n",
    "    latent_dim = grid_size * grid_size\n",
    "    encoder = train_autoencoder(combined_input, latent_dim)\n",
    "    encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "    visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13554d8c-06f4-4b9a-80ba-c5ed5ca5dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Prepare the Text\n",
    "sample_text = \"Natural language processing (NLP) involves the interaction between computers and humans through language. It enables machines to read, understand, and derive meaning from human languages.\"\n",
    "\n",
    "# Step 2: Vectorize the Text using Universal Sentence Encoder from TensorFlow Hub\n",
    "# Load Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "# Convert the sample text into an embedding\n",
    "text_embedding = embedding_layer([sample_text])\n",
    "print(\"Original Embedding Shape:\", text_embedding.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder Model to reduce dimensionality to a 2D array\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embedding.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(32, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(2 * 2, activation='linear')(encoded)  # 2x2 array output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(2 * 2,))\n",
    "decoded = tf.keras.layers.Dense(32, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(64, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embedding.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "# Here, we use the text embedding itself as the target (autoencoder learns to reconstruct its input)\n",
    "autoencoder.fit(text_embedding, text_embedding, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "# Predict the 2D coordinates\n",
    "encoded_2d_array = encoder(text_embedding)\n",
    "print(\"2D Array for the Input Text:\", encoded_2d_array.numpy().reshape(2, 2))\n",
    "\n",
    "# Step 4: Visualize the 2D Array Result\n",
    "encoded_2d_array_reshaped = encoded_2d_array.numpy().reshape(2, 2)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(encoded_2d_array_reshaped, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('2D Array Embedding of the Text')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e2dcd-9cc3-4b69-b51c-a06be4fcacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(encoded_2d_grid.shape[0]):\n",
    "    Z = encoded_2d_grid[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "\n",
    "ax.set_title('3D Visualization of Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca1aa2-814f-4937-b562-8baab5e7c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc7ccd-fb40-4d5b-9c3e-997014f238b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dropout(0.5)(encoded)  # Adding dropout\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56558db2-81c1-485f-8b58-e3dcbcd7616f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33c2f9-0276-42d9-b09d-47817c28d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:10]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 10  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=5) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557eeaf-cf81-4563-bf13-82b9e9eded62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai sentence-transformers scikit-learn numpy matplotlib hdbscan umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb824ca-eb85-425a-b975-0a4686dfe97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to cluster embeddings (HDBSCAN or K-Means)\n",
    "def cluster_embeddings(embeddings, method='hdbscan'):\n",
    "    if method == 'hdbscan':\n",
    "        clusterer = HDBSCAN(min_cluster_size=15, cluster_selection_method='leaf')\n",
    "        cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    else:\n",
    "        clusterer = KMeans(n_clusters=5)\n",
    "        cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    return cluster_labels\n",
    "\n",
    "# Function to visualize embeddings using UMAP\n",
    "def visualize_embeddings(embeddings, labels):\n",
    "    n_neighbors = min(15, len(embeddings) - 1)\n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=n_neighbors)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='Spectral')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def plot_raw_logits(probability_map):\n",
    "    for entry in probability_map:\n",
    "        tokens = entry['tokens']\n",
    "        log_probs = entry['token_logprobs']  # Assuming this key will be added to the dictionary\n",
    "        \n",
    "        # Ensure we have valid data\n",
    "        if tokens and log_probs and len(tokens) == len(log_probs):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(tokens, log_probs)\n",
    "            plt.xlabel('Tokens')\n",
    "            plt.ylabel('Log Probabilities')\n",
    "            plt.title('Raw Log Probabilities of Tokens')\n",
    "            plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "def plot_probabilities(probability_map):\n",
    "    for entry in probability_map:\n",
    "        tokens = entry['tokens']\n",
    "        token_probs = entry['token_probs']\n",
    "        \n",
    "        # Ensure we have valid data\n",
    "        if tokens and token_probs is not None and len(tokens) == len(token_probs):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(tokens, token_probs)\n",
    "            plt.xlabel('Tokens')\n",
    "            plt.ylabel('Probabilities')\n",
    "            plt.title('Token Probabilities')\n",
    "            plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 10\n",
    "    \n",
    "    responses, log_probs = get_responses(\n",
    "    prompt=prompt,\n",
    "    n=5,\n",
    "    max_tokens=100,\n",
    "    temperature=2,\n",
    "    top_p=0.3,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.3\n",
    "    )\n",
    "\n",
    "    embeddings = get_embeddings(responses)\n",
    "    \n",
    "    # Choose clustering method ('hdbscan' or 'kmeans')\n",
    "    clustering_method = 'hdbscan'\n",
    "    cluster_labels = cluster_embeddings(embeddings, method=clustering_method)\n",
    "    \n",
    "    #print(\"Clustering Results:\")\n",
    "    #for i, response in enumerate(responses):\n",
    "        #print(f\"Cluster {cluster_labels[i]}: {response}\")\n",
    "    \n",
    "    visualize_embeddings(embeddings, cluster_labels)\n",
    "\n",
    "    print(log_probs)\n",
    "    \n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    print(probability_map)\n",
    "    plot_probabilities(probability_map)\n",
    "    plot_raw_logits(probability_map)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22516b0f-490e-430d-bbbe-280df6483956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=10, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "def extract_token_probs(data):\n",
    "    # Initialize an empty list to store the token probabilities\n",
    "    token_probs_list = []\n",
    "    \n",
    "    # Iterate over each entry in the data list\n",
    "    for entry in data:\n",
    "        # Check if 'token_probs' key exists in the dictionary\n",
    "        if 'token_probs' in entry:\n",
    "            # Append the numpy array of token probabilities to the list\n",
    "            token_probs_list.append(entry['token_probs'])\n",
    "    \n",
    "    # Return the list of token probabilities\n",
    "    return token_probs_list\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 1\n",
    "    responses, log_probs = get_responses(prompt, n)\n",
    "    embeddings = get_embeddings(responses)\n",
    "\n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    token_probs_list = extract_token_probs(probability_map)\n",
    "    \n",
    "    # Extract token probabilities from the probability map\n",
    "    token_probabilities = np.array([entry['token_probs'] for entry in probability_map])\n",
    "    \n",
    "    # Combine embeddings and token probabilities\n",
    "    combined_input = np.hstack((embeddings, token_probabilities))\n",
    "\n",
    "    grid_size = 30\n",
    "    latent_dim = grid_size * grid_size\n",
    "    encoder = train_autoencoder(combined_input, latent_dim)\n",
    "\n",
    "    encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "    visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d7942-5a5f-40bf-b0e3-9f97fc97bf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

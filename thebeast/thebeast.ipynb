{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225e162-4e87-4b2f-b544-98fa1fac6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai sentence-transformers scikit-learn numpy matplotlib hdbscan umap-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e967da9-c765-4a32-a64d-6a612b376735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded. File ID: file-9jyr72r96jaxWQCY1TN3vGNU\n",
      "Batch job created. Batch ID: batch_RreSlmceawjRlFaBLSP6Bv5K\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     Repeated BATCH     ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'\n",
    "\n",
    "def upload_file(file_path, purpose):\n",
    "    url = \"https://api.openai.com/v1/files\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    }\n",
    "    files = {\n",
    "        'file': open(file_path, 'rb'),\n",
    "        'purpose': (None, purpose),\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "    return response.json()\n",
    "\n",
    "def create_batch(input_file_id, endpoint, completion_window):\n",
    "    url = \"https://api.openai.com/v1/batches\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    data = {\n",
    "        \"input_file_id\": input_file_id,\n",
    "        \"endpoint\": endpoint,\n",
    "        \"completion_window\": completion_window,\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "def create_jsonl_file(prompt, n, file_path):\n",
    "    data = [{\n",
    "        \"custom_id\": f\"request-{i+1}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "    } for i in range(n)]\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def main():\n",
    "    prompt = \"I want a script that generates an analysis of some text data. Pull the text data from any source of text it doesn't matter what. Get the text data from a source you know of.\"\n",
    "    n = 100  # number of jobs\n",
    "    file_path = \"batch_test.jsonl\"\n",
    "    endpoint = \"/v1/chat/completions\"\n",
    "    completion_window = \"24h\"\n",
    "\n",
    "    # Step 1: Create the file\n",
    "    create_jsonl_file(prompt, n, file_path)\n",
    "    \n",
    "    # Step 2: Upload the file\n",
    "    upload_response = upload_file(file_path, \"batch\")\n",
    "    input_file_id = upload_response['id']\n",
    "    print(f\"File uploaded. File ID: {input_file_id}\")\n",
    "\n",
    "    # Step 3: Create the batch job\n",
    "    batch_response = create_batch(input_file_id, endpoint, completion_window)\n",
    "    batch_id = batch_response['id']\n",
    "    print(f\"Batch job created. Batch ID: {batch_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86630db2-c700-48e5-b043-ca86574c80e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded. File ID: file-Px7HiajPuZtpdeFUXCpDcYaL\n",
      "Batch job created. Batch ID: batch_TYryq4oxmpN6MxoOtMQZRVLO\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     HP Sweep BATCH     ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'\n",
    "\n",
    "def upload_file(file_path, purpose):\n",
    "    url = \"https://api.openai.com/v1/files\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    }\n",
    "    files = {\n",
    "        'file': open(file_path, 'rb'),\n",
    "        'purpose': (None, purpose),\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "    return response.json()\n",
    "\n",
    "def create_batch(input_file_id, endpoint, completion_window):\n",
    "    url = \"https://api.openai.com/v1/batches\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    data = {\n",
    "        \"input_file_id\": input_file_id,\n",
    "        \"endpoint\": endpoint,\n",
    "        \"completion_window\": completion_window,\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "def create_jsonl_file(prompts, parameter_combinations, file_path):\n",
    "    data = []\n",
    "    for i, (prompt, params) in enumerate(zip(prompts, parameter_combinations)):\n",
    "        request_data = {\n",
    "            \"custom_id\": f\"request-{i+1}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": params[0],\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": params[1],\n",
    "                \"temperature\": params[2],\n",
    "                \"top_p\": params[3],\n",
    "                \"frequency_penalty\": params[4],\n",
    "                \"presence_penalty\": params[5],\n",
    "                \"stop\": params[6],\n",
    "                \"user\": params[7],\n",
    "                \"logprobs\": True,\n",
    "                \"top_logprobs\": 5\n",
    "            }\n",
    "        }\n",
    "        data.append(request_data)\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def generate_parameter_combinations(models, max_tokens_list, temperatures, top_ps, frequency_penalties, presence_penalties, stops, users):\n",
    "    combinations = list(itertools.product(models, max_tokens_list, temperatures, top_ps, frequency_penalties, presence_penalties, stops, users))\n",
    "    random.shuffle(combinations)  # Randomly shuffle to ensure random selection\n",
    "    return combinations\n",
    "\n",
    "def main():\n",
    "    prompt = \"I want a script that generates an analysis of some text data. Pull the text data from any source of text it doesn't matter what. Get the text data from a source you know of.\"\n",
    "    total_combinations = 2000  # Total number of unique parameter combinations to generate\n",
    "    n = 200  # number of jobs to use from the generated combinations\n",
    "    file_path = \"200_batch_test.jsonl\"\n",
    "    endpoint = \"/v1/chat/completions\"\n",
    "    completion_window = \"24h\"\n",
    "\n",
    "    # Define the parameter ranges for the sweep\n",
    "    models = [\"gpt-3.5-turbo\"]\n",
    "    max_tokens_list = [200, 300, 500, 700]\n",
    "    temperatures = [0.5, 0.7, 0.9, 1.0]\n",
    "    top_ps = [0.8, 0.9, 1.0]\n",
    "    frequency_penalties = [0, 0.5, 1.0]\n",
    "    presence_penalties = [0, 0.5, 1.0]\n",
    "    stops = [[\"\\n\"], [\".\", \"?\", \"!\"], None]\n",
    "    users = [\"user_charles\"]\n",
    "\n",
    "    # Generate a large pool of parameter combinations\n",
    "    parameter_combinations = generate_parameter_combinations(models, max_tokens_list, temperatures, top_ps, frequency_penalties, presence_penalties, stops, users)\n",
    "\n",
    "    # Select `n` random combinations from the pool\n",
    "    selected_combinations = random.sample(parameter_combinations, n)\n",
    "    prompts = [prompt] * len(selected_combinations)\n",
    "\n",
    "    # Step 1: Create the file with the selected parameter combinations\n",
    "    create_jsonl_file(prompts, selected_combinations, file_path)\n",
    "    \n",
    "    # Step 2: Upload the file\n",
    "    upload_response = upload_file(file_path, \"batch\")\n",
    "    input_file_id = upload_response['id']\n",
    "    print(f\"File uploaded. File ID: {input_file_id}\")\n",
    "\n",
    "    # Step 3: Create the batch job\n",
    "    batch_response = create_batch(input_file_id, endpoint, completion_window)\n",
    "    batch_id = batch_response['id']\n",
    "    print(f\"Batch job created. Batch ID: {batch_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed34e21e-7b9e-4952-8c44-3621ab2c9c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch statuses:\n",
      "batch_TYryq4oxmpN6MxoOtMQZRVLO:2024-05-21 17:43:28::In Progress\n",
      "batch_QsQq1inCRe7658QDxPfPop1W:2024-05-21 17:32:09::canceled\n",
      "batch_RreSlmceawjRlFaBLSP6Bv5K:2024-05-21 16:56:52::canceling\n",
      "batch_RcRye5IdrkCDNwNiNZO4FWa1:2024-05-21 16:54:17::Fail\n",
      "batch_GBx1zUqapiftq7vgL1VTltPH:2024-05-21 16:52:03::Fail\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     Status All Batch    ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'\n",
    "\n",
    "def list_batches(limit=20, after=None):\n",
    "    url = \"https://api.openai.com/v1/batches\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    params = {\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    if after:\n",
    "        params[\"after\"] = after\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def get_status_label(status):\n",
    "    if status == \"completed\":\n",
    "        return \"Finished\"\n",
    "    elif status == \"failed\":\n",
    "        return \"Fail\"\n",
    "    elif status == \"cancelling\":\n",
    "        return \"canceling\"\n",
    "    elif status == \"cancelled\":\n",
    "        return \"canceled\"\n",
    "    else:\n",
    "        return \"In Progress\"\n",
    "\n",
    "def main():\n",
    "    limit = 100  # Adjust the limit as needed\n",
    "    list_response = list_batches(limit=limit)\n",
    "\n",
    "    print(\"Batch statuses:\")\n",
    "    for batch in list_response['data']:\n",
    "        batch_id = batch['id']\n",
    "        created_at = datetime.utcfromtimestamp(batch['created_at']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        status_label = get_status_label(batch['status'])\n",
    "        print(f\"{batch_id}:{created_at}::{status_label}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e6fbacf-0319-4e8f-abaf-c1751e28db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch status and information:\n",
      "{\n",
      "  \"id\": \"batch_RreSlmceawjRlFaBLSP6Bv5K\",\n",
      "  \"object\": \"batch\",\n",
      "  \"endpoint\": \"/v1/chat/completions\",\n",
      "  \"errors\": null,\n",
      "  \"input_file_id\": \"file-9jyr72r96jaxWQCY1TN3vGNU\",\n",
      "  \"completion_window\": \"24h\",\n",
      "  \"status\": \"in_progress\",\n",
      "  \"output_file_id\": null,\n",
      "  \"error_file_id\": null,\n",
      "  \"created_at\": 1716310612,\n",
      "  \"in_progress_at\": 1716310613,\n",
      "  \"expires_at\": 1716397012,\n",
      "  \"finalizing_at\": null,\n",
      "  \"completed_at\": null,\n",
      "  \"failed_at\": null,\n",
      "  \"expired_at\": null,\n",
      "  \"cancelling_at\": null,\n",
      "  \"cancelled_at\": null,\n",
      "  \"request_counts\": {\n",
      "    \"total\": 10,\n",
      "    \"completed\": 0,\n",
      "    \"failed\": 0\n",
      "  },\n",
      "  \"metadata\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     Status 1 BATCH     ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'\n",
    "\n",
    "def retrieve_batch(batch_id):\n",
    "    url = f\"https://api.openai.com/v1/batches/{batch_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def main():\n",
    "    batch_id = \"batch_RreSlmceawjRlFaBLSP6Bv5K\"\n",
    "\n",
    "    # Retrieve the job status and info\n",
    "    batch_status = retrieve_batch(batch_id)\n",
    "    print(\"Batch status and information:\")\n",
    "    print(json.dumps(batch_status, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33bb370c-fd77-4cef-83eb-a796de7ab2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancel batch response:\n",
      "{'id': 'batch_RreSlmceawjRlFaBLSP6Bv5K', 'object': 'batch', 'endpoint': '/v1/chat/completions', 'errors': None, 'input_file_id': 'file-9jyr72r96jaxWQCY1TN3vGNU', 'completion_window': '24h', 'status': 'cancelling', 'output_file_id': None, 'error_file_id': None, 'created_at': 1716310612, 'in_progress_at': 1716310613, 'expires_at': 1716397012, 'finalizing_at': None, 'completed_at': None, 'failed_at': None, 'expired_at': None, 'cancelling_at': 1716316970, 'cancelled_at': None, 'request_counts': {'total': 10, 'completed': 0, 'failed': 0}, 'metadata': None}\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     Cancel BATCH     ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import requests\n",
    "\n",
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'\n",
    "\n",
    "def cancel_batch(batch_id):\n",
    "    url = f\"https://api.openai.com/v1/batches/{batch_id}/cancel\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    response = requests.post(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def main():\n",
    "    batch_id = \"batch_RreSlmceawjRlFaBLSP6Bv5K\"  # replace with the actual batch ID\n",
    "\n",
    "    # Cancel the batch\n",
    "    cancel_response = cancel_batch(batch_id)\n",
    "    print(\"Cancel batch response:\")\n",
    "    print(cancel_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24af460d-d9e3-4c14-9f85-b684184498b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": {\n",
      "    \"message\": \"Invalid 'batch_id': 'your_batch_id_here'. Expected an ID that begins with 'batch'.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": \"batch_id\",\n",
      "    \"code\": \"invalid_value\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     RETRIEVE BATCH     ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'\n",
    "\n",
    "def retrieve_batch(batch_id):\n",
    "    url = f\"https://api.openai.com/v1/batches/{batch_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def main():\n",
    "    batch_id = \"your_batch_id_here\"  # replace with the actual batch ID\n",
    "\n",
    "    # Step 4: Retrieve the job\n",
    "    final_status = retrieve_batch(batch_id)\n",
    "    print(json.dumps(final_status, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b40bd-b6af-4a73-a9cb-97ddfef7669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n",
    "model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct').to(device)\n",
    "\n",
    "def single_layer_pool(hidden_states, layer_index):\n",
    "    \"\"\"\n",
    "    Extract embeddings from a specific layer.\n",
    "    \"\"\"\n",
    "    selected_layer = hidden_states[layer_index]\n",
    "    return selected_layer.mean(dim=1)  # Average over the token dimension to get a single vector per sequence\n",
    "\n",
    "def embed_text(input_texts, max_length=500, layer_index=-1, scaling_factor=1.0):\n",
    "    try:\n",
    "        # Tokenize the input texts with truncation and without padding\n",
    "        batch_dict = tokenizer(input_texts, max_length=max_length, padding=False, truncation=True, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Check the length of the tokenized input and ignore if it's less than 100\n",
    "        token_length = batch_dict['input_ids'].shape[1]\n",
    "        if token_length < 100:\n",
    "            return None\n",
    "        \n",
    "        # Set output_hidden_states to True\n",
    "        outputs = model(**batch_dict, output_hidden_states=True)\n",
    "        \n",
    "        # Use the new pooling method with specified layer\n",
    "        embeddings = single_layer_pool(outputs.hidden_states, layer_index)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Scale embeddings\n",
    "        embeddings = embeddings * scaling_factor\n",
    "        \n",
    "        # Convert the embeddings to lists\n",
    "        embedding_lists = embeddings.cpu().tolist()\n",
    "        \n",
    "        return embedding_lists\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            del batch_dict\n",
    "            del outputs\n",
    "            del embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "max_input_text_len = 500  # Set a specific maximum length for truncation\n",
    "min_input_text_len = 100  # Minimum length threshold\n",
    "layer_index = -1  # Use the last layer by default\n",
    "scaling_factor = 1.0  # Scaling factor to amplify the embedding values\n",
    "\n",
    "file_path = './breakingchat.txt'\n",
    "user_list, chatgpt_list = parse_conversations(file_path)\n",
    "\n",
    "print(\"Analyzing User Texts\")\n",
    "embedding_user_list = []\n",
    "for text in user_list:\n",
    "    embeddings = embed_text(text, max_length=max_input_text_len, layer_index=layer_index, scaling_factor=scaling_factor)\n",
    "    if embeddings is not None:\n",
    "        embedding_user_list.append(embeddings)\n",
    "\n",
    "print(\"Analyzing ChatGPT Texts\")\n",
    "embedding_chatgpt_list = []\n",
    "for text in chatgpt_list:\n",
    "    embeddings = embed_text(text, max_length=max_input_text_len, layer_index=layer_index, scaling_factor=scaling_factor)\n",
    "    if embeddings is not None:\n",
    "        embedding_chatgpt_list.append(embeddings)\n",
    "\n",
    "# Convert lists of lists to numpy arrays and reshape to remove the extra dimension\n",
    "embedding_user_array = np.squeeze(np.array(embedding_user_list), axis=1)\n",
    "embedding_chatgpt_array = np.squeeze(np.array(embedding_chatgpt_list), axis=1)\n",
    "\n",
    "# Calculate variances\n",
    "user_variance = np.var(embedding_user_array, axis=0).mean()\n",
    "chatgpt_variance = np.var(embedding_chatgpt_array, axis=0).mean()\n",
    "\n",
    "print(f\"Mean Variance in User Embeddings: {user_variance}\")\n",
    "print(f\"Mean Variance in ChatGPT Embeddings: {chatgpt_variance}\")\n",
    "\n",
    "# Assume the following functions and imports are provided\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_autoencoder(input_data, latent_dim, num_epochs):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    \n",
    "    # Encoder configuration\n",
    "    encoded = tf.keras.layers.Dense(1024, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
    "    encoded = tf.keras.layers.LeakyReLU()(encoded)\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dropout(0.3)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
    "    encoded = tf.keras.layers.LeakyReLU()(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    # Decoder configuration\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.BatchNormalization()(decoded)\n",
    "    decoded = tf.keras.layers.LeakyReLU()(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dropout(0.3)(decoded)\n",
    "    decoded = tf.keras.layers.Dense(1024, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.BatchNormalization()(decoded)\n",
    "    decoded = tf.keras.layers.LeakyReLU()(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    # Autoencoder connecting encoder and decoder\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    # Move the model to GPU if available\n",
    "    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "        # Train the autoencoder\n",
    "        autoencoder.fit(input_data, input_data, epochs=num_epochs, batch_size=32, verbose=1)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    # Applying Gaussian filter to smooth the grids\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=2) for grid in encoded_2d_grid])\n",
    "\n",
    "    # Setting up the figure and 3D axis\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Creating a meshgrid for the x and y coordinates\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    \n",
    "    # Plotting each grid with an increasing offset in Z\n",
    "    offset = 0.0\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i] + i * offset  # Offset each grid\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    # Setting labels and title\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Free up GPU memory by deleting the model\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train the autoencoder and visualize embeddings\n",
    "grid_size = 50\n",
    "latent_dim = grid_size * grid_size\n",
    "num_epochs = 500\n",
    "\n",
    "combined_input = embedding_chatgpt_array\n",
    "encoder = train_autoencoder(combined_input, latent_dim, num_epochs)\n",
    "\n",
    "# Predict using the trained encoder\n",
    "encoded_2d_grid = encoder.predict(combined_input).reshape(-1, grid_size, grid_size)\n",
    "print(\"Variance in Encoded Outputs for ChatGPT Data:\", np.var(encoded_2d_grid, axis=0).mean())\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "# Now predict using user data\n",
    "combined_input = embedding_user_array\n",
    "encoded_2d_grid = encoder.predict(combined_input).reshape(-1, grid_size, grid_size)\n",
    "print(\"Variance in Encoded Outputs for User Data:\", np.var(encoded_2d_grid, axis=0).mean())\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f761a2-994b-44eb-9190-fe636f5675bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_user_list = []\n",
    "embedding_chatgpt_list = []\n",
    "max_input_text_len = 1000\n",
    "for text in user_list:\n",
    "    embedding_user_list.append(embed_text(text[0:max_input_text_len]))\n",
    "for text in chatgpt_list:\n",
    "    embedding_chatgpt_list.append(embed_text(text[0:max_input_text_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad085c7-0a80-4db2-b974-f58c787bc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    return embeddings\n",
    "# Note: You might need to limit the number of texts processed at once if the list is very large\n",
    "embedding_user_list = get_embeddings(user_list[:max_input_text_len])\n",
    "embedding_chatgpt_list = get_embeddings(chatgpt_list[:max_input_text_len])\n",
    "import numpy as np\n",
    "\n",
    "# Calculate variance for each set of embeddin gs\n",
    "user_variance = np.var(embedding_user_list, axis=0).mean()\n",
    "chatgpt_variance = np.var(embedding_chatgpt_list, axis=0).mean()\n",
    "\n",
    "print(f\"Mean Variance in User Embeddings: {user_variance}\")\n",
    "print(f\"Mean Variance in ChatGPT Embeddings: {chatgpt_variance}\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "# Assuming `user_list` and `chatgpt_list` contain your texts\n",
    "embedding_user_array = get_embeddings(user_list[:max_input_text_len])\n",
    "embedding_chatgpt_array = get_embeddings(chatgpt_list[:max_input_text_len])\n",
    "\n",
    "# Ensure both arrays are of the same shape\n",
    "if embedding_user_array.shape[1] != embedding_chatgpt_array.shape[1]:\n",
    "    raise ValueError(\"Embedding dimensions do not match and cannot be concatenated.\")\n",
    "\n",
    "# Here we use only ChatGPT array for training as an example\n",
    "combined_input = embedding_chatgpt_array\n",
    "\n",
    "grid_size = 30\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Train the autoencoder\n",
    "encoder = train_autoencoder(combined_input, latent_dim, 30)\n",
    "\n",
    "# Predict using the trained encoder\n",
    "encoded_2d_grid = encoder.predict(combined_input).reshape(-1, grid_size, grid_size)\n",
    "print(\"Variance in Encoded Outputs for ChatGPT Data:\", np.var(encoded_2d_grid, axis=0).mean())\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "# Now predict using user data\n",
    "combined_input = embedding_user_array\n",
    "encoded_2d_grid = encoder.predict(combined_input).reshape(-1, grid_size, grid_size)\n",
    "print(\"Variance in Encoded Outputs for User Data:\", np.var(encoded_2d_grid, axis=0).mean())\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddce106-3920-4209-ba65-c8a68ac8cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def single_layer_pool(hidden_states, layer_index):\n",
    "    \"\"\"\n",
    "    Extract embeddings from a specific layer.\n",
    "    \"\"\"\n",
    "    selected_layer = hidden_states[layer_index]\n",
    "    return selected_layer.mean(dim=1)  # Average over the token dimension to get a single vector per sequence\n",
    "\n",
    "def embed_text(input_texts, max_length=500, layer_index=-1, scaling_factor=1.0):\n",
    "    try:\n",
    "        # Tokenize the input texts with truncation and without padding\n",
    "        batch_dict = tokenizer(input_texts, max_length=max_length, padding=False, truncation=True, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Check the length of the tokenized input and ignore if it's less than 100\n",
    "        token_length = batch_dict['input_ids'].shape[1]\n",
    "        if token_length < 100:\n",
    "            return None\n",
    "        \n",
    "        # Set output_hidden_states to True\n",
    "        outputs = model(**batch_dict, output_hidden_states=True)\n",
    "        \n",
    "        # Use the new pooling method with specified layer\n",
    "        embeddings = single_layer_pool(outputs.hidden_states, layer_index)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Scale embeddings\n",
    "        embeddings = embeddings * scaling_factor\n",
    "        \n",
    "        # Convert the embeddings to lists\n",
    "        embedding_lists = embeddings.cpu().tolist()\n",
    "        \n",
    "        return embedding_lists\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            del batch_dict\n",
    "            del outputs\n",
    "            del embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "max_input_text_len = 500  # Set a specific maximum length for truncation\n",
    "min_input_text_len = 100  # Minimum length threshold\n",
    "layer_index = -1  # Use the last layer by default\n",
    "scaling_factor = 1000.0  # Scaling factor to amplify the embedding values\n",
    "\n",
    "print(\"Analyzing User Texts\")\n",
    "embedding_user_list = []\n",
    "for text in user_list:\n",
    "    embeddings = embed_text(text, max_length=max_input_text_len, layer_index=layer_index, scaling_factor=scaling_factor)\n",
    "    if embeddings is not None:\n",
    "        embedding_user_list.append(embeddings)\n",
    "\n",
    "print(\"Analyzing ChatGPT Texts\")\n",
    "embedding_chatgpt_list = []\n",
    "for text in chatgpt_list:\n",
    "    embeddings = embed_text(text, max_length=max_input_text_len, layer_index=layer_index, scaling_factor=scaling_factor)\n",
    "    if embeddings is not None:\n",
    "        embedding_chatgpt_list.append(embeddings)\n",
    "\n",
    "# Convert lists of lists to numpy arrays\n",
    "embedding_user_array = np.array(embedding_user_list)\n",
    "embedding_chatgpt_array = np.array(embedding_chatgpt_list)\n",
    "\n",
    "# Calculate variances\n",
    "user_variance = np.var(embedding_user_array, axis=0).mean()\n",
    "chatgpt_variance = np.var(embedding_chatgpt_array, axis=0).mean()\n",
    "\n",
    "print(f\"Mean Variance in User Embeddings: {user_variance}\")\n",
    "print(f\"Mean Variance in ChatGPT Embeddings: {chatgpt_variance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825bdaf7-7f23-4002-a661-f02a353ef633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=1000, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=10, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "def extract_token_probs(data):\n",
    "    # Initialize an empty list to store the token probabilities\n",
    "    token_probs_list = []\n",
    "    \n",
    "    # Iterate over each entry in the data list\n",
    "    for entry in data:\n",
    "        # Check if 'token_probs' key exists in the dictionary\n",
    "        if 'token_probs' in entry:\n",
    "            # Append the numpy array of token probabilities to the list\n",
    "            token_probs_list.append(entry['token_probs'])\n",
    "    \n",
    "    # Return the list of token probabilities\n",
    "    return token_probs_list\n",
    "    \n",
    "def pad_token_probs(probabilities, pad_value=0):\n",
    "    # Find the maximum length of the token probabilities list\n",
    "    max_length = max(len(probs) for probs in probabilities)\n",
    "    # Pad each probabilities list to the maximum length\n",
    "    padded_probabilities = np.array([np.pad(probs, (0, max_length - len(probs)), 'constant', constant_values=pad_value) for probs in probabilities])\n",
    "    return padded_probabilities\n",
    "\n",
    "\n",
    "prompt = \"I want a script that generates an analysis of some text data. Pull the text data from any source of text it doesn't matter what.\"\n",
    "n = 10\n",
    "responses, log_probs = get_responses(prompt, n)\n",
    "embeddings = get_embeddings(responses)\n",
    "\n",
    "probability_map = analyze_log_probs(log_probs)\n",
    "token_probs_list = extract_token_probs(probability_map)\n",
    "\n",
    "token_probabilities = pad_token_probs([entry['token_probs'] for entry in probability_map])\n",
    "\n",
    "# Combine embeddings and token probabilities\n",
    "combined_input = np.hstack((embeddings, token_probabilities))\n",
    "\n",
    "grid_size = 30\n",
    "latent_dim = grid_size * grid_size\n",
    "encoder = train_autoencoder(combined_input, latent_dim)\n",
    "\n",
    "encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093deb9e-9491-4c15-8d48-2ae831e7a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad the token probabilities to the same length\n",
    "def pad_token_probs(probabilities, pad_value=0):\n",
    "    # Find the maximum length of the token probabilities list\n",
    "    max_length = max(len(probs) for probs in probabilities)\n",
    "    # Pad each probabilities list to the maximum length\n",
    "    padded_probabilities = np.array([np.pad(probs, (0, max_length - len(probs)), 'constant', constant_values=pad_value) for probs in probabilities])\n",
    "    return padded_probabilities\n",
    "\n",
    "# Usage in your main workflow\n",
    "token_probabilities = pad_token_probs([entry['token_probs'] for entry in probability_map])\n",
    "\n",
    "combined_input = np.hstack((embeddings, token_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b3087-e335-4c31-8041-8d62202260bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=50, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=5) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "grid_size = 100\n",
    "latent_dim = grid_size * grid_size\n",
    "encoder = train_autoencoder(combined_input, latent_dim)\n",
    "encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf38c31-c8f0-489b-a621-46af1135ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=30, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    # Applying Gaussian filter to smooth the grids\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "    # Setting up the figure and 3D axis\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Creating a meshgrid for the x and y coordinates\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    \n",
    "    # Plotting each grid with an increasing offset in Z\n",
    "    offset = 0.5\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i] + i * offset  # Offset each grid\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    # Setting labels and title\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "grid_size = 30\n",
    "latent_dim = grid_size * grid_size\n",
    "encoder = train_autoencoder(combined_input, latent_dim)\n",
    "encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a79e1-10e2-4efb-ba53-bf96679356ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Set output_hidden_states to True\n",
    "outputs = model(**batch_dict, output_hidden_states=True)\n",
    "# Use the new pooling method\n",
    "embeddings = multi_layer_pool(outputs.hidden_states, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Convert the embeddings to lists\n",
    "embedding_lists = embeddings.cpu().tolist()\n",
    "for i, embedding in enumerate(embedding_lists):\n",
    "    print(f'Embedding for input {i+1}: {embedding}')\n",
    "\n",
    "# To print the similarity scores\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(\"Similarity scores:\", scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba15aee8-d7c8-46c5-9f7c-86d749800f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "# Each query must come with a one-sentence instruction that describes the task\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "queries = [\n",
    "    get_detailed_instruct(task, 'how much protein should a female eat'),\n",
    "    get_detailed_instruct(task, 'summit define')\n",
    "]\n",
    "# No need to add instruction for retrieval documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n",
    "]\n",
    "input_texts = queries + documents\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n",
    "model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc899c-6a9f-41c6-88ff-cc60defcee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_pool(hidden_states: list, attention_mask: Tensor, num_layers: int = 1) -> Tensor:\n",
    "    \"\"\"\n",
    "    Extract embeddings from the last 'num_layers' layers and concatenate them.\n",
    "    \"\"\"\n",
    "    # Get the last num_layers layers\n",
    "    all_layers = hidden_states[-num_layers:]  \n",
    "    # Concatenate them on the embedding dimension\n",
    "    concatenated_layers = torch.cat(all_layers, dim=-1)  \n",
    "\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return concatenated_layers[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = concatenated_layers.shape[0]\n",
    "        return concatenated_layers[torch.arange(batch_size, device=concatenated_layers.device), sequence_lengths]\n",
    "\n",
    "max_length = 4096\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Set output_hidden_states to True\n",
    "outputs = model(**batch_dict, output_hidden_states=True)\n",
    "# Use the new pooling method\n",
    "embeddings = multi_layer_pool(outputs.hidden_states, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Convert the embeddings to lists\n",
    "embedding_lists = embeddings.cpu().tolist()\n",
    "\n",
    "print(len(embedding_lists[0]))\n",
    "print(embedding_lists[0][0:100])\n",
    "\n",
    "# To print the similarity scores\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(\"Similarity scores:\", scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7939c53-084c-4ccb-a858-ca34c4a0b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "def get_full_response(prompt, n=1, max_tokens=1000, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  # Replace with your model ID as needed\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True,  # Enable logprobs\n",
    "            top_logprobs=10  # Specify number of top log probabilities to return\n",
    "        )\n",
    "\n",
    "        # Print the entire response object\n",
    "        return response\n",
    "\n",
    "# Example usage\n",
    "response = get_full_response(\"Tell me a joke\", n=1)\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_response_info(response):\n",
    "    # Extract the conversation response\n",
    "    response_match = re.search(r'content=\"(.*?)\"', str(response), re.DOTALL)\n",
    "    response_text = response_match.group(1) if response_match else \"\"\n",
    "\n",
    "    # Extract the logprobs of the response\n",
    "    logprobs_match = re.findall(r'logprob=(-?\\d+\\.\\d+)', str(response))\n",
    "    logprobs = [float(logprob) for logprob in logprobs_match]\n",
    "\n",
    "    return response_text, logprobs\n",
    "    \n",
    "reply, logprobs= extract_response_info(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95aec0c-f714-441c-bf0e-6db95ec12f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chat_completion_data(response):\n",
    "    # Data structure to hold the results\n",
    "    data = {\n",
    "        \"Response Content\": \"\",\n",
    "        \"Logprobs\": [],\n",
    "        \"Top Logprob Words\": [],\n",
    "        \"Top Logprob Values\": []\n",
    "    }\n",
    "    \n",
    "    # Assume the first choice for simplification; adapt as needed for multiple choices\n",
    "    if response.choices:\n",
    "        choice = response.choices[0]\n",
    "        data[\"Response Content\"] = choice.message.content\n",
    "        \n",
    "        # Extract token logprob information\n",
    "        for token_logprob in choice.logprobs.content:\n",
    "            # Append the logprob of the current token to the list\n",
    "            data[\"Logprobs\"].append(token_logprob.logprob)\n",
    "            \n",
    "            # For collecting top logprob words and their values\n",
    "            top_words = []\n",
    "            top_values = []\n",
    "            \n",
    "            # Extract top logprob details\n",
    "            for top_logprob in token_logprob.top_logprobs:\n",
    "                top_words.append(top_logprob.token)\n",
    "                top_values.append(top_logprob.logprob)\n",
    "            \n",
    "            # Append each token's top logprob words and values\n",
    "            data[\"Top Logprob Words\"].append(top_words)\n",
    "            data[\"Top Logprob Values\"].append(top_values)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a response object from the OpenAI API\n",
    "# print(extract_chat_completion_data(response))\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a response object from the OpenAI API\n",
    "print(extract_chat_completion_data(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33261ab-5276-4b73-944a-e9e19416773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(response))\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cb8fa-310c-4a0f-9cce-b835d9a2d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conversations(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    conversations = content.split('\\n\\n')\n",
    "    user_messages = []\n",
    "    chatgpt_messages = []\n",
    "\n",
    "    current_label = None\n",
    "    current_message = []\n",
    "\n",
    "    for conversation in conversations:\n",
    "        lines = conversation.strip().split('\\n')\n",
    "        if len(lines) >= 1:\n",
    "            if lines[0] == '#USER':\n",
    "                if current_label == '##ChatGPT':\n",
    "                    chatgpt_messages.append(' '.join(current_message))\n",
    "                current_label = '#USER'\n",
    "                current_message = lines[1:]\n",
    "            elif lines[0] == '##ChatGPT':\n",
    "                if current_label == '#USER':\n",
    "                    user_messages.append(' '.join(current_message))\n",
    "                current_label = '##ChatGPT'\n",
    "                current_message = lines[1:]\n",
    "            else:\n",
    "                current_message.extend(lines)\n",
    "\n",
    "    if current_label == '#USER':\n",
    "        user_messages.append(' '.join(current_message))\n",
    "    elif current_label == '##ChatGPT':\n",
    "        chatgpt_messages.append(' '.join(current_message))\n",
    "\n",
    "    return user_messages, chatgpt_messages\n",
    "\n",
    "# Example usage\n",
    "file_path = './breakingchat.txt'\n",
    "user_list, chatgpt_list = parse_conversations(file_path)\n",
    "\n",
    "\"\"\"\n",
    "print(\"User Messages:\")\n",
    "for message in user_list:\n",
    "    print(message)\n",
    "    print()\n",
    "\n",
    "print(\"ChatGPT Messages:\")\n",
    "for message in chatgpt_list:\n",
    "    print(message)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8eda05-7d2d-45ae-a5a8-3c04c69eb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91f16e-7de6-4886-b912-f06499a700db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "        token_probs = softmax(token_logprobs)\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs\n",
    "        })\n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "    autoencoder.fit(input_data, input_data, epochs=20, batch_size=8, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 20\n",
    "    responses, log_probs = get_responses(prompt, n)\n",
    "    embeddings = get_embeddings(responses)\n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    token_probs_list = np.array([entry['token_probs'] for entry in probability_map])\n",
    "    combined_input = np.hstack((embeddings, token_probs_list))\n",
    "    grid_size = 30\n",
    "    latent_dim = grid_size * grid_size\n",
    "    encoder = train_autoencoder(combined_input, latent_dim)\n",
    "    encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "    visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13554d8c-06f4-4b9a-80ba-c5ed5ca5dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Prepare the Text\n",
    "sample_text = \"Natural language processing (NLP) involves the interaction between computers and humans through language. It enables machines to read, understand, and derive meaning from human languages.\"\n",
    "\n",
    "# Step 2: Vectorize the Text using Universal Sentence Encoder from TensorFlow Hub\n",
    "# Load Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "# Convert the sample text into an embedding\n",
    "text_embedding = embedding_layer([sample_text])\n",
    "print(\"Original Embedding Shape:\", text_embedding.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder Model to reduce dimensionality to a 2D array\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embedding.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(32, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(2 * 2, activation='linear')(encoded)  # 2x2 array output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(2 * 2,))\n",
    "decoded = tf.keras.layers.Dense(32, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(64, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embedding.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "# Here, we use the text embedding itself as the target (autoencoder learns to reconstruct its input)\n",
    "autoencoder.fit(text_embedding, text_embedding, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "# Predict the 2D coordinates\n",
    "encoded_2d_array = encoder(text_embedding)\n",
    "print(\"2D Array for the Input Text:\", encoded_2d_array.numpy().reshape(2, 2))\n",
    "\n",
    "# Step 4: Visualize the 2D Array Result\n",
    "encoded_2d_array_reshaped = encoded_2d_array.numpy().reshape(2, 2)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(encoded_2d_array_reshaped, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('2D Array Embedding of the Text')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e2dcd-9cc3-4b69-b51c-a06be4fcacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(encoded_2d_grid.shape[0]):\n",
    "    Z = encoded_2d_grid[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "\n",
    "ax.set_title('3D Visualization of Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca1aa2-814f-4937-b562-8baab5e7c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc7ccd-fb40-4d5b-9c3e-997014f238b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dropout(0.5)(encoded)  # Adding dropout\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56558db2-81c1-485f-8b58-e3dcbcd7616f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33c2f9-0276-42d9-b09d-47817c28d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:10]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 10  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=5) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557eeaf-cf81-4563-bf13-82b9e9eded62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai sentence-transformers scikit-learn numpy matplotlib hdbscan umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb824ca-eb85-425a-b975-0a4686dfe97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to cluster embeddings (HDBSCAN or K-Means)\n",
    "def cluster_embeddings(embeddings, method='hdbscan'):\n",
    "    if method == 'hdbscan':\n",
    "        clusterer = HDBSCAN(min_cluster_size=15, cluster_selection_method='leaf')\n",
    "        cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    else:\n",
    "        clusterer = KMeans(n_clusters=5)\n",
    "        cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    return cluster_labels\n",
    "\n",
    "# Function to visualize embeddings using UMAP\n",
    "def visualize_embeddings(embeddings, labels):\n",
    "    n_neighbors = min(15, len(embeddings) - 1)\n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=n_neighbors)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='Spectral')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def plot_raw_logits(probability_map):\n",
    "    for entry in probability_map:\n",
    "        tokens = entry['tokens']\n",
    "        log_probs = entry['token_logprobs']  # Assuming this key will be added to the dictionary\n",
    "        \n",
    "        # Ensure we have valid data\n",
    "        if tokens and log_probs and len(tokens) == len(log_probs):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(tokens, log_probs)\n",
    "            plt.xlabel('Tokens')\n",
    "            plt.ylabel('Log Probabilities')\n",
    "            plt.title('Raw Log Probabilities of Tokens')\n",
    "            plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "def plot_probabilities(probability_map):\n",
    "    for entry in probability_map:\n",
    "        tokens = entry['tokens']\n",
    "        token_probs = entry['token_probs']\n",
    "        \n",
    "        # Ensure we have valid data\n",
    "        if tokens and token_probs is not None and len(tokens) == len(token_probs):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(tokens, token_probs)\n",
    "            plt.xlabel('Tokens')\n",
    "            plt.ylabel('Probabilities')\n",
    "            plt.title('Token Probabilities')\n",
    "            plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 10\n",
    "    \n",
    "    responses, log_probs = get_responses(\n",
    "    prompt=prompt,\n",
    "    n=5,\n",
    "    max_tokens=100,\n",
    "    temperature=2,\n",
    "    top_p=0.3,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.3\n",
    "    )\n",
    "\n",
    "    embeddings = get_embeddings(responses)\n",
    "    \n",
    "    # Choose clustering method ('hdbscan' or 'kmeans')\n",
    "    clustering_method = 'hdbscan'\n",
    "    cluster_labels = cluster_embeddings(embeddings, method=clustering_method)\n",
    "    \n",
    "    #print(\"Clustering Results:\")\n",
    "    #for i, response in enumerate(responses):\n",
    "        #print(f\"Cluster {cluster_labels[i]}: {response}\")\n",
    "    \n",
    "    visualize_embeddings(embeddings, cluster_labels)\n",
    "\n",
    "    print(log_probs)\n",
    "    \n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    print(probability_map)\n",
    "    plot_probabilities(probability_map)\n",
    "    plot_raw_logits(probability_map)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22516b0f-490e-430d-bbbe-280df6483956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=10, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "def extract_token_probs(data):\n",
    "    # Initialize an empty list to store the token probabilities\n",
    "    token_probs_list = []\n",
    "    \n",
    "    # Iterate over each entry in the data list\n",
    "    for entry in data:\n",
    "        # Check if 'token_probs' key exists in the dictionary\n",
    "        if 'token_probs' in entry:\n",
    "            # Append the numpy array of token probabilities to the list\n",
    "            token_probs_list.append(entry['token_probs'])\n",
    "    \n",
    "    # Return the list of token probabilities\n",
    "    return token_probs_list\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 1\n",
    "    responses, log_probs = get_responses(prompt, n)\n",
    "    embeddings = get_embeddings(responses)\n",
    "\n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    token_probs_list = extract_token_probs(probability_map)\n",
    "    \n",
    "    # Extract token probabilities from the probability map\n",
    "    token_probabilities = np.array([entry['token_probs'] for entry in probability_map])\n",
    "    \n",
    "    # Combine embeddings and token probabilities\n",
    "    combined_input = np.hstack((embeddings, token_probabilities))\n",
    "\n",
    "    grid_size = 30\n",
    "    latent_dim = grid_size * grid_size\n",
    "    encoder = train_autoencoder(combined_input, latent_dim)\n",
    "\n",
    "    encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "    visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d7942-5a5f-40bf-b0e3-9f97fc97bf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

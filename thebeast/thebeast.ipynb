{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225e162-4e87-4b2f-b544-98fa1fac6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai sentence-transformers scikit-learn numpy matplotlib hdbscan umap-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825bdaf7-7f23-4002-a661-f02a353ef633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=1000, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=10, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "def extract_token_probs(data):\n",
    "    # Initialize an empty list to store the token probabilities\n",
    "    token_probs_list = []\n",
    "    \n",
    "    # Iterate over each entry in the data list\n",
    "    for entry in data:\n",
    "        # Check if 'token_probs' key exists in the dictionary\n",
    "        if 'token_probs' in entry:\n",
    "            # Append the numpy array of token probabilities to the list\n",
    "            token_probs_list.append(entry['token_probs'])\n",
    "    \n",
    "    # Return the list of token probabilities\n",
    "    return token_probs_list\n",
    "    \n",
    "def pad_token_probs(probabilities, pad_value=0):\n",
    "    # Find the maximum length of the token probabilities list\n",
    "    max_length = max(len(probs) for probs in probabilities)\n",
    "    # Pad each probabilities list to the maximum length\n",
    "    padded_probabilities = np.array([np.pad(probs, (0, max_length - len(probs)), 'constant', constant_values=pad_value) for probs in probabilities])\n",
    "    return padded_probabilities\n",
    "\n",
    "\n",
    "prompt = \"I want a script that generates an analysis of some text data. Pull the text data from any source of text it doesn't matter what.\"\n",
    "n = 20\n",
    "responses, log_probs = get_responses(prompt, n)\n",
    "embeddings = get_embeddings(responses)\n",
    "\n",
    "probability_map = analyze_log_probs(log_probs)\n",
    "token_probs_list = extract_token_probs(probability_map)\n",
    "\n",
    "token_probabilities = pad_token_probs([entry['token_probs'] for entry in probability_map])\n",
    "\n",
    "# Combine embeddings and token probabilities\n",
    "combined_input = np.hstack((embeddings, token_probabilities))\n",
    "\n",
    "grid_size = 30\n",
    "latent_dim = grid_size * grid_size\n",
    "encoder = train_autoencoder(combined_input, latent_dim)\n",
    "\n",
    "encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093deb9e-9491-4c15-8d48-2ae831e7a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad the token probabilities to the same length\n",
    "def pad_token_probs(probabilities, pad_value=0):\n",
    "    # Find the maximum length of the token probabilities list\n",
    "    max_length = max(len(probs) for probs in probabilities)\n",
    "    # Pad each probabilities list to the maximum length\n",
    "    padded_probabilities = np.array([np.pad(probs, (0, max_length - len(probs)), 'constant', constant_values=pad_value) for probs in probabilities])\n",
    "    return padded_probabilities\n",
    "\n",
    "# Usage in your main workflow\n",
    "token_probabilities = pad_token_probs([entry['token_probs'] for entry in probability_map])\n",
    "\n",
    "combined_input = np.hstack((embeddings, token_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b3087-e335-4c31-8041-8d62202260bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=50, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=5) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "grid_size = 100\n",
    "latent_dim = grid_size * grid_size\n",
    "encoder = train_autoencoder(combined_input, latent_dim)\n",
    "encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8eda05-7d2d-45ae-a5a8-3c04c69eb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582dfa58-751f-4c94-9d1c-8b7d95f1fa4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91f16e-7de6-4886-b912-f06499a700db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "        token_probs = softmax(token_logprobs)\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs\n",
    "        })\n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "    autoencoder.fit(input_data, input_data, epochs=20, batch_size=8, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 20\n",
    "    responses, log_probs = get_responses(prompt, n)\n",
    "    embeddings = get_embeddings(responses)\n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    token_probs_list = np.array([entry['token_probs'] for entry in probability_map])\n",
    "    combined_input = np.hstack((embeddings, token_probs_list))\n",
    "    grid_size = 30\n",
    "    latent_dim = grid_size * grid_size\n",
    "    encoder = train_autoencoder(combined_input, latent_dim)\n",
    "    encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "    visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13554d8c-06f4-4b9a-80ba-c5ed5ca5dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Prepare the Text\n",
    "sample_text = \"Natural language processing (NLP) involves the interaction between computers and humans through language. It enables machines to read, understand, and derive meaning from human languages.\"\n",
    "\n",
    "# Step 2: Vectorize the Text using Universal Sentence Encoder from TensorFlow Hub\n",
    "# Load Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "# Convert the sample text into an embedding\n",
    "text_embedding = embedding_layer([sample_text])\n",
    "print(\"Original Embedding Shape:\", text_embedding.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder Model to reduce dimensionality to a 2D array\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embedding.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(32, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(2 * 2, activation='linear')(encoded)  # 2x2 array output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(2 * 2,))\n",
    "decoded = tf.keras.layers.Dense(32, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(64, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embedding.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "# Here, we use the text embedding itself as the target (autoencoder learns to reconstruct its input)\n",
    "autoencoder.fit(text_embedding, text_embedding, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "# Predict the 2D coordinates\n",
    "encoded_2d_array = encoder(text_embedding)\n",
    "print(\"2D Array for the Input Text:\", encoded_2d_array.numpy().reshape(2, 2))\n",
    "\n",
    "# Step 4: Visualize the 2D Array Result\n",
    "encoded_2d_array_reshaped = encoded_2d_array.numpy().reshape(2, 2)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(encoded_2d_array_reshaped, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('2D Array Embedding of the Text')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e2dcd-9cc3-4b69-b51c-a06be4fcacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(encoded_2d_grid.shape[0]):\n",
    "    Z = encoded_2d_grid[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "\n",
    "ax.set_title('3D Visualization of Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca1aa2-814f-4937-b562-8baab5e7c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc7ccd-fb40-4d5b-9c3e-997014f238b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dropout(0.5)(encoded)  # Adding dropout\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56558db2-81c1-485f-8b58-e3dcbcd7616f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33c2f9-0276-42d9-b09d-47817c28d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:10]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 10  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=5) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557eeaf-cf81-4563-bf13-82b9e9eded62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai sentence-transformers scikit-learn numpy matplotlib hdbscan umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb824ca-eb85-425a-b975-0a4686dfe97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to cluster embeddings (HDBSCAN or K-Means)\n",
    "def cluster_embeddings(embeddings, method='hdbscan'):\n",
    "    if method == 'hdbscan':\n",
    "        clusterer = HDBSCAN(min_cluster_size=15, cluster_selection_method='leaf')\n",
    "        cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    else:\n",
    "        clusterer = KMeans(n_clusters=5)\n",
    "        cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    return cluster_labels\n",
    "\n",
    "# Function to visualize embeddings using UMAP\n",
    "def visualize_embeddings(embeddings, labels):\n",
    "    n_neighbors = min(15, len(embeddings) - 1)\n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=n_neighbors)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='Spectral')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def plot_raw_logits(probability_map):\n",
    "    for entry in probability_map:\n",
    "        tokens = entry['tokens']\n",
    "        log_probs = entry['token_logprobs']  # Assuming this key will be added to the dictionary\n",
    "        \n",
    "        # Ensure we have valid data\n",
    "        if tokens and log_probs and len(tokens) == len(log_probs):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(tokens, log_probs)\n",
    "            plt.xlabel('Tokens')\n",
    "            plt.ylabel('Log Probabilities')\n",
    "            plt.title('Raw Log Probabilities of Tokens')\n",
    "            plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "def plot_probabilities(probability_map):\n",
    "    for entry in probability_map:\n",
    "        tokens = entry['tokens']\n",
    "        token_probs = entry['token_probs']\n",
    "        \n",
    "        # Ensure we have valid data\n",
    "        if tokens and token_probs is not None and len(tokens) == len(token_probs):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(tokens, token_probs)\n",
    "            plt.xlabel('Tokens')\n",
    "            plt.ylabel('Probabilities')\n",
    "            plt.title('Token Probabilities')\n",
    "            plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 10\n",
    "    \n",
    "    responses, log_probs = get_responses(\n",
    "    prompt=prompt,\n",
    "    n=5,\n",
    "    max_tokens=100,\n",
    "    temperature=2,\n",
    "    top_p=0.3,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.3\n",
    "    )\n",
    "\n",
    "    embeddings = get_embeddings(responses)\n",
    "    \n",
    "    # Choose clustering method ('hdbscan' or 'kmeans')\n",
    "    clustering_method = 'hdbscan'\n",
    "    cluster_labels = cluster_embeddings(embeddings, method=clustering_method)\n",
    "    \n",
    "    #print(\"Clustering Results:\")\n",
    "    #for i, response in enumerate(responses):\n",
    "        #print(f\"Cluster {cluster_labels[i]}: {response}\")\n",
    "    \n",
    "    visualize_embeddings(embeddings, cluster_labels)\n",
    "\n",
    "    print(log_probs)\n",
    "    \n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    print(probability_map)\n",
    "    plot_probabilities(probability_map)\n",
    "    plot_raw_logits(probability_map)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22516b0f-490e-430d-bbbe-280df6483956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=10, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "def extract_token_probs(data):\n",
    "    # Initialize an empty list to store the token probabilities\n",
    "    token_probs_list = []\n",
    "    \n",
    "    # Iterate over each entry in the data list\n",
    "    for entry in data:\n",
    "        # Check if 'token_probs' key exists in the dictionary\n",
    "        if 'token_probs' in entry:\n",
    "            # Append the numpy array of token probabilities to the list\n",
    "            token_probs_list.append(entry['token_probs'])\n",
    "    \n",
    "    # Return the list of token probabilities\n",
    "    return token_probs_list\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 1\n",
    "    responses, log_probs = get_responses(prompt, n)\n",
    "    embeddings = get_embeddings(responses)\n",
    "\n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    token_probs_list = extract_token_probs(probability_map)\n",
    "    \n",
    "    # Extract token probabilities from the probability map\n",
    "    token_probabilities = np.array([entry['token_probs'] for entry in probability_map])\n",
    "    \n",
    "    # Combine embeddings and token probabilities\n",
    "    combined_input = np.hstack((embeddings, token_probabilities))\n",
    "\n",
    "    grid_size = 30\n",
    "    latent_dim = grid_size * grid_size\n",
    "    encoder = train_autoencoder(combined_input, latent_dim)\n",
    "\n",
    "    encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "    visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d7942-5a5f-40bf-b0e3-9f97fc97bf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

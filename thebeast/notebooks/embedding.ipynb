{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ded0f8-6fa0-4a8f-b72e-01f78cd47112",
   "metadata": {},
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate fake embedding dataset with different lengths\n",
    "def generate_fake_embeddings(num_groups=10, min_len=5, max_len=20, embedding_dim=50):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    embeddings = []\n",
    "    for _ in range(num_groups):\n",
    "        group_len = np.random.randint(min_len, max_len)\n",
    "        group_embeddings = [np.random.rand(np.random.randint(embedding_dim//2, embedding_dim*2)) for _ in range(group_len)]\n",
    "        embeddings.append(group_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Interpolation method to bring all embeddings to the same size\n",
    "def interpolate_embeddings(embeddings, target_dim=50):\n",
    "    interpolated_embeddings = []\n",
    "    for embed in embeddings:\n",
    "        original_indices = np.linspace(0, 1, num=len(embed))\n",
    "        target_indices = np.linspace(0, 1, num=target_dim)\n",
    "        interpolated_embed = np.interp(target_indices, original_indices, embed)\n",
    "        interpolated_embeddings.append(interpolated_embed)\n",
    "    return interpolated_embeddings\n",
    "\n",
    "# PCA function to project embeddings to a fixed size\n",
    "def apply_pca(embeddings, target_dim=50):\n",
    "    interpolated_embeddings = interpolate_embeddings(embeddings, target_dim)\n",
    "    pca = PCA(n_components=min(target_dim, len(interpolated_embeddings)))\n",
    "    pca_embeddings = pca.fit_transform(interpolated_embeddings)\n",
    "    return pca_embeddings\n",
    "\n",
    "# Mean pooling function\n",
    "def mean_pooling(embeddings, target_dim=50):\n",
    "    interpolated_embeddings = interpolate_embeddings(embeddings, target_dim)\n",
    "    return np.mean(interpolated_embeddings, axis=0)\n",
    "\n",
    "# Hybrid approach: PCA then Mean pooling\n",
    "def hybrid_pca_mean_pooling(embeddings, target_dim=50):\n",
    "    pca_embeddings = apply_pca(embeddings, target_dim)\n",
    "    return mean_pooling(pca_embeddings, target_dim)\n",
    "\n",
    "# Generate fake dataset\n",
    "embeddings = generate_fake_embeddings()\n",
    "\n",
    "# Apply PCA\n",
    "pca_embeddings = [apply_pca(group) for group in embeddings]\n",
    "\n",
    "# Apply Mean Pooling\n",
    "mean_pooled_embeddings = [mean_pooling(group) for group in embeddings]\n",
    "\n",
    "# Apply Hybrid PCA then Mean Pooling\n",
    "hybrid_embeddings = [hybrid_pca_mean_pooling(group) for group in embeddings]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame for better display\n",
    "result_df = pd.DataFrame({\n",
    "    'PCA Embedding': [emb.tolist() for emb in pca_embeddings],\n",
    "    'Mean Pooling Embedding': [emb.tolist() for emb in mean_pooled_embeddings],\n",
    "    'Hybrid Embedding': [emb.tolist() for emb in hybrid_embeddings]\n",
    "})\n",
    "\n",
    "\n",
    "result_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1cb57-1f3f-43ff-8189-ddf077dcd7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated code with separate handling for PCA embedding shape in the test function\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Load the embeddings from the JSON file\n",
    "def load_embeddings(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Interpolation method to bring all embeddings to the same size\n",
    "def interpolate_embeddings(embeddings, target_dim=50):\n",
    "    interpolated_embeddings = []\n",
    "    for embed in embeddings:\n",
    "        original_indices = np.linspace(0, 1, num=len(embed))\n",
    "        target_indices = np.linspace(0, 1, num=target_dim)\n",
    "        interpolated_embed = np.interp(target_indices, original_indices, embed)\n",
    "        interpolated_embeddings.append(interpolated_embed)\n",
    "    return interpolated_embeddings\n",
    "\n",
    "# PCA function to project embeddings to a fixed size\n",
    "def apply_pca(embeddings, target_dim=50):\n",
    "    interpolated_embeddings = interpolate_embeddings(embeddings, target_dim)\n",
    "    n_samples = len(interpolated_embeddings)\n",
    "    n_components = min(target_dim, n_samples)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_embeddings = pca.fit_transform(interpolated_embeddings)\n",
    "    print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"PCA components shape: {pca.components_.shape}\")\n",
    "    return np.mean(pca_embeddings, axis=0)\n",
    "\n",
    "# Mean pooling function\n",
    "def mean_pooling(embeddings, target_dim=50):\n",
    "    interpolated_embeddings = interpolate_embeddings(embeddings, target_dim)\n",
    "    return np.mean(interpolated_embeddings, axis=0)\n",
    "\n",
    "# Hybrid approach: PCA then Mean pooling\n",
    "def hybrid_pca_mean_pooling(embeddings, target_dim=50):\n",
    "    pca_embedding = apply_pca(embeddings, target_dim)\n",
    "    return mean_pooling([pca_embedding], target_dim)\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(filepath):\n",
    "    data = load_embeddings(filepath)\n",
    "    embeddings = [item['embedding'] for item in data['embeddings']]\n",
    "\n",
    "    # Apply PCA\n",
    "    pca_embedding = apply_pca(embeddings, target_dim=50)\n",
    "    print(f\"PCA embedding shape: {pca_embedding.shape}\")\n",
    "\n",
    "    # Apply Mean Pooling\n",
    "    mean_pooled_embedding = mean_pooling(embeddings, target_dim=50)\n",
    "    print(f\"Mean pooling embedding shape: {mean_pooled_embedding.shape}\")\n",
    "\n",
    "    # Apply Hybrid PCA then Mean Pooling\n",
    "    hybrid_embedding = hybrid_pca_mean_pooling(embeddings, target_dim=50)\n",
    "    print(f\"Hybrid embedding shape: {hybrid_embedding.shape}\")\n",
    "\n",
    "    # Add combined embeddings to the data\n",
    "    data['combined'] = {\n",
    "        'PCA': pca_embedding.tolist(),\n",
    "        'MeanPooling': mean_pooled_embedding.tolist(),\n",
    "        'Hybrid': hybrid_embedding.tolist()\n",
    "    }\n",
    "\n",
    "    # Save the updated data to a new file\n",
    "    #new_filepath = filepath.replace('.json', '_updated.json')\n",
    "    #shutil.copyfile(filepath, filepath)  # Make a copy of the original file\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    \n",
    "    return new_filepath\n",
    "\n",
    "# Function to process all files in a directory\n",
    "def process_directory(directory):\n",
    "    updated_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            updated_filepath = process_file(filepath)\n",
    "            updated_files.append(updated_filepath)\n",
    "    return updated_files\n",
    "\n",
    "# Function to test the structure and shape of the final file\n",
    "def test_final_file(filepath, expected_shape=50):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    assert 'combined' in data, \"Missing 'combined' key in JSON file\"\n",
    "    combined = data['combined']\n",
    "\n",
    "    for key in ['MeanPooling', 'Hybrid']:\n",
    "        assert key in combined, f\"Missing '{key}' key in 'combined'\"\n",
    "        assert len(combined[key]) == expected_shape, f\"Incorrect shape for '{key}' embedding\"\n",
    "\n",
    "    # Handle PCA embedding separately\n",
    "    assert 'PCA' in combined, f\"Missing 'PCA' key in 'combined'\"\n",
    "    pca_embedding_shape = len(combined['PCA'])\n",
    "    assert pca_embedding_shape > 0, f\"Incorrect shape for 'PCA' embedding\"\n",
    "\n",
    "    print(f\"File {filepath} passed the tests.\")\n",
    "\n",
    "# Path to the directory containing the JSON files\n",
    "directory_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/test'\n",
    "\n",
    "# Process all files in the directory\n",
    "updated_files = process_directory(directory_path)\n",
    "\n",
    "# Test the first updated file to ensure it is correctly formatted\n",
    "test_final_file(updated_files[0])\n",
    "\n",
    "# Display the combined embeddings\n",
    "updated_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aefa6e-d69f-45e3-a889-24c40fd9d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, AdamW\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "NUM_TEXTS = 160\n",
    "MAX_LENGTH = 300\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Step 1: Generate Fake Data\n",
    "fake_embeddings = [torch.tensor(np.random.rand(EMBEDDING_DIM), dtype=torch.float32) for _ in range(NUM_TEXTS * 2)]\n",
    "\n",
    "# Generate random text sequences from the tokenizer's vocabulary\n",
    "def generate_random_texts(tokenizer, num_texts, max_length):\n",
    "    texts = []\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    for _ in range(num_texts):\n",
    "        random_tokens = np.random.randint(0, vocab_size, size=(max_length,))\n",
    "        texts.append(tokenizer.decode(random_tokens, skip_special_tokens=True))\n",
    "    return texts\n",
    "\n",
    "# Step 2: Initialize Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the padding token\n",
    "\n",
    "fake_texts = generate_random_texts(tokenizer, num_texts=NUM_TEXTS, max_length=MAX_LENGTH)\n",
    "\n",
    "# Convert Text to Tokens\n",
    "tokenized_texts = [tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=MAX_LENGTH)['input_ids'].squeeze(0) for text in fake_texts]\n",
    "\n",
    "# Step 3: Create Dataset\n",
    "class EmbeddingTextDataset(Dataset):\n",
    "    def __init__(self, embeddings, tokenized_texts):\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.tokenized_texts[idx]\n",
    "\n",
    "dataset = EmbeddingTextDataset(fake_embeddings, tokenized_texts)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(\"dataset made\")\n",
    "# Step 4: Define the Model\n",
    "class SimpleTextGenerator(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim, max_length):\n",
    "        super(SimpleTextGenerator, self).__init__()\n",
    "        self.embedding_projection = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size * max_length)\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, embedding):\n",
    "        projected_embedding = self.embedding_projection(embedding)\n",
    "        output = self.fc(projected_embedding)\n",
    "        output = output.view(-1, self.max_length, self.vocab_size)\n",
    "        return output\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = SimpleTextGenerator(EMBEDDING_DIM, vocab_size, HIDDEN_DIM, MAX_LENGTH)\n",
    "print(\"model made\")\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Step 5: Train the Model\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for embeddings, tokenized_texts in dataloader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            tokenized_texts = tokenized_texts.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(embeddings)\n",
    "            loss = criterion(logits.view(-1, vocab_size), tokenized_texts.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "train(model, dataloader, optimizer, criterion, device, NUM_EPOCHS)\n",
    "\n",
    "# Step 6: Generate and Print Text in One Go\n",
    "def generate_text_from_embedding(model, embedding, tokenizer, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = embedding.to(device)\n",
    "        logits = model(embedding)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        generated_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "# Test the model with a new embedding\n",
    "test_embedding = torch.tensor(np.random.rand(EMBEDDING_DIM), dtype=torch.float32).to(device)\n",
    "generated_text = generate_text_from_embedding(model, test_embedding, tokenizer, MAX_LENGTH)\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd793256-9ccf-4fec-9d2e-0e6a8c1f2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def extract_embeddings(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    uuid = data.get('uuid')\n",
    "    combined = data.get('combined', {})\n",
    "    mean_pooling = combined.get('MeanPooling', [])\n",
    "    hybrid = combined.get('Hybrid', [])\n",
    "\n",
    "    return uuid, mean_pooling, hybrid\n",
    "\n",
    "def process_directory(directory):\n",
    "    embeddings_dict = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('_updated.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            uuid, mean_pooling, hybrid = extract_embeddings(filepath)\n",
    "            if uuid is not None:\n",
    "                embeddings_dict[uuid] = {\n",
    "                    'MeanPooling': mean_pooling,\n",
    "                    'Hybrid': hybrid\n",
    "                }\n",
    "        break\n",
    "    return embeddings_dict\n",
    "\n",
    "def load_response_list(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def combine_data(embeddings_dict, response_list):\n",
    "    combined_dict = {}\n",
    "    for response in response_list:\n",
    "        uuid = response.get('uuid')\n",
    "        if uuid in embeddings_dict:\n",
    "            combined_dict[uuid] = {\n",
    "                'response_content': response.get('response_content'),\n",
    "                'MeanPooling': embeddings_dict[uuid]['MeanPooling'],\n",
    "                'Hybrid': embeddings_dict[uuid]['Hybrid']\n",
    "            }\n",
    "    return combined_dict\n",
    "\n",
    "# Path to the directory containing the updated JSON files\n",
    "directory_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/test'\n",
    "\n",
    "# Path to the uuid_response_list.json file\n",
    "response_list_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/uuid_response_list.json'\n",
    "\n",
    "# Process all files in the directory to extract embeddings\n",
    "embeddings_dict = process_directory(directory_path)\n",
    "\n",
    "# Load the response list\n",
    "response_list = load_response_list(response_list_path)\n",
    "\n",
    "# Combine the embeddings with the response content\n",
    "combined_data = combine_data(embeddings_dict, response_list)\n",
    "\n",
    "# Display the resulting combined dictionary\n",
    "print(combined_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0dbfa74-8964-4326-b7bd-d9542af09048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points found: 200\n",
      "Example UUID: 18a52da9-81c3-4104-b11f-e8750e432531\n",
      "Example Response Content: The biggest cat in the world is the Siberian tiger, also known as the Amur tiger. They are the largest of all tiger species and can weigh up to 660 pounds (300 kg) and grow up to 10 feet (3 meters) in length.\n",
      "Example Embedding (MeanPooling): [0.012374771758913994, -0.012974258351890458, 0.0010368387043779136, -0.010370376044693822, 0.007631063993487836]...\n",
      "Example Embedding (Hybrid): [-6.938893903907228e-18, -6.60374802053116e-18, -6.2686021371550914e-18, -5.933456253779023e-18, -5.5983103704029545e-18]...\n",
      "Training data points: 160\n",
      "Test data points: 40\n",
      "Original text: The biggest cat in the world is the Siberian tiger, also known as the Amur tiger. They are the largest of all tiger species and can weigh up to 660 pounds (300 kg) and grow up to 10 feet (3 meters) in length.\n",
      "Tokenized and decoded text: The biggest cat in the world is the Siberian tiger, also known as the Amur tiger. They are the largest of all tiger species and can weigh up to 660 pounds (300 kg) and grow up to 10 feet (3 meters) in length.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x50 and 100x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 232\u001b[0m\n\u001b[1;32m    230\u001b[0m embeddings_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    231\u001b[0m response_list_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/uuid_response_list.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 232\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 219\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Evaluate the model with the test set\u001b[39;00m\n\u001b[1;32m    222\u001b[0m test_embeddings \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(test_data[key][EMBEDDING_TYPE], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m test_data]\n",
      "Cell \u001b[0;32mIn[6], line 154\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, device, epochs, vocab_size)\u001b[0m\n\u001b[1;32m    151\u001b[0m tokenized_texts \u001b[38;5;241m=\u001b[39m tokenized_texts\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    153\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 154\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), tokenized_texts\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    156\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 130\u001b[0m, in \u001b[0;36mSimpleTextGenerator.forward\u001b[0;34m(self, embedding)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, embedding):\n\u001b[0;32m--> 130\u001b[0m     projected_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(projected_embedding)\n\u001b[1;32m    132\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x50 and 100x128)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, AdamW\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration variables\n",
    "SEED = 42\n",
    "NUM_TEXTS = 160\n",
    "MAX_LENGTH = 50\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "EMBEDDING_TYPE = 'Hybrid'  # Options: 'MeanPooling', 'Hybrid'\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# Step 1: Load Embeddings and Response Content\n",
    "def extract_embeddings(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    uuid = data.get('uuid')\n",
    "    combined = data.get('combined', {})\n",
    "    mean_pooling = combined.get('MeanPooling', [])\n",
    "    hybrid = combined.get('Hybrid', [])\n",
    "\n",
    "    return uuid, mean_pooling, hybrid\n",
    "\n",
    "def process_directory(directory):\n",
    "    embeddings_dict = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('_updated.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            uuid, mean_pooling, hybrid = extract_embeddings(filepath)\n",
    "            if uuid is not None:\n",
    "                embeddings_dict[uuid] = {\n",
    "                    'MeanPooling': mean_pooling,\n",
    "                    'Hybrid': hybrid\n",
    "                }\n",
    "    return embeddings_dict\n",
    "\n",
    "def load_response_list(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def combine_data(embeddings_dict, response_list):\n",
    "    combined_dict = {}\n",
    "    for response in response_list:\n",
    "        uuid = response.get('uuid')\n",
    "        if uuid in embeddings_dict:\n",
    "            combined_dict[uuid] = {\n",
    "                'response_content': response.get('response_content'),\n",
    "                'MeanPooling': embeddings_dict[uuid]['MeanPooling'],\n",
    "                'Hybrid': embeddings_dict[uuid]['Hybrid']\n",
    "            }\n",
    "    return combined_dict\n",
    "\n",
    "def load_combined_data(embeddings_path, response_list_path):\n",
    "    embeddings_dict = process_directory(embeddings_path)\n",
    "    response_list = load_response_list(response_list_path)\n",
    "    combined_data = combine_data(embeddings_dict, response_list)\n",
    "    return combined_data\n",
    "\n",
    "def print_intermediate_info(combined_data):\n",
    "    print(f\"Number of data points found: {len(combined_data)}\")\n",
    "    if combined_data:\n",
    "        example_uuid, example_data = next(iter(combined_data.items()))\n",
    "        print(f\"Example UUID: {example_uuid}\")\n",
    "        print(f\"Example Response Content: {example_data['response_content']}\")\n",
    "        print(f\"Example Embedding (MeanPooling): {example_data['MeanPooling'][:5]}...\")  # print first 5 elements\n",
    "        print(f\"Example Embedding (Hybrid): {example_data['Hybrid'][:5]}...\")  # print first 5 elements\n",
    "\n",
    "# Step 2: Initialize Tokenizer\n",
    "def initialize_tokenizer():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set the padding token\n",
    "    return tokenizer\n",
    "\n",
    "def process_texts(texts, tokenizer, max_length):\n",
    "    tokenized_texts = [tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)['input_ids'].squeeze(0) for text in texts]\n",
    "    return tokenized_texts\n",
    "\n",
    "# Step 3: Create Dataset\n",
    "class EmbeddingTextDataset(Dataset):\n",
    "    def __init__(self, combined_data, tokenized_texts, embedding_type):\n",
    "        self.combined_data = combined_data\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.embedding_type = embedding_type\n",
    "        self.keys = list(combined_data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        uuid = self.keys[idx]\n",
    "        embedding = self.combined_data[uuid][self.embedding_type]\n",
    "        embedding = torch.tensor(embedding, dtype=torch.float32)\n",
    "        tokenized_text = self.tokenized_texts[idx]\n",
    "        return embedding, tokenized_text\n",
    "\n",
    "def create_dataloader(combined_data, tokenizer, embedding_type, batch_size):\n",
    "    texts = [item['response_content'] for item in combined_data.values()]\n",
    "    tokenized_texts = process_texts(texts, tokenizer, MAX_LENGTH)\n",
    "    dataset = EmbeddingTextDataset(combined_data, tokenized_texts, embedding_type)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# Step 4: Define the Model\n",
    "class SimpleTextGenerator(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim, max_length):\n",
    "        super(SimpleTextGenerator, self).__init__()\n",
    "        self.embedding_projection = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size * max_length)\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, embedding):\n",
    "        projected_embedding = self.embedding_projection(embedding)\n",
    "        output = self.fc(projected_embedding)\n",
    "        output = output.view(-1, self.max_length, self.vocab_size)\n",
    "        return output\n",
    "\n",
    "def initialize_model(embedding_dim, vocab_size, hidden_dim, max_length):\n",
    "    model = SimpleTextGenerator(embedding_dim, vocab_size, hidden_dim, max_length)\n",
    "    return model\n",
    "\n",
    "def move_model_to_device(model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return model, device\n",
    "\n",
    "# Step 5: Train the Model\n",
    "def train(model, dataloader, optimizer, criterion, device, epochs, vocab_size):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for embeddings, tokenized_texts in dataloader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            tokenized_texts = tokenized_texts.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(embeddings)\n",
    "            loss = criterion(logits.view(-1, vocab_size), tokenized_texts.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "# Step 6: Generate and Print Text in One Go\n",
    "def generate_text_from_embedding(model, embedding, tokenizer, max_length, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = embedding.to(device)\n",
    "        logits = model(embedding)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        generated_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "# Step 7: Tokenizer Test\n",
    "def tokenizer_test(tokenizer, example_text, max_length):\n",
    "    print(f\"Original text: {example_text}\")\n",
    "    tokenized_text = tokenizer(example_text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)['input_ids'].squeeze(0)\n",
    "    decoded_text = tokenizer.decode(tokenized_text, skip_special_tokens=True)\n",
    "    print(f\"Tokenized and decoded text: {decoded_text}\")\n",
    "\n",
    "def split_data(combined_data, test_size=0.2):\n",
    "    keys = list(combined_data.keys())\n",
    "    train_keys, test_keys = train_test_split(keys, test_size=test_size, random_state=SEED)\n",
    "    train_data = {key: combined_data[key] for key in train_keys}\n",
    "    test_data = {key: combined_data[key] for key in test_keys}\n",
    "    return train_data, test_data\n",
    "\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # Load combined data\n",
    "    combined_data = load_combined_data(embeddings_path, response_list_path)\n",
    "    print_intermediate_info(combined_data)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    train_data, test_data = split_data(combined_data)\n",
    "    print(f\"Training data points: {len(train_data)}\")\n",
    "    print(f\"Test data points: {len(test_data)}\")\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = initialize_tokenizer()\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # Tokenizer test with an example text\n",
    "    example_text = next(iter(combined_data.values()))['response_content']\n",
    "    tokenizer_test(tokenizer, example_text, MAX_LENGTH)\n",
    "\n",
    "    # Create dataloader for training data\n",
    "    train_dataloader = create_dataloader(train_data, tokenizer, EMBEDDING_TYPE, BATCH_SIZE)\n",
    "\n",
    "    # Initialize and move model to device\n",
    "    model = initialize_model(EMBEDDING_DIM, vocab_size, HIDDEN_DIM, MAX_LENGTH)\n",
    "    model, device = move_model_to_device(model)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    train(model, train_dataloader, optimizer, criterion, device, NUM_EPOCHS, vocab_size)\n",
    "\n",
    "    # Evaluate the model with the test set\n",
    "    test_embeddings = [torch.tensor(test_data[key][EMBEDDING_TYPE], dtype=torch.float32).to(device) for key in test_data]\n",
    "    for i, test_embedding in enumerate(test_embeddings):\n",
    "        generated_text = generate_text_from_embedding(model, test_embedding, tokenizer, MAX_LENGTH, device)\n",
    "        original_text = test_data[list(test_data.keys())[i]]['response_content']\n",
    "        print(f\"Original text: {original_text}\")\n",
    "        print(f\"Generated text: {generated_text}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    embeddings_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/test'\n",
    "    response_list_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/uuid_response_list.json'\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9a59a-60e4-406d-b303-94120dcaf31f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ea558-6f2a-4813-b044-8fad7f207973",
   "metadata": {},
   "outputs": [],
   "source": [
    "/workspace/slice-monorepo/thebeast/combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987aa8c-f177-46ac-a904-5f453a64a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn tensorflow transformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75213e8d-0e1a-4447-8b2d-d387ec96b148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50000 | Data size: 100.5 | Train Loss: 0.00560266 | Val Loss: 0.00230649 | Train Acc: 0.32% | Val Acc: 0.25%\n",
      "Epoch 2/50000 | Data size: 101.0025 | Train Loss: 0.00212276 | Val Loss: 0.00153161 | Train Acc: -0.09% | Val Acc: 1.19%\n",
      "Epoch 3/50000 | Data size: 101.5075125 | Train Loss: 0.00153863 | Val Loss: 0.00122420 | Train Acc: 1.90% | Val Acc: 3.05%\n",
      "Epoch 4/50000 | Data size: 102.01505006250001 | Train Loss: 0.00127509 | Val Loss: 0.00106259 | Train Acc: 3.17% | Val Acc: 3.83%\n",
      "Epoch 5/50000 | Data size: 102.5251253128125 | Train Loss: 0.00112968 | Val Loss: 0.00093658 | Train Acc: 3.86% | Val Acc: 4.46%\n",
      "Epoch 6/50000 | Data size: 103.03775093937656 | Train Loss: 0.00098667 | Val Loss: 0.00082781 | Train Acc: 4.70% | Val Acc: 5.31%\n",
      "Epoch 7/50000 | Data size: 103.55293969407344 | Train Loss: 0.00087665 | Val Loss: 0.00071954 | Train Acc: 5.11% | Val Acc: 7.00%\n",
      "Epoch 8/50000 | Data size: 104.07070439254382 | Train Loss: 0.00076832 | Val Loss: 0.00062549 | Train Acc: 6.78% | Val Acc: 9.10%\n",
      "Epoch 9/50000 | Data size: 104.59105791450654 | Train Loss: 0.00067286 | Val Loss: 0.00054710 | Train Acc: 8.56% | Val Acc: 11.01%\n",
      "Epoch 10/50000 | Data size: 105.11401320407907 | Train Loss: 0.00059138 | Val Loss: 0.00048253 | Train Acc: 10.07% | Val Acc: 13.19%\n",
      "Epoch 11/50000 | Data size: 105.63958327009946 | Train Loss: 0.00052765 | Val Loss: 0.00043265 | Train Acc: 12.73% | Val Acc: 16.10%\n",
      "Epoch 12/50000 | Data size: 106.16778118644996 | Train Loss: 0.00048058 | Val Loss: 0.00039552 | Train Acc: 14.81% | Val Acc: 19.23%\n",
      "Epoch 13/50000 | Data size: 106.69862009238221 | Train Loss: 0.00044239 | Val Loss: 0.00036649 | Train Acc: 16.80% | Val Acc: 21.88%\n",
      "Epoch 14/50000 | Data size: 107.23211319284412 | Train Loss: 0.00041664 | Val Loss: 0.00034807 | Train Acc: 18.25% | Val Acc: 24.09%\n",
      "Epoch 15/50000 | Data size: 107.76827375880835 | Train Loss: 0.00039402 | Val Loss: 0.00033324 | Train Acc: 20.48% | Val Acc: 26.78%\n",
      "Epoch 16/50000 | Data size: 108.30711512760239 | Train Loss: 0.00038593 | Val Loss: 0.00032462 | Train Acc: 21.17% | Val Acc: 28.46%\n",
      "Epoch 17/50000 | Data size: 108.8486507032404 | Train Loss: 0.00037479 | Val Loss: 0.00031943 | Train Acc: 21.97% | Val Acc: 29.30%\n",
      "Epoch 18/50000 | Data size: 109.3928939567566 | Train Loss: 0.00036448 | Val Loss: 0.00031246 | Train Acc: 23.15% | Val Acc: 30.72%\n",
      "Epoch 19/50000 | Data size: 109.93985842654038 | Train Loss: 0.00036121 | Val Loss: 0.00030963 | Train Acc: 23.32% | Val Acc: 31.09%\n",
      "Epoch 20/50000 | Data size: 110.48955771867308 | Train Loss: 0.00035567 | Val Loss: 0.00030640 | Train Acc: 24.14% | Val Acc: 31.85%\n",
      "Epoch 21/50000 | Data size: 111.04200550726645 | Train Loss: 0.00035358 | Val Loss: 0.00030385 | Train Acc: 24.10% | Val Acc: 32.40%\n",
      "Epoch 22/50000 | Data size: 111.59721553480279 | Train Loss: 0.00034687 | Val Loss: 0.00030083 | Train Acc: 25.17% | Val Acc: 32.69%\n",
      "Epoch 23/50000 | Data size: 112.1552016124768 | Train Loss: 0.00034032 | Val Loss: 0.00029823 | Train Acc: 26.18% | Val Acc: 33.30%\n",
      "Epoch 24/50000 | Data size: 112.71597762053919 | Train Loss: 0.00033800 | Val Loss: 0.00029617 | Train Acc: 26.44% | Val Acc: 33.87%\n",
      "Epoch 25/50000 | Data size: 113.27955750864189 | Train Loss: 0.00033768 | Val Loss: 0.00029312 | Train Acc: 26.20% | Val Acc: 34.36%\n",
      "Epoch 26/50000 | Data size: 113.8459552961851 | Train Loss: 0.00033159 | Val Loss: 0.00029126 | Train Acc: 27.30% | Val Acc: 34.66%\n",
      "Epoch 27/50000 | Data size: 114.41518507266602 | Train Loss: 0.00033179 | Val Loss: 0.00028915 | Train Acc: 26.73% | Val Acc: 35.24%\n",
      "Epoch 28/50000 | Data size: 114.98726099802936 | Train Loss: 0.00032527 | Val Loss: 0.00028700 | Train Acc: 28.13% | Val Acc: 35.66%\n",
      "Epoch 29/50000 | Data size: 115.5621973030195 | Train Loss: 0.00032639 | Val Loss: 0.00028523 | Train Acc: 27.88% | Val Acc: 35.98%\n",
      "Epoch 30/50000 | Data size: 116.1400082895346 | Train Loss: 0.00031964 | Val Loss: 0.00028405 | Train Acc: 29.04% | Val Acc: 36.16%\n",
      "Epoch 31/50000 | Data size: 116.72070833098228 | Train Loss: 0.00031793 | Val Loss: 0.00028128 | Train Acc: 29.15% | Val Acc: 36.92%\n",
      "Epoch 32/50000 | Data size: 117.30431187263719 | Train Loss: 0.00031372 | Val Loss: 0.00027933 | Train Acc: 30.23% | Val Acc: 37.41%\n",
      "Epoch 33/50000 | Data size: 117.89083343200038 | Train Loss: 0.00031041 | Val Loss: 0.00027792 | Train Acc: 30.78% | Val Acc: 37.72%\n",
      "Epoch 34/50000 | Data size: 118.48028759916038 | Train Loss: 0.00031080 | Val Loss: 0.00027639 | Train Acc: 30.47% | Val Acc: 38.00%\n",
      "Epoch 35/50000 | Data size: 119.07268903715618 | Train Loss: 0.00030557 | Val Loss: 0.00027425 | Train Acc: 31.65% | Val Acc: 38.67%\n",
      "Epoch 36/50000 | Data size: 119.66805248234196 | Train Loss: 0.00030706 | Val Loss: 0.00027272 | Train Acc: 31.07% | Val Acc: 38.95%\n",
      "Epoch 37/50000 | Data size: 120.26639274475367 | Train Loss: 0.00030427 | Val Loss: 0.00027101 | Train Acc: 31.47% | Val Acc: 39.26%\n",
      "Epoch 38/50000 | Data size: 120.86772470847744 | Train Loss: 0.00029893 | Val Loss: 0.00026997 | Train Acc: 32.77% | Val Acc: 39.52%\n",
      "Epoch 39/50000 | Data size: 121.47206333201983 | Train Loss: 0.00029761 | Val Loss: 0.00026814 | Train Acc: 33.11% | Val Acc: 40.05%\n",
      "Epoch 40/50000 | Data size: 122.07942364867993 | Train Loss: 0.00029791 | Val Loss: 0.00026622 | Train Acc: 32.90% | Val Acc: 40.47%\n",
      "Epoch 41/50000 | Data size: 122.68982076692333 | Train Loss: 0.00029523 | Val Loss: 0.00026496 | Train Acc: 33.37% | Val Acc: 40.89%\n",
      "Epoch 42/50000 | Data size: 123.30326987075794 | Train Loss: 0.00029264 | Val Loss: 0.00026361 | Train Acc: 34.06% | Val Acc: 41.12%\n",
      "Epoch 43/50000 | Data size: 123.91978622011173 | Train Loss: 0.00029336 | Val Loss: 0.00026181 | Train Acc: 33.64% | Val Acc: 41.45%\n",
      "Epoch 44/50000 | Data size: 124.53938515121229 | Train Loss: 0.00028843 | Val Loss: 0.00026009 | Train Acc: 34.80% | Val Acc: 41.89%\n",
      "Epoch 45/50000 | Data size: 125.16208207696835 | Train Loss: 0.00028543 | Val Loss: 0.00025910 | Train Acc: 35.48% | Val Acc: 42.07%\n",
      "Epoch 46/50000 | Data size: 125.78789248735319 | Train Loss: 0.00028527 | Val Loss: 0.00025730 | Train Acc: 35.44% | Val Acc: 42.61%\n",
      "Epoch 47/50000 | Data size: 126.41683194978995 | Train Loss: 0.00028118 | Val Loss: 0.00025547 | Train Acc: 36.48% | Val Acc: 43.03%\n",
      "Epoch 48/50000 | Data size: 127.04891610953891 | Train Loss: 0.00028016 | Val Loss: 0.00025424 | Train Acc: 36.71% | Val Acc: 43.22%\n",
      "Epoch 49/50000 | Data size: 127.6841606900866 | Train Loss: 0.00028123 | Val Loss: 0.00025260 | Train Acc: 36.14% | Val Acc: 43.62%\n",
      "Epoch 50/50000 | Data size: 128.32258149353703 | Train Loss: 0.00027546 | Val Loss: 0.00025104 | Train Acc: 37.71% | Val Acc: 44.01%\n",
      "Epoch 51/50000 | Data size: 128.9641944010047 | Train Loss: 0.00027410 | Val Loss: 0.00024976 | Train Acc: 37.91% | Val Acc: 44.23%\n",
      "Epoch 52/50000 | Data size: 129.60901537300973 | Train Loss: 0.00027194 | Val Loss: 0.00024816 | Train Acc: 38.41% | Val Acc: 44.61%\n",
      "Epoch 53/50000 | Data size: 130.25706044987479 | Train Loss: 0.00027015 | Val Loss: 0.00024640 | Train Acc: 38.67% | Val Acc: 44.93%\n",
      "Epoch 54/50000 | Data size: 130.90834575212415 | Train Loss: 0.00026654 | Val Loss: 0.00024516 | Train Acc: 39.84% | Val Acc: 45.24%\n",
      "Epoch 55/50000 | Data size: 131.56288748088477 | Train Loss: 0.00026887 | Val Loss: 0.00024312 | Train Acc: 38.83% | Val Acc: 45.58%\n",
      "Epoch 56/50000 | Data size: 132.2207019182892 | Train Loss: 0.00026805 | Val Loss: 0.00024180 | Train Acc: 39.12% | Val Acc: 45.78%\n",
      "Epoch 57/50000 | Data size: 132.88180542788064 | Train Loss: 0.00026298 | Val Loss: 0.00023896 | Train Acc: 40.06% | Val Acc: 46.35%\n",
      "Epoch 58/50000 | Data size: 133.54621445502005 | Train Loss: 0.00026180 | Val Loss: 0.00023734 | Train Acc: 40.11% | Val Acc: 46.69%\n",
      "Epoch 59/50000 | Data size: 134.21394552729515 | Train Loss: 0.00025635 | Val Loss: 0.00023526 | Train Acc: 41.55% | Val Acc: 47.29%\n",
      "Epoch 60/50000 | Data size: 134.88501525493163 | Train Loss: 0.00026272 | Val Loss: 0.00023304 | Train Acc: 39.95% | Val Acc: 47.74%\n",
      "Epoch 61/50000 | Data size: 135.55944033120628 | Train Loss: 0.00025102 | Val Loss: 0.00023118 | Train Acc: 42.52% | Val Acc: 47.91%\n",
      "Epoch 62/50000 | Data size: 136.2372375328623 | Train Loss: 0.00024960 | Val Loss: 0.00022902 | Train Acc: 42.77% | Val Acc: 48.55%\n",
      "Epoch 63/50000 | Data size: 136.91842372052662 | Train Loss: 0.00025168 | Val Loss: 0.00022708 | Train Acc: 42.26% | Val Acc: 48.86%\n",
      "Epoch 64/50000 | Data size: 137.60301583912926 | Train Loss: 0.00024764 | Val Loss: 0.00022575 | Train Acc: 43.18% | Val Acc: 49.14%\n",
      "Epoch 65/50000 | Data size: 138.2910309183249 | Train Loss: 0.00024561 | Val Loss: 0.00022350 | Train Acc: 43.51% | Val Acc: 49.57%\n",
      "Epoch 66/50000 | Data size: 138.98248607291654 | Train Loss: 0.00024232 | Val Loss: 0.00022163 | Train Acc: 44.24% | Val Acc: 50.00%\n",
      "Epoch 67/50000 | Data size: 139.67739850328113 | Train Loss: 0.00023935 | Val Loss: 0.00021976 | Train Acc: 44.81% | Val Acc: 50.31%\n",
      "Epoch 68/50000 | Data size: 140.37578549579754 | Train Loss: 0.00023656 | Val Loss: 0.00021780 | Train Acc: 45.44% | Val Acc: 50.75%\n",
      "Epoch 69/50000 | Data size: 141.07766442327653 | Train Loss: 0.00024030 | Val Loss: 0.00021624 | Train Acc: 44.52% | Val Acc: 51.12%\n",
      "Epoch 70/50000 | Data size: 141.78305274539292 | Train Loss: 0.00023645 | Val Loss: 0.00021470 | Train Acc: 45.39% | Val Acc: 51.27%\n",
      "Epoch 71/50000 | Data size: 142.4919680091199 | Train Loss: 0.00023126 | Val Loss: 0.00021300 | Train Acc: 46.21% | Val Acc: 51.55%\n",
      "Epoch 72/50000 | Data size: 143.20442784916548 | Train Loss: 0.00023107 | Val Loss: 0.00021114 | Train Acc: 46.30% | Val Acc: 52.00%\n",
      "Epoch 73/50000 | Data size: 143.92044998841132 | Train Loss: 0.00023170 | Val Loss: 0.00020968 | Train Acc: 46.19% | Val Acc: 52.44%\n",
      "Epoch 74/50000 | Data size: 144.64005223835338 | Train Loss: 0.00022514 | Val Loss: 0.00020831 | Train Acc: 47.63% | Val Acc: 52.49%\n",
      "Epoch 75/50000 | Data size: 145.36325249954515 | Train Loss: 0.00022709 | Val Loss: 0.00020671 | Train Acc: 47.12% | Val Acc: 52.91%\n",
      "Epoch 76/50000 | Data size: 146.09006876204288 | Train Loss: 0.00022818 | Val Loss: 0.00020535 | Train Acc: 46.82% | Val Acc: 53.24%\n",
      "Epoch 77/50000 | Data size: 146.8205191058531 | Train Loss: 0.00022252 | Val Loss: 0.00020416 | Train Acc: 48.09% | Val Acc: 53.34%\n",
      "Epoch 78/50000 | Data size: 147.55462170138236 | Train Loss: 0.00021990 | Val Loss: 0.00020268 | Train Acc: 48.62% | Val Acc: 53.73%\n",
      "Epoch 79/50000 | Data size: 148.29239480988926 | Train Loss: 0.00021589 | Val Loss: 0.00020133 | Train Acc: 49.54% | Val Acc: 54.09%\n",
      "Epoch 80/50000 | Data size: 149.03385678393872 | Train Loss: 0.00022222 | Val Loss: 0.00020020 | Train Acc: 48.26% | Val Acc: 54.27%\n",
      "Epoch 81/50000 | Data size: 149.7790260678584 | Train Loss: 0.00021676 | Val Loss: 0.00019882 | Train Acc: 49.19% | Val Acc: 54.60%\n",
      "Epoch 82/50000 | Data size: 150.5279211981977 | Train Loss: 0.00021499 | Val Loss: 0.00019765 | Train Acc: 49.69% | Val Acc: 54.83%\n",
      "Epoch 83/50000 | Data size: 151.2805608041887 | Train Loss: 0.00021580 | Val Loss: 0.00019655 | Train Acc: 49.44% | Val Acc: 55.12%\n",
      "Epoch 84/50000 | Data size: 152.03696360820965 | Train Loss: 0.00021043 | Val Loss: 0.00019566 | Train Acc: 50.54% | Val Acc: 55.20%\n",
      "Epoch 85/50000 | Data size: 152.7971484262507 | Train Loss: 0.00020894 | Val Loss: 0.00019468 | Train Acc: 51.08% | Val Acc: 55.43%\n",
      "Epoch 86/50000 | Data size: 153.56113416838193 | Train Loss: 0.00020974 | Val Loss: 0.00019355 | Train Acc: 50.94% | Val Acc: 55.70%\n",
      "Epoch 87/50000 | Data size: 154.32893983922384 | Train Loss: 0.00020577 | Val Loss: 0.00019263 | Train Acc: 51.74% | Val Acc: 55.87%\n",
      "Epoch 88/50000 | Data size: 155.10058453841995 | Train Loss: 0.00020894 | Val Loss: 0.00019158 | Train Acc: 51.06% | Val Acc: 56.22%\n",
      "Epoch 89/50000 | Data size: 155.87608746111206 | Train Loss: 0.00020515 | Val Loss: 0.00019083 | Train Acc: 51.98% | Val Acc: 56.27%\n",
      "Epoch 90/50000 | Data size: 156.6554678984176 | Train Loss: 0.00020669 | Val Loss: 0.00018978 | Train Acc: 51.52% | Val Acc: 56.66%\n",
      "Epoch 91/50000 | Data size: 157.4387452379097 | Train Loss: 0.00020578 | Val Loss: 0.00018905 | Train Acc: 51.78% | Val Acc: 56.70%\n",
      "Epoch 92/50000 | Data size: 158.22593896409924 | Train Loss: 0.00020410 | Val Loss: 0.00018806 | Train Acc: 52.09% | Val Acc: 57.05%\n",
      "Epoch 93/50000 | Data size: 159.01706865891973 | Train Loss: 0.00020677 | Val Loss: 0.00018732 | Train Acc: 51.63% | Val Acc: 57.17%\n",
      "Epoch 94/50000 | Data size: 159.81215400221433 | Train Loss: 0.00020090 | Val Loss: 0.00018651 | Train Acc: 52.82% | Val Acc: 57.38%\n",
      "Epoch 95/50000 | Data size: 160.6112147722254 | Train Loss: 0.00019887 | Val Loss: 0.00018567 | Train Acc: 53.28% | Val Acc: 57.60%\n",
      "Epoch 96/50000 | Data size: 161.4142708460865 | Train Loss: 0.00019724 | Val Loss: 0.00018508 | Train Acc: 53.68% | Val Acc: 57.76%\n",
      "Epoch 97/50000 | Data size: 162.22134220031694 | Train Loss: 0.00019815 | Val Loss: 0.00018459 | Train Acc: 53.50% | Val Acc: 57.77%\n",
      "Epoch 98/50000 | Data size: 163.0324489113185 | Train Loss: 0.00019831 | Val Loss: 0.00018370 | Train Acc: 53.37% | Val Acc: 57.94%\n",
      "Epoch 99/50000 | Data size: 163.84761115587511 | Train Loss: 0.00019728 | Val Loss: 0.00018296 | Train Acc: 53.74% | Val Acc: 58.26%\n",
      "Epoch 100/50000 | Data size: 164.6668492116545 | Train Loss: 0.00020300 | Val Loss: 0.00018217 | Train Acc: 52.65% | Val Acc: 58.27%\n",
      "Epoch 101/50000 | Data size: 165.49018345771276 | Train Loss: 0.00020046 | Val Loss: 0.00018143 | Train Acc: 53.08% | Val Acc: 58.66%\n",
      "Epoch 102/50000 | Data size: 166.31763437500132 | Train Loss: 0.00019021 | Val Loss: 0.00018022 | Train Acc: 55.15% | Val Acc: 58.85%\n",
      "Epoch 103/50000 | Data size: 167.14922254687633 | Train Loss: 0.00019478 | Val Loss: 0.00017927 | Train Acc: 54.67% | Val Acc: 59.16%\n",
      "Epoch 104/50000 | Data size: 167.9849686596107 | Train Loss: 0.00019415 | Val Loss: 0.00017822 | Train Acc: 54.71% | Val Acc: 59.46%\n",
      "Epoch 105/50000 | Data size: 168.82489350290876 | Train Loss: 0.00019210 | Val Loss: 0.00017710 | Train Acc: 55.24% | Val Acc: 59.76%\n",
      "Epoch 106/50000 | Data size: 169.6690179704233 | Train Loss: 0.00019355 | Val Loss: 0.00017613 | Train Acc: 54.94% | Val Acc: 60.05%\n",
      "Epoch 107/50000 | Data size: 170.51736306027541 | Train Loss: 0.00019364 | Val Loss: 0.00017528 | Train Acc: 54.95% | Val Acc: 60.29%\n",
      "Epoch 108/50000 | Data size: 171.3699498755768 | Train Loss: 0.00018906 | Val Loss: 0.00017444 | Train Acc: 56.04% | Val Acc: 60.40%\n",
      "Epoch 109/50000 | Data size: 172.22679962495468 | Train Loss: 0.00018831 | Val Loss: 0.00017330 | Train Acc: 56.21% | Val Acc: 60.83%\n",
      "Epoch 110/50000 | Data size: 173.08793362307946 | Train Loss: 0.00018531 | Val Loss: 0.00017236 | Train Acc: 56.95% | Val Acc: 61.02%\n",
      "Epoch 111/50000 | Data size: 173.95337329119485 | Train Loss: 0.00018771 | Val Loss: 0.00017148 | Train Acc: 56.61% | Val Acc: 61.15%\n",
      "Epoch 112/50000 | Data size: 174.8231401576508 | Train Loss: 0.00018500 | Val Loss: 0.00017061 | Train Acc: 57.00% | Val Acc: 61.48%\n",
      "Epoch 113/50000 | Data size: 175.69725585843906 | Train Loss: 0.00018202 | Val Loss: 0.00016955 | Train Acc: 57.78% | Val Acc: 61.72%\n",
      "Epoch 114/50000 | Data size: 176.57574213773125 | Train Loss: 0.00017914 | Val Loss: 0.00016845 | Train Acc: 58.53% | Val Acc: 61.98%\n",
      "Epoch 115/50000 | Data size: 177.4586208484199 | Train Loss: 0.00018156 | Val Loss: 0.00016735 | Train Acc: 58.16% | Val Acc: 62.22%\n",
      "Epoch 116/50000 | Data size: 178.34591395266202 | Train Loss: 0.00018044 | Val Loss: 0.00016621 | Train Acc: 58.37% | Val Acc: 62.57%\n",
      "Epoch 117/50000 | Data size: 179.23764352242532 | Train Loss: 0.00017902 | Val Loss: 0.00016536 | Train Acc: 58.70% | Val Acc: 62.72%\n",
      "Epoch 118/50000 | Data size: 180.13383174003744 | Train Loss: 0.00017800 | Val Loss: 0.00016411 | Train Acc: 58.98% | Val Acc: 63.08%\n",
      "Epoch 119/50000 | Data size: 181.03450089873763 | Train Loss: 0.00017539 | Val Loss: 0.00016308 | Train Acc: 59.60% | Val Acc: 63.29%\n",
      "Epoch 120/50000 | Data size: 181.93967340323132 | Train Loss: 0.00017593 | Val Loss: 0.00016202 | Train Acc: 59.63% | Val Acc: 63.51%\n",
      "Epoch 121/50000 | Data size: 182.84937177024747 | Train Loss: 0.00017623 | Val Loss: 0.00016122 | Train Acc: 59.53% | Val Acc: 63.80%\n",
      "Epoch 122/50000 | Data size: 183.76361862909872 | Train Loss: 0.00017477 | Val Loss: 0.00015984 | Train Acc: 59.90% | Val Acc: 64.06%\n",
      "Epoch 123/50000 | Data size: 184.6824367222442 | Train Loss: 0.00016891 | Val Loss: 0.00015897 | Train Acc: 61.15% | Val Acc: 64.14%\n",
      "Epoch 124/50000 | Data size: 185.60584890585542 | Train Loss: 0.00016975 | Val Loss: 0.00015784 | Train Acc: 61.00% | Val Acc: 64.47%\n",
      "Epoch 125/50000 | Data size: 186.5338781503847 | Train Loss: 0.00016978 | Val Loss: 0.00015676 | Train Acc: 61.03% | Val Acc: 64.72%\n",
      "Epoch 126/50000 | Data size: 187.46654754113663 | Train Loss: 0.00017189 | Val Loss: 0.00015566 | Train Acc: 60.79% | Val Acc: 65.07%\n",
      "Epoch 127/50000 | Data size: 188.4038802788423 | Train Loss: 0.00016805 | Val Loss: 0.00015458 | Train Acc: 61.55% | Val Acc: 65.22%\n",
      "Epoch 128/50000 | Data size: 189.34589968023653 | Train Loss: 0.00016726 | Val Loss: 0.00015356 | Train Acc: 61.71% | Val Acc: 65.43%\n",
      "Epoch 129/50000 | Data size: 190.2926291786377 | Train Loss: 0.00016744 | Val Loss: 0.00015240 | Train Acc: 61.66% | Val Acc: 65.79%\n",
      "Epoch 130/50000 | Data size: 191.2440923245309 | Train Loss: 0.00016648 | Val Loss: 0.00015143 | Train Acc: 61.92% | Val Acc: 66.01%\n",
      "Epoch 131/50000 | Data size: 192.20031278615355 | Train Loss: 0.00016254 | Val Loss: 0.00015021 | Train Acc: 62.92% | Val Acc: 66.26%\n",
      "Epoch 132/50000 | Data size: 193.16131435008433 | Train Loss: 0.00016256 | Val Loss: 0.00014925 | Train Acc: 62.81% | Val Acc: 66.47%\n",
      "Epoch 133/50000 | Data size: 194.12712092183475 | Train Loss: 0.00016119 | Val Loss: 0.00014862 | Train Acc: 63.13% | Val Acc: 66.61%\n",
      "Epoch 134/50000 | Data size: 195.09775652644393 | Train Loss: 0.00016004 | Val Loss: 0.00014866 | Train Acc: 63.26% | Val Acc: 66.50%\n",
      "Epoch 135/50000 | Data size: 196.07324530907616 | Train Loss: 0.00016051 | Val Loss: 0.00014659 | Train Acc: 63.33% | Val Acc: 67.00%\n",
      "Epoch 136/50000 | Data size: 197.05361153562154 | Train Loss: 0.00016031 | Val Loss: 0.00014517 | Train Acc: 63.49% | Val Acc: 67.50%\n",
      "Epoch 137/50000 | Data size: 198.03887959329964 | Train Loss: 0.00015378 | Val Loss: 0.00014411 | Train Acc: 64.91% | Val Acc: 67.65%\n",
      "Epoch 138/50000 | Data size: 199.02907399126613 | Train Loss: 0.00015295 | Val Loss: 0.00014267 | Train Acc: 65.03% | Val Acc: 68.09%\n",
      "Epoch 139/50000 | Data size: 200.02421936122246 | Train Loss: 0.00015965 | Val Loss: 0.00014205 | Train Acc: 63.88% | Val Acc: 68.31%\n",
      "Epoch 140/50000 | Data size: 201.02434045802858 | Train Loss: 0.00015655 | Val Loss: 0.00014051 | Train Acc: 64.48% | Val Acc: 68.74%\n",
      "Epoch 141/50000 | Data size: 202.02946216031873 | Train Loss: 0.00014885 | Val Loss: 0.00013918 | Train Acc: 66.13% | Val Acc: 69.10%\n",
      "Epoch 142/50000 | Data size: 203.03960947112031 | Train Loss: 0.00014800 | Val Loss: 0.00013796 | Train Acc: 66.54% | Val Acc: 69.48%\n",
      "Epoch 143/50000 | Data size: 204.0548075184759 | Train Loss: 0.00014655 | Val Loss: 0.00013677 | Train Acc: 66.88% | Val Acc: 69.89%\n",
      "Epoch 144/50000 | Data size: 205.07508155606828 | Train Loss: 0.00015300 | Val Loss: 0.00013552 | Train Acc: 65.79% | Val Acc: 70.35%\n",
      "Epoch 145/50000 | Data size: 206.10045696384861 | Train Loss: 0.00013924 | Val Loss: 0.00013417 | Train Acc: 68.72% | Val Acc: 70.81%\n",
      "Epoch 146/50000 | Data size: 207.13095924866786 | Train Loss: 0.00014608 | Val Loss: 0.00013284 | Train Acc: 67.59% | Val Acc: 71.18%\n",
      "Epoch 147/50000 | Data size: 208.1666140449112 | Train Loss: 0.00014690 | Val Loss: 0.00013141 | Train Acc: 67.55% | Val Acc: 71.72%\n",
      "Epoch 148/50000 | Data size: 209.20744711513575 | Train Loss: 0.00014321 | Val Loss: 0.00012978 | Train Acc: 68.46% | Val Acc: 72.30%\n",
      "Epoch 149/50000 | Data size: 210.25348435071143 | Train Loss: 0.00013909 | Val Loss: 0.00012844 | Train Acc: 69.63% | Val Acc: 72.77%\n",
      "Epoch 150/50000 | Data size: 211.30475177246498 | Train Loss: 0.00013526 | Val Loss: 0.00012718 | Train Acc: 70.66% | Val Acc: 73.17%\n",
      "Epoch 151/50000 | Data size: 212.3612755313273 | Train Loss: 0.00013709 | Val Loss: 0.00012575 | Train Acc: 70.45% | Val Acc: 73.66%\n",
      "Epoch 152/50000 | Data size: 213.42308190898393 | Train Loss: 0.00013959 | Val Loss: 0.00012409 | Train Acc: 70.13% | Val Acc: 74.25%\n",
      "Epoch 153/50000 | Data size: 214.49019731852886 | Train Loss: 0.00013763 | Val Loss: 0.00012275 | Train Acc: 70.81% | Val Acc: 74.69%\n",
      "Epoch 154/50000 | Data size: 215.5626483051215 | Train Loss: 0.00013586 | Val Loss: 0.00012148 | Train Acc: 71.35% | Val Acc: 75.13%\n",
      "Epoch 155/50000 | Data size: 216.64046154664712 | Train Loss: 0.00013040 | Val Loss: 0.00012024 | Train Acc: 72.65% | Val Acc: 75.56%\n",
      "Epoch 156/50000 | Data size: 217.72366385438036 | Train Loss: 0.00013464 | Val Loss: 0.00011860 | Train Acc: 72.04% | Val Acc: 76.06%\n",
      "Epoch 157/50000 | Data size: 218.81228217365228 | Train Loss: 0.00013491 | Val Loss: 0.00011729 | Train Acc: 72.15% | Val Acc: 76.49%\n",
      "Epoch 158/50000 | Data size: 219.90634358452053 | Train Loss: 0.00012762 | Val Loss: 0.00011589 | Train Acc: 73.77% | Val Acc: 76.96%\n",
      "Epoch 159/50000 | Data size: 221.00587530244314 | Train Loss: 0.00012983 | Val Loss: 0.00011437 | Train Acc: 73.59% | Val Acc: 77.42%\n",
      "Epoch 160/50000 | Data size: 222.11090467895536 | Train Loss: 0.00012680 | Val Loss: 0.00011360 | Train Acc: 74.32% | Val Acc: 77.70%\n",
      "Epoch 161/50000 | Data size: 223.22145920235013 | Train Loss: 0.00012603 | Val Loss: 0.00011209 | Train Acc: 74.71% | Val Acc: 78.17%\n",
      "Epoch 162/50000 | Data size: 224.3375664983619 | Train Loss: 0.00011891 | Val Loss: 0.00011081 | Train Acc: 76.39% | Val Acc: 78.56%\n",
      "Epoch 163/50000 | Data size: 225.4592543308537 | Train Loss: 0.00011708 | Val Loss: 0.00010969 | Train Acc: 76.90% | Val Acc: 78.87%\n",
      "Epoch 164/50000 | Data size: 226.58655060250797 | Train Loss: 0.00011884 | Val Loss: 0.00010941 | Train Acc: 76.56% | Val Acc: 79.06%\n",
      "Epoch 165/50000 | Data size: 227.7194833555205 | Train Loss: 0.00012075 | Val Loss: 0.00010864 | Train Acc: 76.43% | Val Acc: 79.36%\n",
      "Epoch 166/50000 | Data size: 228.8580807722981 | Train Loss: 0.00011752 | Val Loss: 0.00010703 | Train Acc: 77.30% | Val Acc: 79.74%\n",
      "Epoch 167/50000 | Data size: 230.0023711761596 | Train Loss: 0.00011501 | Val Loss: 0.00010557 | Train Acc: 77.85% | Val Acc: 80.13%\n",
      "Epoch 168/50000 | Data size: 231.1523830320404 | Train Loss: 0.00011425 | Val Loss: 0.00010496 | Train Acc: 78.08% | Val Acc: 80.41%\n",
      "Epoch 169/50000 | Data size: 232.30814494720062 | Train Loss: 0.00012034 | Val Loss: 0.00010384 | Train Acc: 76.84% | Val Acc: 80.67%\n",
      "Epoch 170/50000 | Data size: 233.46968567193662 | Train Loss: 0.00011254 | Val Loss: 0.00010249 | Train Acc: 78.61% | Val Acc: 81.01%\n",
      "Epoch 171/50000 | Data size: 234.6370341002963 | Train Loss: 0.00010728 | Val Loss: 0.00010175 | Train Acc: 79.73% | Val Acc: 81.25%\n",
      "Epoch 172/50000 | Data size: 235.81021927079777 | Train Loss: 0.00011030 | Val Loss: 0.00010078 | Train Acc: 79.22% | Val Acc: 81.46%\n",
      "Epoch 173/50000 | Data size: 236.98927036715176 | Train Loss: 0.00010904 | Val Loss: 0.00010018 | Train Acc: 79.49% | Val Acc: 81.67%\n",
      "Epoch 174/50000 | Data size: 238.1742167189875 | Train Loss: 0.00011078 | Val Loss: 0.00009957 | Train Acc: 79.39% | Val Acc: 81.81%\n",
      "Epoch 175/50000 | Data size: 239.36508780258245 | Train Loss: 0.00011095 | Val Loss: 0.00009892 | Train Acc: 79.20% | Val Acc: 81.98%\n",
      "Epoch 176/50000 | Data size: 240.56191324159536 | Train Loss: 0.00010757 | Val Loss: 0.00009825 | Train Acc: 80.03% | Val Acc: 82.16%\n",
      "Epoch 177/50000 | Data size: 241.76472280780334 | Train Loss: 0.00010768 | Val Loss: 0.00009770 | Train Acc: 80.06% | Val Acc: 82.30%\n",
      "Epoch 178/50000 | Data size: 242.97354642184234 | Train Loss: 0.00011437 | Val Loss: 0.00009737 | Train Acc: 78.60% | Val Acc: 82.37%\n",
      "Epoch 179/50000 | Data size: 244.18841415395156 | Train Loss: 0.00010372 | Val Loss: 0.00009691 | Train Acc: 80.83% | Val Acc: 82.50%\n",
      "Epoch 180/50000 | Data size: 245.4093562247213 | Train Loss: 0.00009807 | Val Loss: 0.00009655 | Train Acc: 82.10% | Val Acc: 82.58%\n",
      "Epoch 181/50000 | Data size: 246.63640300584493 | Train Loss: 0.00010659 | Val Loss: 0.00009598 | Train Acc: 80.39% | Val Acc: 82.70%\n",
      "Epoch 182/50000 | Data size: 247.86958502087415 | Train Loss: 0.00010366 | Val Loss: 0.00009568 | Train Acc: 80.92% | Val Acc: 82.78%\n",
      "Epoch 183/50000 | Data size: 249.10893294597852 | Train Loss: 0.00010539 | Val Loss: 0.00009559 | Train Acc: 80.63% | Val Acc: 82.87%\n",
      "Epoch 184/50000 | Data size: 250.35447761070841 | Train Loss: 0.00010394 | Val Loss: 0.00009501 | Train Acc: 81.01% | Val Acc: 82.95%\n",
      "Epoch 185/50000 | Data size: 251.60624999876197 | Train Loss: 0.00010201 | Val Loss: 0.00009468 | Train Acc: 81.35% | Val Acc: 83.00%\n",
      "Epoch 186/50000 | Data size: 252.8642812487558 | Train Loss: 0.00010689 | Val Loss: 0.00009441 | Train Acc: 80.27% | Val Acc: 83.05%\n",
      "Epoch 187/50000 | Data size: 254.12860265499955 | Train Loss: 0.00010396 | Val Loss: 0.00009432 | Train Acc: 81.00% | Val Acc: 83.09%\n",
      "Epoch 188/50000 | Data size: 255.39924566827455 | Train Loss: 0.00010295 | Val Loss: 0.00009400 | Train Acc: 81.20% | Val Acc: 83.18%\n",
      "Epoch 189/50000 | Data size: 256.6762418966159 | Train Loss: 0.00010331 | Val Loss: 0.00009372 | Train Acc: 81.13% | Val Acc: 83.24%\n",
      "Epoch 190/50000 | Data size: 257.959623106099 | Train Loss: 0.00009937 | Val Loss: 0.00009362 | Train Acc: 81.96% | Val Acc: 83.26%\n",
      "Epoch 191/50000 | Data size: 259.2494212216295 | Train Loss: 0.00010547 | Val Loss: 0.00009354 | Train Acc: 80.68% | Val Acc: 83.24%\n",
      "Epoch 192/50000 | Data size: 260.54566832773764 | Train Loss: 0.00010126 | Val Loss: 0.00009338 | Train Acc: 81.56% | Val Acc: 83.29%\n",
      "Epoch 193/50000 | Data size: 261.8483966693763 | Train Loss: 0.00010411 | Val Loss: 0.00009303 | Train Acc: 81.05% | Val Acc: 83.37%\n",
      "Epoch 194/50000 | Data size: 263.1576386527232 | Train Loss: 0.00010532 | Val Loss: 0.00009271 | Train Acc: 80.67% | Val Acc: 83.40%\n",
      "Epoch 195/50000 | Data size: 264.47342684598686 | Train Loss: 0.00009858 | Val Loss: 0.00009225 | Train Acc: 82.08% | Val Acc: 83.52%\n",
      "Epoch 196/50000 | Data size: 265.7957939802168 | Train Loss: 0.00010170 | Val Loss: 0.00009203 | Train Acc: 81.42% | Val Acc: 83.55%\n",
      "Epoch 197/50000 | Data size: 267.12477295011786 | Train Loss: 0.00010682 | Val Loss: 0.00009175 | Train Acc: 80.35% | Val Acc: 83.61%\n",
      "Epoch 198/50000 | Data size: 268.46039681486843 | Train Loss: 0.00010243 | Val Loss: 0.00009159 | Train Acc: 81.27% | Val Acc: 83.62%\n",
      "Epoch 199/50000 | Data size: 269.8026987989428 | Train Loss: 0.00010539 | Val Loss: 0.00009144 | Train Acc: 80.58% | Val Acc: 83.64%\n",
      "Epoch 200/50000 | Data size: 271.15171229293753 | Train Loss: 0.00009786 | Val Loss: 0.00009087 | Train Acc: 82.30% | Val Acc: 83.78%\n",
      "Epoch 201/50000 | Data size: 272.50747085440224 | Train Loss: 0.00010048 | Val Loss: 0.00009081 | Train Acc: 81.71% | Val Acc: 83.77%\n",
      "Epoch 202/50000 | Data size: 273.8700082086743 | Train Loss: 0.00009744 | Val Loss: 0.00009063 | Train Acc: 82.22% | Val Acc: 83.81%\n",
      "Epoch 203/50000 | Data size: 275.23935824971767 | Train Loss: 0.00009752 | Val Loss: 0.00009012 | Train Acc: 82.35% | Val Acc: 83.91%\n",
      "Epoch 204/50000 | Data size: 276.6155550409662 | Train Loss: 0.00009672 | Val Loss: 0.00009008 | Train Acc: 82.45% | Val Acc: 83.92%\n",
      "Epoch 205/50000 | Data size: 277.99863281617104 | Train Loss: 0.00009847 | Val Loss: 0.00008977 | Train Acc: 82.10% | Val Acc: 83.97%\n",
      "Epoch 206/50000 | Data size: 279.3886259802519 | Train Loss: 0.00009815 | Val Loss: 0.00008984 | Train Acc: 82.09% | Val Acc: 83.95%\n",
      "Epoch 207/50000 | Data size: 280.7855691101532 | Train Loss: 0.00010006 | Val Loss: 0.00008957 | Train Acc: 81.67% | Val Acc: 83.99%\n",
      "Epoch 208/50000 | Data size: 282.18949695570393 | Train Loss: 0.00010170 | Val Loss: 0.00008936 | Train Acc: 81.28% | Val Acc: 84.05%\n",
      "Epoch 209/50000 | Data size: 283.6004444404825 | Train Loss: 0.00009514 | Val Loss: 0.00008917 | Train Acc: 82.71% | Val Acc: 84.06%\n",
      "Epoch 210/50000 | Data size: 285.0184466626849 | Train Loss: 0.00009476 | Val Loss: 0.00008881 | Train Acc: 82.83% | Val Acc: 84.12%\n",
      "Epoch 211/50000 | Data size: 286.4435388959983 | Train Loss: 0.00009851 | Val Loss: 0.00008884 | Train Acc: 82.04% | Val Acc: 84.13%\n",
      "Epoch 212/50000 | Data size: 287.8757565904783 | Train Loss: 0.00009370 | Val Loss: 0.00008853 | Train Acc: 83.08% | Val Acc: 84.17%\n",
      "Epoch 213/50000 | Data size: 289.31513537343073 | Train Loss: 0.00009199 | Val Loss: 0.00008826 | Train Acc: 83.40% | Val Acc: 84.22%\n",
      "Epoch 214/50000 | Data size: 290.7617110502979 | Train Loss: 0.00009210 | Val Loss: 0.00008876 | Train Acc: 83.35% | Val Acc: 84.10%\n",
      "Epoch 215/50000 | Data size: 292.2155196055494 | Train Loss: 0.00009720 | Val Loss: 0.00008880 | Train Acc: 82.38% | Val Acc: 84.10%\n",
      "Epoch 216/50000 | Data size: 293.67659720357716 | Train Loss: 0.00009287 | Val Loss: 0.00008837 | Train Acc: 83.03% | Val Acc: 84.15%\n",
      "Epoch 217/50000 | Data size: 295.14498018959506 | Train Loss: 0.00009058 | Val Loss: 0.00008785 | Train Acc: 83.58% | Val Acc: 84.29%\n",
      "Epoch 218/50000 | Data size: 296.620705090543 | Train Loss: 0.00009392 | Val Loss: 0.00008720 | Train Acc: 82.94% | Val Acc: 84.35%\n",
      "Epoch 219/50000 | Data size: 298.1038086159957 | Train Loss: 0.00009753 | Val Loss: 0.00008705 | Train Acc: 82.15% | Val Acc: 84.38%\n",
      "Epoch 220/50000 | Data size: 299.5943276590757 | Train Loss: 0.00008975 | Val Loss: 0.00008678 | Train Acc: 83.84% | Val Acc: 84.43%\n",
      "Epoch 221/50000 | Data size: 301.0922992973711 | Train Loss: 0.00009294 | Val Loss: 0.00008625 | Train Acc: 83.09% | Val Acc: 84.53%\n",
      "Epoch 222/50000 | Data size: 302.59776079385796 | Train Loss: 0.00009229 | Val Loss: 0.00008633 | Train Acc: 83.21% | Val Acc: 84.54%\n",
      "Epoch 223/50000 | Data size: 304.11074959782724 | Train Loss: 0.00009372 | Val Loss: 0.00008598 | Train Acc: 83.10% | Val Acc: 84.56%\n",
      "Epoch 224/50000 | Data size: 305.6313033458164 | Train Loss: 0.00009105 | Val Loss: 0.00008580 | Train Acc: 83.42% | Val Acc: 84.61%\n",
      "Epoch 225/50000 | Data size: 307.1594598625455 | Train Loss: 0.00009543 | Val Loss: 0.00008532 | Train Acc: 82.57% | Val Acc: 84.69%\n",
      "Epoch 226/50000 | Data size: 308.6952571618582 | Train Loss: 0.00008733 | Val Loss: 0.00008523 | Train Acc: 84.28% | Val Acc: 84.71%\n",
      "Epoch 227/50000 | Data size: 310.23873344766747 | Train Loss: 0.00009303 | Val Loss: 0.00008491 | Train Acc: 82.97% | Val Acc: 84.76%\n",
      "Epoch 228/50000 | Data size: 311.7899271149058 | Train Loss: 0.00009264 | Val Loss: 0.00008451 | Train Acc: 83.13% | Val Acc: 84.83%\n",
      "Epoch 229/50000 | Data size: 313.3488767504804 | Train Loss: 0.00009422 | Val Loss: 0.00008440 | Train Acc: 82.76% | Val Acc: 84.85%\n",
      "Epoch 230/50000 | Data size: 314.91562113423277 | Train Loss: 0.00008868 | Val Loss: 0.00008412 | Train Acc: 83.84% | Val Acc: 84.88%\n",
      "Epoch 231/50000 | Data size: 316.4901992399039 | Train Loss: 0.00008613 | Val Loss: 0.00008396 | Train Acc: 84.50% | Val Acc: 84.95%\n",
      "Epoch 232/50000 | Data size: 318.0726502361034 | Train Loss: 0.00008312 | Val Loss: 0.00008367 | Train Acc: 84.90% | Val Acc: 84.96%\n",
      "Epoch 233/50000 | Data size: 319.66301348728393 | Train Loss: 0.00008936 | Val Loss: 0.00008316 | Train Acc: 83.65% | Val Acc: 85.03%\n",
      "Epoch 234/50000 | Data size: 321.26132855472036 | Train Loss: 0.00009244 | Val Loss: 0.00008310 | Train Acc: 82.89% | Val Acc: 85.04%\n",
      "Epoch 235/50000 | Data size: 322.86763519749394 | Train Loss: 0.00008558 | Val Loss: 0.00008313 | Train Acc: 84.46% | Val Acc: 85.01%\n",
      "Epoch 236/50000 | Data size: 324.4819733734814 | Train Loss: 0.00008652 | Val Loss: 0.00008270 | Train Acc: 84.30% | Val Acc: 85.10%\n",
      "Epoch 237/50000 | Data size: 326.10438324034885 | Train Loss: 0.00008627 | Val Loss: 0.00008211 | Train Acc: 84.34% | Val Acc: 85.20%\n",
      "Epoch 238/50000 | Data size: 327.7349051565506 | Train Loss: 0.00008286 | Val Loss: 0.00008176 | Train Acc: 85.00% | Val Acc: 85.25%\n",
      "Epoch 239/50000 | Data size: 329.37357968233334 | Train Loss: 0.00008383 | Val Loss: 0.00008127 | Train Acc: 84.68% | Val Acc: 85.36%\n",
      "Epoch 240/50000 | Data size: 331.020447580745 | Train Loss: 0.00008973 | Val Loss: 0.00008065 | Train Acc: 83.55% | Val Acc: 85.43%\n",
      "Epoch 241/50000 | Data size: 332.67554981864873 | Train Loss: 0.00008804 | Val Loss: 0.00008059 | Train Acc: 83.98% | Val Acc: 85.47%\n",
      "Epoch 242/50000 | Data size: 334.33892756774196 | Train Loss: 0.00008516 | Val Loss: 0.00008023 | Train Acc: 84.46% | Val Acc: 85.52%\n",
      "Epoch 243/50000 | Data size: 336.01062220558066 | Train Loss: 0.00008342 | Val Loss: 0.00007970 | Train Acc: 84.78% | Val Acc: 85.61%\n",
      "Epoch 244/50000 | Data size: 337.6906753166086 | Train Loss: 0.00008101 | Val Loss: 0.00007943 | Train Acc: 85.20% | Val Acc: 85.66%\n",
      "Epoch 245/50000 | Data size: 339.3791286931916 | Train Loss: 0.00008860 | Val Loss: 0.00007907 | Train Acc: 83.69% | Val Acc: 85.72%\n",
      "Epoch 246/50000 | Data size: 341.07602433665755 | Train Loss: 0.00008122 | Val Loss: 0.00007893 | Train Acc: 85.32% | Val Acc: 85.76%\n",
      "Epoch 247/50000 | Data size: 342.78140445834083 | Train Loss: 0.00008505 | Val Loss: 0.00007856 | Train Acc: 84.48% | Val Acc: 85.81%\n",
      "Epoch 248/50000 | Data size: 344.49531148063255 | Train Loss: 0.00008518 | Val Loss: 0.00007823 | Train Acc: 84.40% | Val Acc: 85.87%\n",
      "Epoch 249/50000 | Data size: 346.21778803803574 | Train Loss: 0.00008626 | Val Loss: 0.00007802 | Train Acc: 84.24% | Val Acc: 85.90%\n",
      "Epoch 250/50000 | Data size: 347.94887697822594 | Train Loss: 0.00008546 | Val Loss: 0.00007761 | Train Acc: 84.25% | Val Acc: 85.97%\n",
      "Epoch 251/50000 | Data size: 349.6886213631171 | Train Loss: 0.00008283 | Val Loss: 0.00007736 | Train Acc: 84.85% | Val Acc: 86.04%\n",
      "Epoch 252/50000 | Data size: 351.43706446993264 | Train Loss: 0.00008234 | Val Loss: 0.00007695 | Train Acc: 85.05% | Val Acc: 86.09%\n",
      "Epoch 253/50000 | Data size: 353.1942497922823 | Train Loss: 0.00008333 | Val Loss: 0.00007664 | Train Acc: 84.60% | Val Acc: 86.14%\n",
      "Epoch 254/50000 | Data size: 354.9602210412437 | Train Loss: 0.00007995 | Val Loss: 0.00007628 | Train Acc: 85.44% | Val Acc: 86.17%\n",
      "Epoch 255/50000 | Data size: 356.7350221464499 | Train Loss: 0.00007842 | Val Loss: 0.00007647 | Train Acc: 85.53% | Val Acc: 86.15%\n",
      "Epoch 256/50000 | Data size: 358.51869725718217 | Train Loss: 0.00007947 | Val Loss: 0.00007628 | Train Acc: 85.29% | Val Acc: 86.19%\n",
      "Epoch 257/50000 | Data size: 360.3112907434681 | Train Loss: 0.00007994 | Val Loss: 0.00007584 | Train Acc: 85.27% | Val Acc: 86.27%\n",
      "Epoch 258/50000 | Data size: 362.11284719718543 | Train Loss: 0.00007542 | Val Loss: 0.00007502 | Train Acc: 86.16% | Val Acc: 86.40%\n",
      "Epoch 259/50000 | Data size: 363.92341143317134 | Train Loss: 0.00008140 | Val Loss: 0.00007464 | Train Acc: 84.95% | Val Acc: 86.47%\n",
      "Epoch 260/50000 | Data size: 365.74302849033717 | Train Loss: 0.00008177 | Val Loss: 0.00007426 | Train Acc: 84.98% | Val Acc: 86.53%\n",
      "Epoch 261/50000 | Data size: 367.57174363278887 | Train Loss: 0.00007722 | Val Loss: 0.00007383 | Train Acc: 85.84% | Val Acc: 86.61%\n",
      "Epoch 262/50000 | Data size: 369.4096023509528 | Train Loss: 0.00007629 | Val Loss: 0.00007386 | Train Acc: 86.04% | Val Acc: 86.61%\n",
      "Epoch 263/50000 | Data size: 371.25665036270755 | Train Loss: 0.00007748 | Val Loss: 0.00007315 | Train Acc: 85.85% | Val Acc: 86.72%\n",
      "Epoch 264/50000 | Data size: 373.11293361452107 | Train Loss: 0.00008226 | Val Loss: 0.00007327 | Train Acc: 84.76% | Val Acc: 86.75%\n",
      "Epoch 265/50000 | Data size: 374.97849828259365 | Train Loss: 0.00008085 | Val Loss: 0.00007248 | Train Acc: 85.04% | Val Acc: 86.85%\n",
      "Epoch 266/50000 | Data size: 376.85339077400664 | Train Loss: 0.00007694 | Val Loss: 0.00007230 | Train Acc: 85.87% | Val Acc: 86.88%\n",
      "Epoch 267/50000 | Data size: 378.7376577278767 | Train Loss: 0.00008100 | Val Loss: 0.00007213 | Train Acc: 85.11% | Val Acc: 86.94%\n",
      "Epoch 268/50000 | Data size: 380.6313460165161 | Train Loss: 0.00007147 | Val Loss: 0.00007147 | Train Acc: 87.09% | Val Acc: 87.03%\n",
      "Epoch 269/50000 | Data size: 382.53450274659866 | Train Loss: 0.00008101 | Val Loss: 0.00007179 | Train Acc: 84.94% | Val Acc: 87.01%\n",
      "Epoch 270/50000 | Data size: 384.4471752603317 | Train Loss: 0.00007931 | Val Loss: 0.00007126 | Train Acc: 85.24% | Val Acc: 87.10%\n",
      "Epoch 271/50000 | Data size: 386.36941113663335 | Train Loss: 0.00007940 | Val Loss: 0.00007072 | Train Acc: 85.30% | Val Acc: 87.18%\n",
      "Epoch 272/50000 | Data size: 388.30125819231654 | Train Loss: 0.00008354 | Val Loss: 0.00007086 | Train Acc: 84.48% | Val Acc: 87.15%\n",
      "Epoch 273/50000 | Data size: 390.2427644832781 | Train Loss: 0.00007087 | Val Loss: 0.00007047 | Train Acc: 87.09% | Val Acc: 87.22%\n",
      "Epoch 274/50000 | Data size: 392.1939783056945 | Train Loss: 0.00006690 | Val Loss: 0.00007027 | Train Acc: 87.76% | Val Acc: 87.27%\n",
      "Epoch 275/50000 | Data size: 394.15494819722295 | Train Loss: 0.00007200 | Val Loss: 0.00006961 | Train Acc: 86.81% | Val Acc: 87.38%\n",
      "Epoch 276/50000 | Data size: 396.12572293820904 | Train Loss: 0.00007279 | Val Loss: 0.00006951 | Train Acc: 86.82% | Val Acc: 87.40%\n",
      "Epoch 277/50000 | Data size: 398.1063515529001 | Train Loss: 0.00007365 | Val Loss: 0.00006911 | Train Acc: 86.48% | Val Acc: 87.48%\n",
      "Epoch 278/50000 | Data size: 400.09688331066457 | Train Loss: 0.00007205 | Val Loss: 0.00006880 | Train Acc: 86.81% | Val Acc: 87.53%\n",
      "Epoch 279/50000 | Data size: 402.0973677272179 | Train Loss: 0.00007023 | Val Loss: 0.00006876 | Train Acc: 87.12% | Val Acc: 87.56%\n",
      "Epoch 280/50000 | Data size: 404.107854565854 | Train Loss: 0.00007281 | Val Loss: 0.00006852 | Train Acc: 86.59% | Val Acc: 87.60%\n",
      "Epoch 281/50000 | Data size: 406.1283938386833 | Train Loss: 0.00007352 | Val Loss: 0.00006801 | Train Acc: 86.45% | Val Acc: 87.67%\n",
      "Epoch 282/50000 | Data size: 408.1590358078767 | Train Loss: 0.00007241 | Val Loss: 0.00006764 | Train Acc: 86.70% | Val Acc: 87.76%\n",
      "Epoch 283/50000 | Data size: 410.1998309869161 | Train Loss: 0.00007593 | Val Loss: 0.00006742 | Train Acc: 85.88% | Val Acc: 87.79%\n",
      "Epoch 284/50000 | Data size: 412.25083014185066 | Train Loss: 0.00007307 | Val Loss: 0.00006717 | Train Acc: 86.54% | Val Acc: 87.84%\n",
      "Epoch 285/50000 | Data size: 414.3120842925599 | Train Loss: 0.00007184 | Val Loss: 0.00006704 | Train Acc: 86.85% | Val Acc: 87.88%\n",
      "Epoch 286/50000 | Data size: 416.3836447140227 | Train Loss: 0.00007318 | Val Loss: 0.00006631 | Train Acc: 86.54% | Val Acc: 87.96%\n",
      "Epoch 287/50000 | Data size: 418.46556293759284 | Train Loss: 0.00006997 | Val Loss: 0.00006645 | Train Acc: 87.17% | Val Acc: 87.98%\n",
      "Epoch 288/50000 | Data size: 420.5578907522808 | Train Loss: 0.00007107 | Val Loss: 0.00006665 | Train Acc: 87.10% | Val Acc: 87.94%\n",
      "Epoch 289/50000 | Data size: 422.66068020604223 | Train Loss: 0.00006971 | Val Loss: 0.00006623 | Train Acc: 87.24% | Val Acc: 88.00%\n",
      "Epoch 290/50000 | Data size: 424.77398360707247 | Train Loss: 0.00007215 | Val Loss: 0.00006574 | Train Acc: 86.58% | Val Acc: 88.11%\n",
      "Epoch 291/50000 | Data size: 426.89785352510785 | Train Loss: 0.00007311 | Val Loss: 0.00006532 | Train Acc: 86.52% | Val Acc: 88.16%\n",
      "Epoch 292/50000 | Data size: 429.0323427927334 | Train Loss: 0.00007305 | Val Loss: 0.00006504 | Train Acc: 86.59% | Val Acc: 88.24%\n",
      "Epoch 293/50000 | Data size: 431.17750450669706 | Train Loss: 0.00006958 | Val Loss: 0.00006462 | Train Acc: 87.34% | Val Acc: 88.31%\n",
      "Epoch 294/50000 | Data size: 433.33339202923054 | Train Loss: 0.00006870 | Val Loss: 0.00006470 | Train Acc: 87.47% | Val Acc: 88.31%\n",
      "Epoch 295/50000 | Data size: 435.50005898937667 | Train Loss: 0.00006723 | Val Loss: 0.00006433 | Train Acc: 87.76% | Val Acc: 88.39%\n",
      "Epoch 296/50000 | Data size: 437.67755928432354 | Train Loss: 0.00007093 | Val Loss: 0.00006396 | Train Acc: 86.94% | Val Acc: 88.43%\n",
      "Epoch 297/50000 | Data size: 439.8659470807452 | Train Loss: 0.00007288 | Val Loss: 0.00006365 | Train Acc: 86.60% | Val Acc: 88.48%\n",
      "Epoch 298/50000 | Data size: 442.0652768161489 | Train Loss: 0.00006904 | Val Loss: 0.00006340 | Train Acc: 87.35% | Val Acc: 88.54%\n",
      "Epoch 299/50000 | Data size: 444.2756032002297 | Train Loss: 0.00006531 | Val Loss: 0.00006335 | Train Acc: 88.07% | Val Acc: 88.57%\n",
      "Epoch 300/50000 | Data size: 446.49698121623084 | Train Loss: 0.00006669 | Val Loss: 0.00006294 | Train Acc: 87.84% | Val Acc: 88.63%\n",
      "Epoch 301/50000 | Data size: 448.729466122312 | Train Loss: 0.00006788 | Val Loss: 0.00006276 | Train Acc: 87.55% | Val Acc: 88.67%\n",
      "Epoch 302/50000 | Data size: 450.97311345292354 | Train Loss: 0.00006469 | Val Loss: 0.00006253 | Train Acc: 88.18% | Val Acc: 88.70%\n",
      "Epoch 303/50000 | Data size: 453.2279790201882 | Train Loss: 0.00006580 | Val Loss: 0.00006245 | Train Acc: 88.01% | Val Acc: 88.71%\n",
      "Epoch 304/50000 | Data size: 455.4941189152891 | Train Loss: 0.00006897 | Val Loss: 0.00006237 | Train Acc: 87.33% | Val Acc: 88.75%\n",
      "Epoch 305/50000 | Data size: 457.7715895098656 | Train Loss: 0.00006886 | Val Loss: 0.00006205 | Train Acc: 87.34% | Val Acc: 88.79%\n",
      "Epoch 306/50000 | Data size: 460.0604474574149 | Train Loss: 0.00006632 | Val Loss: 0.00006202 | Train Acc: 87.92% | Val Acc: 88.83%\n",
      "Epoch 307/50000 | Data size: 462.360749694702 | Train Loss: 0.00006438 | Val Loss: 0.00006147 | Train Acc: 88.34% | Val Acc: 88.91%\n",
      "Epoch 308/50000 | Data size: 464.6725534431755 | Train Loss: 0.00006511 | Val Loss: 0.00006100 | Train Acc: 88.21% | Val Acc: 88.99%\n",
      "Epoch 309/50000 | Data size: 466.9959162103914 | Train Loss: 0.00006655 | Val Loss: 0.00006074 | Train Acc: 87.79% | Val Acc: 89.04%\n",
      "Epoch 310/50000 | Data size: 469.33089579144337 | Train Loss: 0.00006399 | Val Loss: 0.00006054 | Train Acc: 88.33% | Val Acc: 89.08%\n",
      "Epoch 311/50000 | Data size: 471.67755027040056 | Train Loss: 0.00006274 | Val Loss: 0.00006041 | Train Acc: 88.57% | Val Acc: 89.12%\n",
      "Epoch 312/50000 | Data size: 474.03593802175254 | Train Loss: 0.00006698 | Val Loss: 0.00006020 | Train Acc: 87.73% | Val Acc: 89.15%\n",
      "Epoch 313/50000 | Data size: 476.4061177118613 | Train Loss: 0.00006098 | Val Loss: 0.00005987 | Train Acc: 88.93% | Val Acc: 89.21%\n",
      "Epoch 314/50000 | Data size: 478.78814830042063 | Train Loss: 0.00006357 | Val Loss: 0.00005955 | Train Acc: 88.39% | Val Acc: 89.26%\n",
      "Epoch 315/50000 | Data size: 481.1820890419227 | Train Loss: 0.00006350 | Val Loss: 0.00005930 | Train Acc: 88.53% | Val Acc: 89.30%\n",
      "Epoch 316/50000 | Data size: 483.5879994871323 | Train Loss: 0.00006110 | Val Loss: 0.00005953 | Train Acc: 88.85% | Val Acc: 89.25%\n",
      "Epoch 317/50000 | Data size: 486.005939484568 | Train Loss: 0.00006577 | Val Loss: 0.00005939 | Train Acc: 88.01% | Val Acc: 89.29%\n",
      "Epoch 318/50000 | Data size: 488.43596918199086 | Train Loss: 0.00006047 | Val Loss: 0.00005872 | Train Acc: 89.00% | Val Acc: 89.42%\n",
      "Epoch 319/50000 | Data size: 490.8781490279008 | Train Loss: 0.00006183 | Val Loss: 0.00005875 | Train Acc: 88.63% | Val Acc: 89.44%\n",
      "Epoch 320/50000 | Data size: 493.3325397730403 | Train Loss: 0.00006103 | Val Loss: 0.00005845 | Train Acc: 88.96% | Val Acc: 89.48%\n",
      "Epoch 321/50000 | Data size: 495.7992024719055 | Train Loss: 0.00006365 | Val Loss: 0.00005812 | Train Acc: 88.35% | Val Acc: 89.55%\n",
      "Epoch 322/50000 | Data size: 498.27819848426503 | Train Loss: 0.00006155 | Val Loss: 0.00005765 | Train Acc: 88.82% | Val Acc: 89.62%\n",
      "Epoch 323/50000 | Data size: 500.76958947668635 | Train Loss: 0.00006052 | Val Loss: 0.00005755 | Train Acc: 89.03% | Val Acc: 89.63%\n",
      "Epoch 324/50000 | Data size: 503.27343742406975 | Train Loss: 0.00006282 | Val Loss: 0.00005717 | Train Acc: 88.48% | Val Acc: 89.69%\n",
      "Epoch 325/50000 | Data size: 505.7898046111901 | Train Loss: 0.00006363 | Val Loss: 0.00005696 | Train Acc: 88.34% | Val Acc: 89.74%\n",
      "Epoch 326/50000 | Data size: 508.31875363424604 | Train Loss: 0.00006289 | Val Loss: 0.00005679 | Train Acc: 88.54% | Val Acc: 89.80%\n",
      "Epoch 327/50000 | Data size: 510.8603474024173 | Train Loss: 0.00005716 | Val Loss: 0.00005663 | Train Acc: 89.70% | Val Acc: 89.82%\n",
      "Epoch 328/50000 | Data size: 513.4146491394293 | Train Loss: 0.00006100 | Val Loss: 0.00005667 | Train Acc: 88.93% | Val Acc: 89.83%\n",
      "Epoch 329/50000 | Data size: 515.9817223851265 | Train Loss: 0.00006377 | Val Loss: 0.00005683 | Train Acc: 88.34% | Val Acc: 89.77%\n",
      "Epoch 330/50000 | Data size: 518.5616309970521 | Train Loss: 0.00005768 | Val Loss: 0.00005629 | Train Acc: 89.53% | Val Acc: 89.87%\n",
      "Epoch 331/50000 | Data size: 521.1544391520374 | Train Loss: 0.00006083 | Val Loss: 0.00005639 | Train Acc: 88.94% | Val Acc: 89.90%\n",
      "Epoch 332/50000 | Data size: 523.7602113477976 | Train Loss: 0.00005805 | Val Loss: 0.00005577 | Train Acc: 89.44% | Val Acc: 89.99%\n",
      "Epoch 333/50000 | Data size: 526.3790124045366 | Train Loss: 0.00006031 | Val Loss: 0.00005562 | Train Acc: 88.98% | Val Acc: 90.03%\n",
      "Epoch 334/50000 | Data size: 529.0109074665593 | Train Loss: 0.00006144 | Val Loss: 0.00005518 | Train Acc: 88.75% | Val Acc: 90.10%\n",
      "Epoch 335/50000 | Data size: 531.6559620038921 | Train Loss: 0.00005815 | Val Loss: 0.00005485 | Train Acc: 89.47% | Val Acc: 90.14%\n",
      "Epoch 336/50000 | Data size: 534.3142418139116 | Train Loss: 0.00005787 | Val Loss: 0.00005465 | Train Acc: 89.53% | Val Acc: 90.21%\n",
      "Epoch 337/50000 | Data size: 536.9858130229812 | Train Loss: 0.00006044 | Val Loss: 0.00005451 | Train Acc: 89.00% | Val Acc: 90.22%\n",
      "Epoch 338/50000 | Data size: 539.6707420880962 | Train Loss: 0.00006075 | Val Loss: 0.00005406 | Train Acc: 88.90% | Val Acc: 90.29%\n",
      "Epoch 339/50000 | Data size: 542.3690957985367 | Train Loss: 0.00005746 | Val Loss: 0.00005407 | Train Acc: 89.57% | Val Acc: 90.31%\n",
      "Epoch 340/50000 | Data size: 545.0809412775294 | Train Loss: 0.00005829 | Val Loss: 0.00005372 | Train Acc: 89.36% | Val Acc: 90.37%\n",
      "Epoch 341/50000 | Data size: 547.806345983917 | Train Loss: 0.00005786 | Val Loss: 0.00005410 | Train Acc: 89.64% | Val Acc: 90.30%\n",
      "Epoch 342/50000 | Data size: 550.5453777138366 | Train Loss: 0.00005509 | Val Loss: 0.00005404 | Train Acc: 90.01% | Val Acc: 90.32%\n",
      "Epoch 343/50000 | Data size: 553.2981046024058 | Train Loss: 0.00005443 | Val Loss: 0.00005359 | Train Acc: 90.07% | Val Acc: 90.39%\n",
      "Epoch 344/50000 | Data size: 556.0645951254179 | Train Loss: 0.00005602 | Val Loss: 0.00005317 | Train Acc: 89.80% | Val Acc: 90.48%\n",
      "Epoch 345/50000 | Data size: 558.844918101045 | Train Loss: 0.00005389 | Val Loss: 0.00005277 | Train Acc: 90.23% | Val Acc: 90.54%\n",
      "Epoch 346/50000 | Data size: 561.6391426915502 | Train Loss: 0.00005732 | Val Loss: 0.00005268 | Train Acc: 89.62% | Val Acc: 90.59%\n",
      "Epoch 347/50000 | Data size: 564.4473384050079 | Train Loss: 0.00005309 | Val Loss: 0.00005271 | Train Acc: 90.40% | Val Acc: 90.60%\n",
      "Epoch 348/50000 | Data size: 567.269575097033 | Train Loss: 0.00005766 | Val Loss: 0.00005204 | Train Acc: 89.47% | Val Acc: 90.67%\n",
      "Epoch 349/50000 | Data size: 570.1059229725182 | Train Loss: 0.00005496 | Val Loss: 0.00005204 | Train Acc: 90.04% | Val Acc: 90.70%\n",
      "Epoch 350/50000 | Data size: 572.9564525873808 | Train Loss: 0.00005483 | Val Loss: 0.00005176 | Train Acc: 90.07% | Val Acc: 90.74%\n",
      "Epoch 351/50000 | Data size: 575.8212348503178 | Train Loss: 0.00005344 | Val Loss: 0.00005158 | Train Acc: 90.33% | Val Acc: 90.79%\n",
      "Epoch 352/50000 | Data size: 578.7003410245693 | Train Loss: 0.00005398 | Val Loss: 0.00005152 | Train Acc: 90.24% | Val Acc: 90.80%\n",
      "Epoch 353/50000 | Data size: 581.5938427296921 | Train Loss: 0.00005361 | Val Loss: 0.00005130 | Train Acc: 90.27% | Val Acc: 90.82%\n",
      "Epoch 354/50000 | Data size: 584.5018119433406 | Train Loss: 0.00005279 | Val Loss: 0.00005098 | Train Acc: 90.47% | Val Acc: 90.88%\n",
      "Epoch 355/50000 | Data size: 587.4243210030573 | Train Loss: 0.00005428 | Val Loss: 0.00005063 | Train Acc: 90.13% | Val Acc: 90.94%\n",
      "Epoch 356/50000 | Data size: 590.3614426080726 | Train Loss: 0.00005427 | Val Loss: 0.00005070 | Train Acc: 90.14% | Val Acc: 90.94%\n",
      "Epoch 357/50000 | Data size: 593.313249821113 | Train Loss: 0.00005237 | Val Loss: 0.00005035 | Train Acc: 90.54% | Val Acc: 91.01%\n",
      "Epoch 358/50000 | Data size: 596.2798160702185 | Train Loss: 0.00005243 | Val Loss: 0.00005028 | Train Acc: 90.56% | Val Acc: 91.05%\n",
      "Epoch 359/50000 | Data size: 599.2612151505697 | Train Loss: 0.00005141 | Val Loss: 0.00004982 | Train Acc: 90.67% | Val Acc: 91.10%\n",
      "Epoch 360/50000 | Data size: 602.2575212263225 | Train Loss: 0.00005276 | Val Loss: 0.00004969 | Train Acc: 90.43% | Val Acc: 91.17%\n",
      "Epoch 361/50000 | Data size: 605.2688088324542 | Train Loss: 0.00005429 | Val Loss: 0.00004931 | Train Acc: 90.14% | Val Acc: 91.20%\n",
      "Epoch 362/50000 | Data size: 608.2951528766164 | Train Loss: 0.00005224 | Val Loss: 0.00004908 | Train Acc: 90.59% | Val Acc: 91.25%\n",
      "Epoch 363/50000 | Data size: 611.3366286409995 | Train Loss: 0.00005243 | Val Loss: 0.00004881 | Train Acc: 90.56% | Val Acc: 91.30%\n",
      "Epoch 364/50000 | Data size: 614.3933117842045 | Train Loss: 0.00005140 | Val Loss: 0.00004922 | Train Acc: 90.76% | Val Acc: 91.23%\n",
      "Epoch 365/50000 | Data size: 617.4652783431255 | Train Loss: 0.00004864 | Val Loss: 0.00004884 | Train Acc: 91.29% | Val Acc: 91.29%\n",
      "Epoch 366/50000 | Data size: 620.5526047348411 | Train Loss: 0.00005120 | Val Loss: 0.00004850 | Train Acc: 90.82% | Val Acc: 91.37%\n",
      "Epoch 367/50000 | Data size: 623.6553677585154 | Train Loss: 0.00004943 | Val Loss: 0.00004831 | Train Acc: 91.14% | Val Acc: 91.40%\n",
      "Epoch 368/50000 | Data size: 626.7736445973079 | Train Loss: 0.00005018 | Val Loss: 0.00004788 | Train Acc: 90.98% | Val Acc: 91.48%\n",
      "Epoch 369/50000 | Data size: 629.9075128202944 | Train Loss: 0.00005118 | Val Loss: 0.00004790 | Train Acc: 90.78% | Val Acc: 91.46%\n",
      "Epoch 370/50000 | Data size: 633.0570503843959 | Train Loss: 0.00005043 | Val Loss: 0.00004749 | Train Acc: 90.93% | Val Acc: 91.53%\n",
      "Epoch 371/50000 | Data size: 636.2223356363179 | Train Loss: 0.00005107 | Val Loss: 0.00004725 | Train Acc: 90.74% | Val Acc: 91.59%\n",
      "Epoch 372/50000 | Data size: 639.4034473144994 | Train Loss: 0.00004898 | Val Loss: 0.00004692 | Train Acc: 91.25% | Val Acc: 91.65%\n",
      "Epoch 373/50000 | Data size: 642.6004645510719 | Train Loss: 0.00004897 | Val Loss: 0.00004671 | Train Acc: 91.24% | Val Acc: 91.69%\n",
      "Epoch 374/50000 | Data size: 645.8134668738273 | Train Loss: 0.00005161 | Val Loss: 0.00004714 | Train Acc: 90.66% | Val Acc: 91.61%\n",
      "Epoch 375/50000 | Data size: 649.0425342081965 | Train Loss: 0.00005070 | Val Loss: 0.00004680 | Train Acc: 90.91% | Val Acc: 91.67%\n",
      "Epoch 376/50000 | Data size: 652.2877468792375 | Train Loss: 0.00004778 | Val Loss: 0.00004654 | Train Acc: 91.43% | Val Acc: 91.74%\n",
      "Epoch 377/50000 | Data size: 655.5491856136337 | Train Loss: 0.00004674 | Val Loss: 0.00004608 | Train Acc: 91.64% | Val Acc: 91.81%\n",
      "Epoch 378/50000 | Data size: 658.8269315417018 | Train Loss: 0.00004894 | Val Loss: 0.00004598 | Train Acc: 91.18% | Val Acc: 91.82%\n",
      "Epoch 379/50000 | Data size: 662.1210661994103 | Train Loss: 0.00004743 | Val Loss: 0.00004571 | Train Acc: 91.53% | Val Acc: 91.89%\n",
      "Epoch 380/50000 | Data size: 665.4316715304074 | Train Loss: 0.00004892 | Val Loss: 0.00004569 | Train Acc: 91.21% | Val Acc: 91.91%\n",
      "Epoch 381/50000 | Data size: 668.7588298880594 | Train Loss: 0.00004635 | Val Loss: 0.00004539 | Train Acc: 91.72% | Val Acc: 91.94%\n",
      "Epoch 382/50000 | Data size: 672.1026240374997 | Train Loss: 0.00004710 | Val Loss: 0.00004512 | Train Acc: 91.54% | Val Acc: 91.99%\n",
      "Epoch 383/50000 | Data size: 675.4631371576872 | Train Loss: 0.00004766 | Val Loss: 0.00004483 | Train Acc: 91.43% | Val Acc: 92.05%\n",
      "Epoch 384/50000 | Data size: 678.8404528434756 | Train Loss: 0.00004818 | Val Loss: 0.00004514 | Train Acc: 91.35% | Val Acc: 92.00%\n",
      "Epoch 385/50000 | Data size: 682.234655107693 | Train Loss: 0.00004786 | Val Loss: 0.00004475 | Train Acc: 91.43% | Val Acc: 92.05%\n",
      "Epoch 386/50000 | Data size: 685.6458283832314 | Train Loss: 0.00004841 | Val Loss: 0.00004460 | Train Acc: 91.25% | Val Acc: 92.12%\n",
      "Epoch 387/50000 | Data size: 689.0740575251476 | Train Loss: 0.00004618 | Val Loss: 0.00004420 | Train Acc: 91.74% | Val Acc: 92.16%\n",
      "Epoch 388/50000 | Data size: 692.5194278127734 | Train Loss: 0.00004400 | Val Loss: 0.00004404 | Train Acc: 92.11% | Val Acc: 92.22%\n",
      "Epoch 389/50000 | Data size: 695.9820249518373 | Train Loss: 0.00004675 | Val Loss: 0.00004383 | Train Acc: 91.63% | Val Acc: 92.24%\n",
      "Epoch 390/50000 | Data size: 699.4619350765964 | Train Loss: 0.00004674 | Val Loss: 0.00004389 | Train Acc: 91.64% | Val Acc: 92.25%\n",
      "Epoch 391/50000 | Data size: 702.9592447519794 | Train Loss: 0.00004622 | Val Loss: 0.00004342 | Train Acc: 91.70% | Val Acc: 92.32%\n",
      "Epoch 392/50000 | Data size: 706.4740409757393 | Train Loss: 0.00004540 | Val Loss: 0.00004330 | Train Acc: 91.87% | Val Acc: 92.34%\n",
      "Epoch 393/50000 | Data size: 710.006411180618 | Train Loss: 0.00004436 | Val Loss: 0.00004333 | Train Acc: 92.06% | Val Acc: 92.33%\n",
      "Epoch 394/50000 | Data size: 713.5564432365211 | Train Loss: 0.00004647 | Val Loss: 0.00004313 | Train Acc: 91.69% | Val Acc: 92.39%\n",
      "Epoch 395/50000 | Data size: 717.1242254527037 | Train Loss: 0.00004516 | Val Loss: 0.00004315 | Train Acc: 91.94% | Val Acc: 92.39%\n",
      "Epoch 396/50000 | Data size: 720.7098465799672 | Train Loss: 0.00004375 | Val Loss: 0.00004264 | Train Acc: 92.18% | Val Acc: 92.47%\n",
      "Epoch 397/50000 | Data size: 724.313395812867 | Train Loss: 0.00004458 | Val Loss: 0.00004253 | Train Acc: 92.01% | Val Acc: 92.50%\n",
      "Epoch 398/50000 | Data size: 727.9349627919314 | Train Loss: 0.00004534 | Val Loss: 0.00004228 | Train Acc: 91.91% | Val Acc: 92.56%\n",
      "Epoch 399/50000 | Data size: 731.5746376058911 | Train Loss: 0.00004362 | Val Loss: 0.00004214 | Train Acc: 92.23% | Val Acc: 92.58%\n",
      "Epoch 400/50000 | Data size: 735.2325107939206 | Train Loss: 0.00004346 | Val Loss: 0.00004194 | Train Acc: 92.28% | Val Acc: 92.62%\n",
      "Epoch 401/50000 | Data size: 738.9086733478902 | Train Loss: 0.00004487 | Val Loss: 0.00004180 | Train Acc: 91.97% | Val Acc: 92.63%\n",
      "Epoch 402/50000 | Data size: 742.6032167146296 | Train Loss: 0.00004448 | Val Loss: 0.00004228 | Train Acc: 92.06% | Val Acc: 92.56%\n",
      "Epoch 403/50000 | Data size: 746.3162327982028 | Train Loss: 0.00004490 | Val Loss: 0.00004170 | Train Acc: 92.01% | Val Acc: 92.68%\n",
      "Epoch 404/50000 | Data size: 750.0478139621938 | Train Loss: 0.00004377 | Val Loss: 0.00004143 | Train Acc: 92.19% | Val Acc: 92.73%\n",
      "Epoch 405/50000 | Data size: 753.7980530320048 | Train Loss: 0.00004255 | Val Loss: 0.00004118 | Train Acc: 92.43% | Val Acc: 92.77%\n",
      "Epoch 406/50000 | Data size: 757.5670432971648 | Train Loss: 0.00004289 | Val Loss: 0.00004102 | Train Acc: 92.40% | Val Acc: 92.79%\n",
      "Epoch 407/50000 | Data size: 761.3548785136506 | Train Loss: 0.00004216 | Val Loss: 0.00004093 | Train Acc: 92.50% | Val Acc: 92.81%\n",
      "Epoch 408/50000 | Data size: 765.1616529062189 | Train Loss: 0.00004276 | Val Loss: 0.00004074 | Train Acc: 92.38% | Val Acc: 92.86%\n",
      "Epoch 409/50000 | Data size: 768.9874611707501 | Train Loss: 0.00004273 | Val Loss: 0.00004047 | Train Acc: 92.42% | Val Acc: 92.90%\n",
      "Epoch 410/50000 | Data size: 772.8323984766038 | Train Loss: 0.00004359 | Val Loss: 0.00004048 | Train Acc: 92.26% | Val Acc: 92.91%\n",
      "Epoch 411/50000 | Data size: 776.6965604689868 | Train Loss: 0.00004198 | Val Loss: 0.00004064 | Train Acc: 92.57% | Val Acc: 92.89%\n",
      "Epoch 412/50000 | Data size: 780.5800432713318 | Train Loss: 0.00004414 | Val Loss: 0.00004035 | Train Acc: 92.17% | Val Acc: 92.92%\n",
      "Epoch 413/50000 | Data size: 784.4829434876884 | Train Loss: 0.00004374 | Val Loss: 0.00003992 | Train Acc: 92.25% | Val Acc: 93.01%\n",
      "Epoch 414/50000 | Data size: 788.4053582051268 | Train Loss: 0.00004152 | Val Loss: 0.00004006 | Train Acc: 92.67% | Val Acc: 93.01%\n",
      "Epoch 415/50000 | Data size: 792.3473849961525 | Train Loss: 0.00004130 | Val Loss: 0.00003975 | Train Acc: 92.72% | Val Acc: 93.06%\n",
      "Epoch 416/50000 | Data size: 796.3091219211333 | Train Loss: 0.00003969 | Val Loss: 0.00003960 | Train Acc: 93.01% | Val Acc: 93.09%\n",
      "Epoch 417/50000 | Data size: 800.2906675307389 | Train Loss: 0.00004166 | Val Loss: 0.00003957 | Train Acc: 92.65% | Val Acc: 93.08%\n",
      "Epoch 418/50000 | Data size: 804.2921208683927 | Train Loss: 0.00004126 | Val Loss: 0.00003931 | Train Acc: 92.74% | Val Acc: 93.14%\n",
      "Epoch 419/50000 | Data size: 808.3135814727347 | Train Loss: 0.00003948 | Val Loss: 0.00003918 | Train Acc: 93.05% | Val Acc: 93.16%\n",
      "Epoch 420/50000 | Data size: 812.3551493800983 | Train Loss: 0.00003958 | Val Loss: 0.00003896 | Train Acc: 93.04% | Val Acc: 93.20%\n",
      "Epoch 421/50000 | Data size: 816.4169251269988 | Train Loss: 0.00004044 | Val Loss: 0.00003914 | Train Acc: 92.89% | Val Acc: 93.19%\n",
      "Epoch 422/50000 | Data size: 820.4990097526338 | Train Loss: 0.00004080 | Val Loss: 0.00003882 | Train Acc: 92.82% | Val Acc: 93.23%\n",
      "Epoch 423/50000 | Data size: 824.601504801397 | Train Loss: 0.00003969 | Val Loss: 0.00003866 | Train Acc: 93.02% | Val Acc: 93.26%\n",
      "Epoch 424/50000 | Data size: 828.724512325404 | Train Loss: 0.00004082 | Val Loss: 0.00003859 | Train Acc: 92.81% | Val Acc: 93.29%\n",
      "Epoch 425/50000 | Data size: 832.868134887031 | Train Loss: 0.00004103 | Val Loss: 0.00003828 | Train Acc: 92.78% | Val Acc: 93.35%\n",
      "Epoch 426/50000 | Data size: 837.0324755614662 | Train Loss: 0.00004033 | Val Loss: 0.00003834 | Train Acc: 92.91% | Val Acc: 93.33%\n",
      "Epoch 427/50000 | Data size: 841.2176379392736 | Train Loss: 0.00003893 | Val Loss: 0.00003852 | Train Acc: 93.16% | Val Acc: 93.30%\n",
      "Epoch 428/50000 | Data size: 845.42372612897 | Train Loss: 0.00003916 | Val Loss: 0.00003801 | Train Acc: 93.15% | Val Acc: 93.40%\n",
      "Epoch 429/50000 | Data size: 849.6508447596149 | Train Loss: 0.00003917 | Val Loss: 0.00003779 | Train Acc: 93.13% | Val Acc: 93.43%\n",
      "Epoch 430/50000 | Data size: 853.899098983413 | Train Loss: 0.00004022 | Val Loss: 0.00003772 | Train Acc: 92.93% | Val Acc: 93.45%\n",
      "Epoch 431/50000 | Data size: 858.16859447833 | Train Loss: 0.00003773 | Val Loss: 0.00003753 | Train Acc: 93.40% | Val Acc: 93.48%\n",
      "Epoch 432/50000 | Data size: 862.4594374507216 | Train Loss: 0.00003832 | Val Loss: 0.00003748 | Train Acc: 93.28% | Val Acc: 93.49%\n",
      "Epoch 433/50000 | Data size: 866.7717346379752 | Train Loss: 0.00003803 | Val Loss: 0.00003736 | Train Acc: 93.35% | Val Acc: 93.52%\n",
      "Epoch 434/50000 | Data size: 871.1055933111651 | Train Loss: 0.00003858 | Val Loss: 0.00003799 | Train Acc: 93.25% | Val Acc: 93.41%\n",
      "Epoch 435/50000 | Data size: 875.4611212777208 | Train Loss: 0.00003852 | Val Loss: 0.00003750 | Train Acc: 93.26% | Val Acc: 93.51%\n",
      "Epoch 436/50000 | Data size: 879.8384268841095 | Train Loss: 0.00003872 | Val Loss: 0.00003711 | Train Acc: 93.21% | Val Acc: 93.57%\n",
      "Epoch 437/50000 | Data size: 884.23761901853 | Train Loss: 0.00003900 | Val Loss: 0.00003693 | Train Acc: 93.17% | Val Acc: 93.59%\n",
      "Epoch 438/50000 | Data size: 888.6588071136227 | Train Loss: 0.00003776 | Val Loss: 0.00003676 | Train Acc: 93.41% | Val Acc: 93.63%\n",
      "Epoch 439/50000 | Data size: 893.1021011491908 | Train Loss: 0.00003858 | Val Loss: 0.00003680 | Train Acc: 93.24% | Val Acc: 93.63%\n",
      "Epoch 440/50000 | Data size: 897.5676116549369 | Train Loss: 0.00003673 | Val Loss: 0.00003648 | Train Acc: 93.58% | Val Acc: 93.68%\n",
      "Epoch 441/50000 | Data size: 902.0554497132115 | Train Loss: 0.00003643 | Val Loss: 0.00003669 | Train Acc: 93.65% | Val Acc: 93.64%\n",
      "Epoch 442/50000 | Data size: 906.5657269617775 | Train Loss: 0.00003775 | Val Loss: 0.00003621 | Train Acc: 93.40% | Val Acc: 93.71%\n",
      "Epoch 443/50000 | Data size: 911.0985555965864 | Train Loss: 0.00003602 | Val Loss: 0.00003625 | Train Acc: 93.72% | Val Acc: 93.73%\n",
      "Epoch 444/50000 | Data size: 915.6540483745694 | Train Loss: 0.00003723 | Val Loss: 0.00003612 | Train Acc: 93.50% | Val Acc: 93.76%\n",
      "Epoch 445/50000 | Data size: 920.2323186164422 | Train Loss: 0.00003768 | Val Loss: 0.00003598 | Train Acc: 93.42% | Val Acc: 93.78%\n",
      "Epoch 446/50000 | Data size: 924.8334802095244 | Train Loss: 0.00003834 | Val Loss: 0.00003577 | Train Acc: 93.31% | Val Acc: 93.81%\n",
      "Epoch 447/50000 | Data size: 929.457647610572 | Train Loss: 0.00003546 | Val Loss: 0.00003562 | Train Acc: 93.82% | Val Acc: 93.83%\n",
      "Epoch 448/50000 | Data size: 934.1049358486249 | Train Loss: 0.00003700 | Val Loss: 0.00003684 | Train Acc: 93.54% | Val Acc: 93.63%\n",
      "Epoch 449/50000 | Data size: 938.7754605278681 | Train Loss: 0.00003737 | Val Loss: 0.00003579 | Train Acc: 93.47% | Val Acc: 93.81%\n",
      "Epoch 450/50000 | Data size: 943.4693378305074 | Train Loss: 0.00003763 | Val Loss: 0.00003545 | Train Acc: 93.44% | Val Acc: 93.86%\n",
      "Epoch 451/50000 | Data size: 948.1866845196599 | Train Loss: 0.00003560 | Val Loss: 0.00003529 | Train Acc: 93.80% | Val Acc: 93.89%\n",
      "Epoch 452/50000 | Data size: 952.9276179422583 | Train Loss: 0.00003524 | Val Loss: 0.00003519 | Train Acc: 93.87% | Val Acc: 93.91%\n",
      "Epoch 453/50000 | Data size: 957.6922560319696 | Train Loss: 0.00003555 | Val Loss: 0.00003491 | Train Acc: 93.81% | Val Acc: 93.97%\n",
      "Epoch 454/50000 | Data size: 962.4807173121294 | Train Loss: 0.00003642 | Val Loss: 0.00003491 | Train Acc: 93.66% | Val Acc: 93.97%\n",
      "Epoch 455/50000 | Data size: 967.2931208986901 | Train Loss: 0.00003753 | Val Loss: 0.00003572 | Train Acc: 93.46% | Val Acc: 93.84%\n",
      "Epoch 456/50000 | Data size: 972.1295865031835 | Train Loss: 0.00003628 | Val Loss: 0.00003488 | Train Acc: 93.67% | Val Acc: 93.97%\n",
      "Epoch 457/50000 | Data size: 976.9902344356994 | Train Loss: 0.00003451 | Val Loss: 0.00003493 | Train Acc: 94.00% | Val Acc: 93.98%\n",
      "Epoch 458/50000 | Data size: 981.8751856078779 | Train Loss: 0.00003575 | Val Loss: 0.00003448 | Train Acc: 93.78% | Val Acc: 94.06%\n",
      "Epoch 459/50000 | Data size: 986.7845615359173 | Train Loss: 0.00003545 | Val Loss: 0.00003425 | Train Acc: 93.84% | Val Acc: 94.07%\n",
      "Epoch 460/50000 | Data size: 991.7184843435969 | Train Loss: 0.00003527 | Val Loss: 0.00003419 | Train Acc: 93.85% | Val Acc: 94.10%\n",
      "Epoch 461/50000 | Data size: 996.6770767653148 | Train Loss: 0.00003627 | Val Loss: 0.00003414 | Train Acc: 93.68% | Val Acc: 94.10%\n",
      "Epoch 462/50000 | Data size: 1001.6604621491414 | Train Loss: 0.00003557 | Val Loss: 0.00003418 | Train Acc: 93.80% | Val Acc: 94.08%\n",
      "Epoch 463/50000 | Data size: 1006.6687644598871 | Train Loss: 0.00003472 | Val Loss: 0.00003413 | Train Acc: 93.95% | Val Acc: 94.11%\n",
      "Epoch 464/50000 | Data size: 1011.7021082821866 | Train Loss: 0.00003610 | Val Loss: 0.00003365 | Train Acc: 93.71% | Val Acc: 94.17%\n",
      "Epoch 465/50000 | Data size: 1016.7606188235975 | Train Loss: 0.00003405 | Val Loss: 0.00003375 | Train Acc: 94.07% | Val Acc: 94.17%\n",
      "Epoch 466/50000 | Data size: 1021.8444219177155 | Train Loss: 0.00003549 | Val Loss: 0.00003348 | Train Acc: 93.81% | Val Acc: 94.21%\n",
      "Epoch 467/50000 | Data size: 1026.953644027304 | Train Loss: 0.00003364 | Val Loss: 0.00003333 | Train Acc: 94.15% | Val Acc: 94.24%\n",
      "Epoch 468/50000 | Data size: 1032.0884122474406 | Train Loss: 0.00003371 | Val Loss: 0.00003358 | Train Acc: 94.14% | Val Acc: 94.19%\n",
      "Epoch 469/50000 | Data size: 1037.248854308678 | Train Loss: 0.00003406 | Val Loss: 0.00003336 | Train Acc: 94.05% | Val Acc: 94.22%\n",
      "Epoch 470/50000 | Data size: 1042.4350985802214 | Train Loss: 0.00003471 | Val Loss: 0.00003319 | Train Acc: 93.95% | Val Acc: 94.26%\n",
      "Epoch 471/50000 | Data size: 1047.6472740731224 | Train Loss: 0.00003407 | Val Loss: 0.00003295 | Train Acc: 94.07% | Val Acc: 94.29%\n",
      "Epoch 472/50000 | Data size: 1052.885510443488 | Train Loss: 0.00003404 | Val Loss: 0.00003285 | Train Acc: 94.08% | Val Acc: 94.32%\n",
      "Epoch 473/50000 | Data size: 1058.1499379957056 | Train Loss: 0.00003255 | Val Loss: 0.00003279 | Train Acc: 94.34% | Val Acc: 94.34%\n",
      "Epoch 474/50000 | Data size: 1063.4406876856842 | Train Loss: 0.00003266 | Val Loss: 0.00003289 | Train Acc: 94.32% | Val Acc: 94.30%\n",
      "Epoch 475/50000 | Data size: 1068.7578911241126 | Train Loss: 0.00003380 | Val Loss: 0.00003280 | Train Acc: 94.12% | Val Acc: 94.33%\n",
      "Epoch 476/50000 | Data size: 1074.101680579733 | Train Loss: 0.00003384 | Val Loss: 0.00003242 | Train Acc: 94.10% | Val Acc: 94.39%\n",
      "Epoch 477/50000 | Data size: 1079.4721889826317 | Train Loss: 0.00003406 | Val Loss: 0.00003243 | Train Acc: 94.07% | Val Acc: 94.40%\n",
      "Epoch 478/50000 | Data size: 1084.8695499275448 | Train Loss: 0.00003337 | Val Loss: 0.00003220 | Train Acc: 94.20% | Val Acc: 94.42%\n",
      "Epoch 479/50000 | Data size: 1090.2938976771825 | Train Loss: 0.00003255 | Val Loss: 0.00003205 | Train Acc: 94.34% | Val Acc: 94.45%\n",
      "Epoch 480/50000 | Data size: 1095.7453671655685 | Train Loss: 0.00003181 | Val Loss: 0.00003262 | Train Acc: 94.45% | Val Acc: 94.33%\n",
      "Epoch 481/50000 | Data size: 1101.2240940013962 | Train Loss: 0.00003262 | Val Loss: 0.00003207 | Train Acc: 94.32% | Val Acc: 94.47%\n",
      "Epoch 482/50000 | Data size: 1106.7302144714033 | Train Loss: 0.00003189 | Val Loss: 0.00003175 | Train Acc: 94.45% | Val Acc: 94.50%\n",
      "Epoch 483/50000 | Data size: 1112.2638655437604 | Train Loss: 0.00003312 | Val Loss: 0.00003167 | Train Acc: 94.23% | Val Acc: 94.52%\n",
      "Epoch 484/50000 | Data size: 1117.8251848714792 | Train Loss: 0.00003262 | Val Loss: 0.00003147 | Train Acc: 94.32% | Val Acc: 94.55%\n",
      "Epoch 485/50000 | Data size: 1123.4143107958366 | Train Loss: 0.00003260 | Val Loss: 0.00003146 | Train Acc: 94.31% | Val Acc: 94.56%\n",
      "Epoch 486/50000 | Data size: 1129.0313823498159 | Train Loss: 0.00003312 | Val Loss: 0.00003176 | Train Acc: 94.22% | Val Acc: 94.53%\n",
      "Epoch 487/50000 | Data size: 1134.676539261565 | Train Loss: 0.00003180 | Val Loss: 0.00003124 | Train Acc: 94.47% | Val Acc: 94.59%\n",
      "Epoch 488/50000 | Data size: 1140.3499219578728 | Train Loss: 0.00003095 | Val Loss: 0.00003106 | Train Acc: 94.62% | Val Acc: 94.62%\n",
      "Epoch 489/50000 | Data size: 1146.0516715676622 | Train Loss: 0.00003042 | Val Loss: 0.00003101 | Train Acc: 94.71% | Val Acc: 94.64%\n",
      "Epoch 490/50000 | Data size: 1151.7819299255004 | Train Loss: 0.00003161 | Val Loss: 0.00003081 | Train Acc: 94.50% | Val Acc: 94.66%\n",
      "Epoch 491/50000 | Data size: 1157.5408395751278 | Train Loss: 0.00003111 | Val Loss: 0.00003075 | Train Acc: 94.59% | Val Acc: 94.68%\n",
      "Epoch 492/50000 | Data size: 1163.3285437730035 | Train Loss: 0.00003075 | Val Loss: 0.00003077 | Train Acc: 94.64% | Val Acc: 94.66%\n",
      "Epoch 493/50000 | Data size: 1169.1451864918686 | Train Loss: 0.00003093 | Val Loss: 0.00003079 | Train Acc: 94.62% | Val Acc: 94.67%\n",
      "Epoch 494/50000 | Data size: 1174.990912424328 | Train Loss: 0.00003058 | Val Loss: 0.00003049 | Train Acc: 94.67% | Val Acc: 94.72%\n",
      "Epoch 495/50000 | Data size: 1180.8658669864496 | Train Loss: 0.00003142 | Val Loss: 0.00003054 | Train Acc: 94.53% | Val Acc: 94.72%\n",
      "Epoch 496/50000 | Data size: 1186.770196321382 | Train Loss: 0.00003097 | Val Loss: 0.00003032 | Train Acc: 94.61% | Val Acc: 94.75%\n",
      "Epoch 497/50000 | Data size: 1192.7040473029888 | Train Loss: 0.00003145 | Val Loss: 0.00003056 | Train Acc: 94.52% | Val Acc: 94.69%\n",
      "Epoch 498/50000 | Data size: 1198.6675675395038 | Train Loss: 0.00003076 | Val Loss: 0.00003007 | Train Acc: 94.65% | Val Acc: 94.79%\n",
      "Epoch 499/50000 | Data size: 1204.6609053772013 | Train Loss: 0.00003069 | Val Loss: 0.00003038 | Train Acc: 94.65% | Val Acc: 94.75%\n",
      "Epoch 500/50000 | Data size: 1210.6842099040873 | Train Loss: 0.00003045 | Val Loss: 0.00003025 | Train Acc: 94.70% | Val Acc: 94.76%\n",
      "Epoch 501/50000 | Data size: 1216.7376309536078 | Train Loss: 0.00003076 | Val Loss: 0.00002981 | Train Acc: 94.64% | Val Acc: 94.83%\n",
      "Epoch 502/50000 | Data size: 1222.8213191083757 | Train Loss: 0.00002906 | Val Loss: 0.00002974 | Train Acc: 94.95% | Val Acc: 94.85%\n",
      "Epoch 503/50000 | Data size: 1228.9354257039176 | Train Loss: 0.00003100 | Val Loss: 0.00002968 | Train Acc: 94.60% | Val Acc: 94.85%\n",
      "Epoch 504/50000 | Data size: 1235.0801028324372 | Train Loss: 0.00003049 | Val Loss: 0.00002949 | Train Acc: 94.69% | Val Acc: 94.89%\n",
      "Epoch 505/50000 | Data size: 1241.2555033465994 | Train Loss: 0.00002909 | Val Loss: 0.00002950 | Train Acc: 94.94% | Val Acc: 94.88%\n",
      "Epoch 506/50000 | Data size: 1247.4617808633325 | Train Loss: 0.00002966 | Val Loss: 0.00002939 | Train Acc: 94.84% | Val Acc: 94.91%\n",
      "Epoch 507/50000 | Data size: 1253.6990897676492 | Train Loss: 0.00002885 | Val Loss: 0.00002929 | Train Acc: 94.97% | Val Acc: 94.92%\n",
      "Epoch 508/50000 | Data size: 1259.9675852164874 | Train Loss: 0.00002974 | Val Loss: 0.00002933 | Train Acc: 94.83% | Val Acc: 94.92%\n",
      "Epoch 509/50000 | Data size: 1266.26742314257 | Train Loss: 0.00003036 | Val Loss: 0.00002903 | Train Acc: 94.71% | Val Acc: 94.96%\n",
      "Epoch 510/50000 | Data size: 1272.5987602582827 | Train Loss: 0.00002912 | Val Loss: 0.00002891 | Train Acc: 94.93% | Val Acc: 94.98%\n",
      "Epoch 511/50000 | Data size: 1278.961754059574 | Train Loss: 0.00002996 | Val Loss: 0.00002891 | Train Acc: 94.78% | Val Acc: 95.00%\n",
      "Epoch 512/50000 | Data size: 1285.356562829872 | Train Loss: 0.00002918 | Val Loss: 0.00002878 | Train Acc: 94.92% | Val Acc: 95.01%\n",
      "Epoch 513/50000 | Data size: 1291.7833456440214 | Train Loss: 0.00002850 | Val Loss: 0.00002881 | Train Acc: 95.04% | Val Acc: 95.00%\n",
      "Epoch 514/50000 | Data size: 1298.2422623722416 | Train Loss: 0.00002889 | Val Loss: 0.00002862 | Train Acc: 94.97% | Val Acc: 95.04%\n",
      "Epoch 515/50000 | Data size: 1304.7334736841028 | Train Loss: 0.00002851 | Val Loss: 0.00002848 | Train Acc: 95.03% | Val Acc: 95.06%\n",
      "Epoch 516/50000 | Data size: 1311.2571410525234 | Train Loss: 0.00002920 | Val Loss: 0.00002839 | Train Acc: 94.91% | Val Acc: 95.08%\n",
      "Epoch 517/50000 | Data size: 1317.813426757786 | Train Loss: 0.00002934 | Val Loss: 0.00002818 | Train Acc: 94.88% | Val Acc: 95.11%\n",
      "Epoch 518/50000 | Data size: 1324.4024938915747 | Train Loss: 0.00002801 | Val Loss: 0.00002860 | Train Acc: 95.13% | Val Acc: 95.04%\n",
      "Epoch 519/50000 | Data size: 1331.0245063610325 | Train Loss: 0.00002917 | Val Loss: 0.00002817 | Train Acc: 94.92% | Val Acc: 95.11%\n",
      "Epoch 520/50000 | Data size: 1337.6796288928376 | Train Loss: 0.00002841 | Val Loss: 0.00002798 | Train Acc: 95.05% | Val Acc: 95.14%\n",
      "Epoch 521/50000 | Data size: 1344.3680270373018 | Train Loss: 0.00002766 | Val Loss: 0.00002785 | Train Acc: 95.19% | Val Acc: 95.16%\n",
      "Epoch 522/50000 | Data size: 1351.0898671724883 | Train Loss: 0.00002747 | Val Loss: 0.00002770 | Train Acc: 95.22% | Val Acc: 95.19%\n",
      "Epoch 523/50000 | Data size: 1357.8453165083508 | Train Loss: 0.00002798 | Val Loss: 0.00002777 | Train Acc: 95.13% | Val Acc: 95.19%\n",
      "Epoch 524/50000 | Data size: 1364.6345430908925 | Train Loss: 0.00002770 | Val Loss: 0.00002755 | Train Acc: 95.18% | Val Acc: 95.22%\n",
      "Epoch 525/50000 | Data size: 1371.457715806347 | Train Loss: 0.00002820 | Val Loss: 0.00002753 | Train Acc: 95.09% | Val Acc: 95.22%\n",
      "Epoch 526/50000 | Data size: 1378.3150043853789 | Train Loss: 0.00002745 | Val Loss: 0.00002737 | Train Acc: 95.22% | Val Acc: 95.25%\n",
      "Epoch 527/50000 | Data size: 1385.2065794073058 | Train Loss: 0.00002794 | Val Loss: 0.00002771 | Train Acc: 95.13% | Val Acc: 95.19%\n",
      "Epoch 528/50000 | Data size: 1392.1326123043423 | Train Loss: 0.00002745 | Val Loss: 0.00002729 | Train Acc: 95.22% | Val Acc: 95.27%\n",
      "Epoch 529/50000 | Data size: 1399.093275365864 | Train Loss: 0.00002683 | Val Loss: 0.00002719 | Train Acc: 95.33% | Val Acc: 95.28%\n",
      "Epoch 530/50000 | Data size: 1406.0887417426934 | Train Loss: 0.00002697 | Val Loss: 0.00002695 | Train Acc: 95.30% | Val Acc: 95.32%\n",
      "Epoch 531/50000 | Data size: 1413.1191854514068 | Train Loss: 0.00002652 | Val Loss: 0.00002681 | Train Acc: 95.38% | Val Acc: 95.34%\n",
      "Epoch 532/50000 | Data size: 1420.1847813786637 | Train Loss: 0.00002701 | Val Loss: 0.00002689 | Train Acc: 95.29% | Val Acc: 95.34%\n",
      "Epoch 533/50000 | Data size: 1427.285705285557 | Train Loss: 0.00002652 | Val Loss: 0.00002676 | Train Acc: 95.39% | Val Acc: 95.37%\n",
      "Epoch 534/50000 | Data size: 1434.4221338119848 | Train Loss: 0.00002631 | Val Loss: 0.00002652 | Train Acc: 95.42% | Val Acc: 95.39%\n",
      "Epoch 535/50000 | Data size: 1441.5942444810448 | Train Loss: 0.00002707 | Val Loss: 0.00002651 | Train Acc: 95.28% | Val Acc: 95.39%\n",
      "Epoch 536/50000 | Data size: 1448.80221570345 | Train Loss: 0.00002745 | Val Loss: 0.00002899 | Train Acc: 95.21% | Val Acc: 94.95%\n",
      "Epoch 537/50000 | Data size: 1456.0462267819673 | Train Loss: 0.00002953 | Val Loss: 0.00002685 | Train Acc: 94.84% | Val Acc: 95.33%\n",
      "Epoch 538/50000 | Data size: 1463.3264579158772 | Train Loss: 0.00002721 | Val Loss: 0.00002647 | Train Acc: 95.26% | Val Acc: 95.41%\n",
      "Epoch 539/50000 | Data size: 1470.6430902054565 | Train Loss: 0.00002654 | Val Loss: 0.00002639 | Train Acc: 95.38% | Val Acc: 95.42%\n",
      "Epoch 540/50000 | Data size: 1477.9963056564839 | Train Loss: 0.00002702 | Val Loss: 0.00002609 | Train Acc: 95.28% | Val Acc: 95.46%\n",
      "Epoch 541/50000 | Data size: 1485.3862871847664 | Train Loss: 0.00002601 | Val Loss: 0.00002610 | Train Acc: 95.47% | Val Acc: 95.46%\n",
      "Epoch 542/50000 | Data size: 1492.8132186206901 | Train Loss: 0.00002549 | Val Loss: 0.00002602 | Train Acc: 95.56% | Val Acc: 95.49%\n",
      "Epoch 543/50000 | Data size: 1500.2772847137935 | Train Loss: 0.00002594 | Val Loss: 0.00002574 | Train Acc: 95.47% | Val Acc: 95.52%\n",
      "Epoch 544/50000 | Data size: 1507.7786711373624 | Train Loss: 0.00002592 | Val Loss: 0.00002587 | Train Acc: 95.48% | Val Acc: 95.50%\n",
      "Epoch 545/50000 | Data size: 1515.3175644930493 | Train Loss: 0.00002623 | Val Loss: 0.00002581 | Train Acc: 95.42% | Val Acc: 95.51%\n",
      "Epoch 546/50000 | Data size: 1522.8941523155145 | Train Loss: 0.00002613 | Val Loss: 0.00002559 | Train Acc: 95.45% | Val Acc: 95.55%\n",
      "Epoch 547/50000 | Data size: 1530.5086230770921 | Train Loss: 0.00002609 | Val Loss: 0.00002547 | Train Acc: 95.44% | Val Acc: 95.57%\n",
      "Epoch 548/50000 | Data size: 1538.1611661924776 | Train Loss: 0.00002598 | Val Loss: 0.00002537 | Train Acc: 95.46% | Val Acc: 95.58%\n",
      "Epoch 549/50000 | Data size: 1545.8519720234399 | Train Loss: 0.00002430 | Val Loss: 0.00002533 | Train Acc: 95.76% | Val Acc: 95.59%\n",
      "Epoch 550/50000 | Data size: 1553.5812318835572 | Train Loss: 0.00002509 | Val Loss: 0.00002522 | Train Acc: 95.62% | Val Acc: 95.60%\n",
      "Epoch 551/50000 | Data size: 1561.349138042975 | Train Loss: 0.00002546 | Val Loss: 0.00002520 | Train Acc: 95.56% | Val Acc: 95.62%\n",
      "Epoch 552/50000 | Data size: 1569.15588373319 | Train Loss: 0.00002474 | Val Loss: 0.00002512 | Train Acc: 95.69% | Val Acc: 95.63%\n",
      "Epoch 553/50000 | Data size: 1577.0016631518558 | Train Loss: 0.00002450 | Val Loss: 0.00002565 | Train Acc: 95.73% | Val Acc: 95.54%\n",
      "Epoch 554/50000 | Data size: 1584.886671467615 | Train Loss: 0.00002593 | Val Loss: 0.00002495 | Train Acc: 95.47% | Val Acc: 95.67%\n",
      "Epoch 555/50000 | Data size: 1592.8111048249532 | Train Loss: 0.00002463 | Val Loss: 0.00002482 | Train Acc: 95.70% | Val Acc: 95.68%\n",
      "Epoch 556/50000 | Data size: 1600.7751603490779 | Train Loss: 0.00002477 | Val Loss: 0.00002473 | Train Acc: 95.69% | Val Acc: 95.70%\n",
      "Epoch 557/50000 | Data size: 1608.7790361508232 | Train Loss: 0.00002529 | Val Loss: 0.00002457 | Train Acc: 95.58% | Val Acc: 95.72%\n",
      "Epoch 558/50000 | Data size: 1616.8229313315774 | Train Loss: 0.00002465 | Val Loss: 0.00002458 | Train Acc: 95.69% | Val Acc: 95.71%\n",
      "Epoch 559/50000 | Data size: 1624.9070459882353 | Train Loss: 0.00002431 | Val Loss: 0.00002457 | Train Acc: 95.76% | Val Acc: 95.72%\n",
      "Epoch 560/50000 | Data size: 1633.0315812181764 | Train Loss: 0.00002462 | Val Loss: 0.00002438 | Train Acc: 95.70% | Val Acc: 95.75%\n",
      "Epoch 561/50000 | Data size: 1641.1967391242672 | Train Loss: 0.00002421 | Val Loss: 0.00002472 | Train Acc: 95.77% | Val Acc: 95.70%\n",
      "Epoch 562/50000 | Data size: 1649.4027228198886 | Train Loss: 0.00002400 | Val Loss: 0.00002424 | Train Acc: 95.81% | Val Acc: 95.77%\n",
      "Epoch 563/50000 | Data size: 1657.649736433988 | Train Loss: 0.00002423 | Val Loss: 0.00002418 | Train Acc: 95.76% | Val Acc: 95.79%\n",
      "Epoch 564/50000 | Data size: 1665.937985116158 | Train Loss: 0.00002440 | Val Loss: 0.00002403 | Train Acc: 95.74% | Val Acc: 95.81%\n",
      "Epoch 565/50000 | Data size: 1674.2676750417388 | Train Loss: 0.00002340 | Val Loss: 0.00002434 | Train Acc: 95.92% | Val Acc: 95.75%\n",
      "Epoch 566/50000 | Data size: 1682.6390134169476 | Train Loss: 0.00002445 | Val Loss: 0.00002389 | Train Acc: 95.72% | Val Acc: 95.83%\n",
      "Epoch 567/50000 | Data size: 1691.0522084840322 | Train Loss: 0.00002348 | Val Loss: 0.00002384 | Train Acc: 95.90% | Val Acc: 95.85%\n",
      "Epoch 568/50000 | Data size: 1699.5074695264525 | Train Loss: 0.00002307 | Val Loss: 0.00002372 | Train Acc: 95.97% | Val Acc: 95.87%\n",
      "Epoch 569/50000 | Data size: 1708.0050068740848 | Train Loss: 0.00002322 | Val Loss: 0.00002384 | Train Acc: 95.95% | Val Acc: 95.84%\n",
      "Epoch 570/50000 | Data size: 1716.5450319084553 | Train Loss: 0.00002367 | Val Loss: 0.00002356 | Train Acc: 95.87% | Val Acc: 95.90%\n",
      "Epoch 571/50000 | Data size: 1725.1277570679974 | Train Loss: 0.00002355 | Val Loss: 0.00002348 | Train Acc: 95.88% | Val Acc: 95.91%\n"
     ]
    }
   ],
   "source": [
    "#EMBEDDINGS AUTOENCODER >90% accuracy\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Custom Dataset\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.data = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".json\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    if isinstance(json_data, dict):  # To handle different structures\n",
    "                        json_data = [json_data]\n",
    "                    for item in json_data:\n",
    "                        embedding = item['embedding']\n",
    "                        temp = float(item['configuration']['temperature'])\n",
    "                        top_p = float(item['configuration']['top_p'])\n",
    "                        self.data.append((embedding, temp, top_p))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding, temp, top_p = self.data[idx]\n",
    "        return torch.tensor(embedding, dtype=torch.float32), temp, top_p\n",
    "\n",
    "# Define the Autoencoder model using a pretrained BERT model as the encoder\n",
    "class BertAutoencoder(nn.Module):\n",
    "    def __init__(self, bert_model_name, embedding_dim=3072, hidden_dim=768, lstm_units=256, sequence_length=4):\n",
    "        super(BertAutoencoder, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embedding_adapter = nn.Linear(embedding_dim, hidden_dim * sequence_length)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.decoder = nn.LSTM(hidden_dim, lstm_units, batch_first=True)\n",
    "        self.output_layer = nn.Linear(lstm_units, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_adapter(x)\n",
    "        x = x.view(x.size(0), self.sequence_length, -1)  # Reshape to (batch_size, sequence_length, hidden_dim)\n",
    "        encoder_outputs = self.bert(inputs_embeds=x).last_hidden_state\n",
    "        decoder_outputs, _ = self.decoder(encoder_outputs)\n",
    "        output = self.output_layer(decoder_outputs)\n",
    "        return output\n",
    "\n",
    "# Calculate accuracy as a similarity measure\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    cos = nn.CosineSimilarity(dim=-1)\n",
    "    accuracy = cos(outputs, targets).mean().item()\n",
    "    return accuracy * 100  # Convert to percentage\n",
    "\n",
    "# Progressive data increment method\n",
    "def progressive_training(model, dataset, initial_size, increment_ratio, max_epochs, device):\n",
    "    total_size = len(dataset)\n",
    "    test_size = 0.1\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    metrics = []\n",
    "\n",
    "    # Separate the test data and ensure it's shuffled\n",
    "    train_size = int((1 - test_size) * total_size)\n",
    "    test_size = total_size - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    current_size = initial_size\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        if current_size > train_size:\n",
    "            break\n",
    "\n",
    "        current_train_indices = torch.randperm(train_size)[:int(current_size)]\n",
    "        current_train_dataset = torch.utils.data.Subset(train_dataset, current_train_indices)\n",
    "        train_loader = DataLoader(current_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        for inputs, _, _ in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += calculate_accuracy(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "\n",
    "        train_accuracy /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, _, _ in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += calculate_accuracy(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "\n",
    "        val_accuracy /= len(test_loader)\n",
    "\n",
    "        metrics.append((current_size, train_loss/len(train_loader), val_loss/len(test_loader), train_accuracy, val_accuracy))\n",
    "        current_size += current_size * increment_ratio\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{max_epochs} | Data size: {current_size} | Train Loss: {train_loss/len(train_loader):.8f} | Val Loss: {val_loss/len(test_loader):.8f} | Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Plotting function\n",
    "def plot_metrics(metrics):\n",
    "    sizes, train_losses, val_losses, train_accuracies, val_accuracies = zip(*metrics)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(sizes, train_losses, label='Training Loss')\n",
    "    plt.plot(sizes, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss vs Data Size')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(sizes, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(sizes, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy vs Data Size')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot temp/top_p vs accuracy\n",
    "def plot_temp_top_p_vs_accuracy(dataset, model, device):\n",
    "    temps, top_ps, accuracies = [], [], []\n",
    "\n",
    "    test_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, temp, top_p in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            accuracy = calculate_accuracy(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "\n",
    "            temps.append(temp.item())\n",
    "            top_ps.append(top_p.item())\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(temps, accuracies, alpha=0.5)\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Temperature')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(top_ps, accuracies, alpha=0.5)\n",
    "    plt.xlabel('Top_p')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Top_p')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "def main(folder_path):\n",
    "    dataset = EmbeddingDataset(folder_path)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BertAutoencoder('bert-base-uncased', lstm_units=256, hidden_dim=768, sequence_length=4).to(device)\n",
    "\n",
    "    initial_size = 100\n",
    "    increment_ratio = 0.005\n",
    "    max_epochs = 50000\n",
    "\n",
    "    metrics = progressive_training(model, dataset, initial_size, increment_ratio, max_epochs, device)\n",
    "\n",
    "    # Save the metrics to a file\n",
    "    with open('training_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'bert_autoencoder_model.pth')\n",
    "\n",
    "    plot_metrics(metrics)\n",
    "    plot_temp_top_p_vs_accuracy(dataset, model, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = '/workspace/slice-monorepo/thebeast/combined'  # Replace with the path to your folder containing JSON files\n",
    "    main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09c77d-1f1d-48b7-aa77-bc41e00acc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZE EMBEDDINGS FROM ABOVE MODEL\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize': (14, 10)})\n",
    "\n",
    "class BertAutoencoder(nn.Module):\n",
    "    def __init__(self, bert_model_name, embedding_dim=3072, hidden_dim=768, lstm_units=256, sequence_length=4):\n",
    "        super(BertAutoencoder, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embedding_adapter = nn.Linear(embedding_dim, hidden_dim * sequence_length)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.decoder = nn.LSTM(hidden_dim, lstm_units, batch_first=True)\n",
    "        self.output_layer = nn.Linear(lstm_units, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_adapter(x)\n",
    "        x = x.view(x.size(0), self.sequence_length, -1)  # Reshape to (batch_size, sequence_length, hidden_dim)\n",
    "        encoder_outputs = self.bert(inputs_embeds=x).last_hidden_state\n",
    "        decoder_outputs, _ = self.decoder(encoder_outputs)\n",
    "        output = self.output_layer(decoder_outputs)\n",
    "        return output\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.embedding_adapter(x)\n",
    "        x = x.view(x.size(0), self.sequence_length, -1)  # Reshape to (batch_size, sequence_length, hidden_dim)\n",
    "        encoder_outputs = self.bert(inputs_embeds=x).last_hidden_state\n",
    "        return encoder_outputs\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    model = BertAutoencoder('bert-base-uncased', lstm_units=256, hidden_dim=768, sequence_length=4).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def process_embedding(model, embedding, device):\n",
    "    embedding = torch.tensor(embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        encoded = model.encode(embedding)\n",
    "    return encoded.squeeze(0).cpu().numpy()\n",
    "\n",
    "def load_embedding_data(folder_path):\n",
    "    embeddings = []\n",
    "    temps = []\n",
    "    top_ps = []\n",
    "    configs = []\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "                if isinstance(json_data, dict):  # To handle different structures\n",
    "                    json_data = [json_data]\n",
    "                for item in json_data:\n",
    "                    embedding = item['embedding']\n",
    "                    temp = float(item['configuration']['temperature'])\n",
    "                    top_p = float(item['configuration']['top_p'])\n",
    "                    embeddings.append(embedding)\n",
    "                    temps.append(temp)\n",
    "                    top_ps.append(top_p)\n",
    "                    configs.append(item['configuration'])\n",
    "                    data.append(item)\n",
    "    return embeddings, temps, top_ps, configs, data\n",
    "\n",
    "def umap_embeddings(encoded_embeddings, temps, top_ps):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(encoded_embeddings)\n",
    "\n",
    "    reducer = umap.UMAP()\n",
    "    umap_embeddings = reducer.fit_transform(scaled_embeddings)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=temps, cmap='viridis', s=5)\n",
    "    plt.colorbar(label='Temperature')\n",
    "    plt.title('UMAP Embeddings Colored by Temperature')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=top_ps, cmap='plasma', s=5)\n",
    "    plt.colorbar(label='Top_p')\n",
    "    plt.title('UMAP Embeddings Colored by Top_p')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return umap_embeddings\n",
    "\n",
    "def kmeans_clustering(encoded_embeddings, n_clusters=5):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(encoded_embeddings)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(scaled_embeddings)\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.scatter(scaled_embeddings[:, 0], scaled_embeddings[:, 1], c=clusters, cmap='tab10', s=5)\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.title('Raw Autoencoder Embeddings Colored by KMeans Clusters')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def visualize_clusters_by_model(embeddings, labels, configs, title):\n",
    "    model_colors = {\n",
    "        'gpt-3.5-turbo': 'r',\n",
    "        'gpt-4o': 'g',\n",
    "        'llama37B': 'b'\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, (embedding, config) in enumerate(zip(embeddings, configs)):\n",
    "        model_name = config.get('model', 'llama37B')\n",
    "        color = model_colors.get(model_name, 'b')\n",
    "        plt.scatter(embedding[0], embedding[1], color=color, label=model_name if i == 0 else \"\")\n",
    "\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.title(title)\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=model) \n",
    "               for model, color in model_colors.items()]\n",
    "    plt.legend(handles=handles)\n",
    "    plt.show()\n",
    "\n",
    "def load_results(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    results['embeddings'] = np.array(results['embeddings'])\n",
    "    results['labels'] = np.array(results['labels'])\n",
    "    return results\n",
    "\n",
    "def save_results(results, filename):\n",
    "    results['embeddings'] = results['embeddings'].tolist()\n",
    "    results['labels'] = results['labels'].tolist()\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "def remove_clusters(data, labels, configs, clusters_to_remove):\n",
    "    filtered_data = [item for i, item in enumerate(data) if labels[i] not in clusters_to_remove]\n",
    "    filtered_embeddings = [embedding for i, embedding in enumerate(results['embeddings']) if labels[i] not in clusters_to_remove]\n",
    "    filtered_labels = [label for label in labels if label not in clusters_to_remove]\n",
    "    filtered_configs = [configs[i] for i, label in enumerate(labels) if label not in clusters_to_remove]\n",
    "    return filtered_data, np.array(filtered_embeddings), filtered_labels, filtered_configs\n",
    "\n",
    "def recluster_data(embeddings, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    new_labels = kmeans.fit_predict(embeddings)\n",
    "    return kmeans, new_labels\n",
    "\n",
    "def calculate_central_configs(configs, labels, n_clusters):\n",
    "    config_df = pd.DataFrame(configs)\n",
    "    config_df = config_df.drop(columns=['batch_size', 'max_tokens', 'max_seq_len'], errors='ignore')\n",
    "    numeric_columns = config_df.select_dtypes(include=[np.number]).columns\n",
    "    central_configs = config_df.groupby(labels)[numeric_columns].mean().to_dict(orient='records')\n",
    "    return central_configs\n",
    "\n",
    "def visualize_and_analyze(new_labels, embeddings):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for cluster in range(max(new_labels)+1):\n",
    "        points = reduced_embeddings[new_labels == cluster]\n",
    "        plt.scatter(points[:, 0], points[:, 1], label=f'Cluster {cluster}')\n",
    "    plt.legend()\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title('Re-clustered Visualization')\n",
    "    plt.show()\n",
    "\n",
    "def display_cluster_details(data, labels):\n",
    "    for i in range(max(labels)+1):\n",
    "        cluster_items = [item['response_content'] for j, item in enumerate(data) if labels[j] == i]\n",
    "        avg_length = np.mean([len(item) for item in cluster_items])\n",
    "        random_sample = random.sample(cluster_items, min(5, len(cluster_items)))\n",
    "        print(f'Cluster {i}:')\n",
    "        print('Random Sample of Responses:', random_sample)\n",
    "        print('Total Responses:', len(cluster_items))\n",
    "        print('Average Length of Responses:', avg_length)\n",
    "        print()\n",
    "\n",
    "def main(folder_path):\n",
    "    model_path = 'bert_autoencoder_model.pth'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = load_model(model_path, device)\n",
    "\n",
    "    embeddings, temps, top_ps, configs, data = load_embedding_data(folder_path)\n",
    "\n",
    "    # Process all embeddings to get encoded representations\n",
    "    encoded_embeddings = []\n",
    "    for embedding in embeddings:\n",
    "        encoded = process_embedding(model, embedding, device)\n",
    "        encoded_embeddings.append(encoded.flatten())  # Flatten to 2D array for UMAP\n",
    "\n",
    "    encoded_embeddings = np.array(encoded_embeddings)\n",
    "\n",
    "    # Apply UMAP and visualize\n",
    "    umap_embeds = umap_embeddings(encoded_embeddings, temps, top_ps)\n",
    "\n",
    "    # Apply KMeans and visualize\n",
    "    kmeans_clusters = kmeans_clustering(umap_embeds, n_clusters=15)\n",
    "\n",
    "    # Visualize clusters by model type using UMAP embeddings\n",
    "    visualize_clusters_by_model(umap_embeds, kmeans_clusters, configs, 'UMAP Embeddings Colored by Model')\n",
    "    # Save clustering results\n",
    "    results = {\n",
    "        'embeddings': encoded_embeddings,\n",
    "        'labels': kmeans_clusters,\n",
    "        'configs': configs\n",
    "    }\n",
    "    save_results(results, 'cluster_results.json')\n",
    "\n",
    "    # Display cluster details\n",
    "    display_cluster_details(data, kmeans_clusters)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = '/workspace/combined'  # Replace with the path to your folder containing JSON files\n",
    "    main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f5b06-d2c4-4378-91b9-9e778a7e6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Custom Dataset\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.data = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".json\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    if isinstance(json_data, dict):  # To handle different structures\n",
    "                        json_data = [json_data]\n",
    "                    for item in json_data:\n",
    "                        embedding = item['embedding']\n",
    "                        temp = float(item['configuration']['temperature'])\n",
    "                        top_p = float(item['configuration']['top_p'])\n",
    "                        self.data.append((embedding, temp, top_p))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding, temp, top_p = self.data[idx]\n",
    "        return torch.tensor(embedding, dtype=torch.float32), temp, top_p\n",
    "\n",
    "# Define the Autoencoder model using a pretrained BERT model as the encoder\n",
    "class BertAutoencoder(nn.Module):\n",
    "    def __init__(self, bert_model_name, embedding_dim=3072, hidden_dim=768, lstm_units=256, sequence_length=4):\n",
    "        super(BertAutoencoder, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embedding_adapter = nn.Linear(embedding_dim, hidden_dim * sequence_length)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.decoder = nn.LSTM(hidden_dim, lstm_units, batch_first=True)\n",
    "        self.output_layer = nn.Linear(lstm_units, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_adapter(x)\n",
    "        x = x.view(x.size(0), self.sequence_length, -1)  # Reshape to (batch_size, sequence_length, hidden_dim)\n",
    "        encoder_outputs = self.bert(inputs_embeds=x).last_hidden_state\n",
    "        decoder_outputs, _ = self.decoder(encoder_outputs)\n",
    "        output = self.output_layer(decoder_outputs)\n",
    "        return output\n",
    "\n",
    "# Calculate accuracy as a similarity measure\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    cos = nn.CosineSimilarity(dim=-1)\n",
    "    accuracy = cos(outputs, targets).mean().item()\n",
    "    return accuracy * 100  # Convert to percentage\n",
    "\n",
    "# Progressive data increment method\n",
    "def progressive_training(model, dataset, initial_size, increment_ratio, max_epochs, device):\n",
    "    total_size = len(dataset)\n",
    "    test_size = 0.1\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    metrics = []\n",
    "\n",
    "    # Separate the test data and ensure it's shuffled\n",
    "    train_size = int((1 - test_size) * total_size)\n",
    "    test_size = total_size - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    current_size = initial_size\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        if current_size > train_size:\n",
    "            break\n",
    "\n",
    "        current_train_indices = torch.randperm(train_size)[:current_size]\n",
    "        current_train_dataset = torch.utils.data.Subset(train_dataset, current_train_indices)\n",
    "        train_loader = DataLoader(current_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        for inputs, _, _ in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += calculate_accuracy(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "\n",
    "        train_accuracy /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, _, _ in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += calculate_accuracy(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "\n",
    "        val_accuracy /= len(test_loader)\n",
    "\n",
    "        metrics.append((current_size, train_loss/len(train_loader), val_loss/len(test_loader), train_accuracy, val_accuracy))\n",
    "        current_size += int(current_size * increment_ratio)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{max_epochs} | Data size: {current_size} | Train Loss: {train_loss/len(train_loader):.8f} | Val Loss: {val_loss/len(test_loader):.8f} | Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Plotting function\n",
    "def plot_metrics(metrics):\n",
    "    sizes, train_losses, val_losses, train_accuracies, val_accuracies = zip(*metrics)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(sizes, train_losses, label='Training Loss')\n",
    "    plt.plot(sizes, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss vs Data Size')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(sizes, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(sizes, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy vs Data Size')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot temp/top_p vs accuracy\n",
    "def plot_temp_top_p_vs_accuracy(dataset, model, device):\n",
    "    temps, top_ps, accuracies = [], [], []\n",
    "\n",
    "    test_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, temp, top_p in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            accuracy = calculate_accuracy(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "\n",
    "            temps.append(temp.item())\n",
    "            top_ps.append(top_p.item())\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(temps, accuracies, alpha=0.5)\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Temperature')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(top_ps, accuracies, alpha=0.5)\n",
    "    plt.xlabel('Top_p')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Top_p')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "def main(folder_path):\n",
    "    dataset = EmbeddingDataset(folder_path)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BertAutoencoder('bert-base-uncased', lstm_units=256, hidden_dim=768, sequence_length=4).to(device)\n",
    "\n",
    "    initial_size = 10\n",
    "    increment_ratio = 0.1\n",
    "    max_epochs = 5000\n",
    "\n",
    "    metrics = progressive_training(model, dataset, initial_size, increment_ratio, max_epochs, device)\n",
    "\n",
    "    # Save the metrics to a file\n",
    "    with open('training_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'bert_autoencoder_model.pth')\n",
    "\n",
    "    plot_metrics(metrics)\n",
    "    plot_temp_top_p_vs_accuracy(dataset, model, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = '/workspace/combined'  # Replace with the path to your folder containing JSON files\n",
    "    main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf3faf-44cb-46ac-bb92-6e3fd94a951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from JSON files in the specified folder\n",
    "def load_data(folder_path):\n",
    "    texts = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                if isinstance(data, dict) and 'response_content' in data:\n",
    "                    texts.append(data['response_content'])\n",
    "                elif isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        if 'response_content' in item:\n",
    "                            texts.append(item['response_content'])\n",
    "    return texts\n",
    "\n",
    "# Preprocess text data using a tokenizer\n",
    "def preprocess_texts(texts, tokenizer, max_length=512):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    return encodings.input_ids, encodings.attention_mask\n",
    "\n",
    "# Define the Autoencoder model using a pretrained BERT model as the encoder\n",
    "class BertAutoencoder(nn.Module):\n",
    "    def __init__(self, bert_model_name, lstm_units=256, max_length=512):\n",
    "        super(BertAutoencoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.encoder = self.bert.encoder\n",
    "        self.decoder = nn.LSTM(self.bert.config.hidden_size, lstm_units, batch_first=True)\n",
    "        self.output_layer = nn.Linear(lstm_units, self.bert.config.vocab_size)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        encoder_outputs = bert_outputs.last_hidden_state\n",
    "        decoder_outputs, _ = self.decoder(encoder_outputs)\n",
    "        output = self.output_layer(decoder_outputs)\n",
    "        return output\n",
    "\n",
    "# Progressive data increment method\n",
    "def progressive_training(model, data, attention_masks, tokenizer, initial_size, increment_ratio, max_epochs, device):\n",
    "    current_size = initial_size\n",
    "    total_size = len(data)\n",
    "    test_size = 0.1\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    metrics = []\n",
    "    \n",
    "    # Split data into training and test sets\n",
    "    train_indices, test_indices = train_test_split(np.arange(total_size), test_size=test_size, shuffle=True)\n",
    "    \n",
    "    x_test, mask_test = data[test_indices], attention_masks[test_indices]\n",
    "    x_train_full, mask_train_full = data[train_indices], attention_masks[train_indices]\n",
    "    \n",
    "    test_dataset = TensorDataset(x_test, x_test, mask_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        if current_size > len(train_indices):\n",
    "            break\n",
    "            \n",
    "        current_train_indices = train_indices[:int(current_size)]\n",
    "        current_data = data[current_train_indices]\n",
    "        current_masks = attention_masks[current_train_indices]\n",
    "        \n",
    "        x_train, y_train, mask_train = current_data, current_data, current_masks\n",
    "        \n",
    "        train_dataset = TensorDataset(x_train, y_train, mask_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for inputs, targets, masks in train_loader:\n",
    "            inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, masks)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(2)\n",
    "            total += targets.size(0) * targets.size(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        train_accuracy = 100. * correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, masks in test_loader:\n",
    "                inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "                outputs = model(inputs, masks)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(2)\n",
    "                total += targets.size(0) * targets.size(1)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        val_accuracy = 100. * correct / total\n",
    "        metrics.append((current_size, train_loss/len(train_loader), val_loss/len(test_loader), train_accuracy, val_accuracy))\n",
    "        current_size += current_size * increment_ratio\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{max_epochs} | Data size: {current_size} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(test_loader):.4f} | Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Plotting function\n",
    "def plot_metrics(metrics):\n",
    "    sizes, train_losses, val_losses, train_accuracies, val_accuracies = zip(*metrics)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(sizes, train_losses, label='Training Loss')\n",
    "    plt.plot(sizes, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss vs Data Size')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(sizes, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(sizes, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy vs Data Size')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_test_data_and_accuracy(test_data, test_masks, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(TensorDataset(test_data, test_masks), batch_size=1, shuffle=False)\n",
    "\n",
    "    original_texts = tokenizer.batch_decode(test_data, skip_special_tokens=True)\n",
    "    predicted_texts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in test_loader:\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "            outputs = model(inputs, masks)\n",
    "            _, predicted = outputs.max(2)\n",
    "            predicted_texts.extend(tokenizer.batch_decode(predicted.cpu(), skip_special_tokens=True))\n",
    "\n",
    "    for original, predicted in zip(original_texts, predicted_texts):\n",
    "        print(f'Original: {original}')\n",
    "        print(f'Predicted: {predicted}')\n",
    "        print('---')\n",
    "\n",
    "# Main function\n",
    "def main(folder_path):\n",
    "    texts = load_data(folder_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "    input_ids, attention_masks = preprocess_texts(texts, tokenizer)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BertAutoencoder('bert-base-uncased', lstm_units=256, max_length=512).to(device)\n",
    "\n",
    "    initial_size = 100\n",
    "    increment_ratio = 0.01\n",
    "    max_epochs = 50000\n",
    "\n",
    "    metrics = progressive_training(model, input_ids, attention_masks, tokenizer, initial_size, increment_ratio, max_epochs, device)\n",
    "\n",
    "    # Save the metrics to a file\n",
    "    with open('training_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    plot_metrics(metrics)\n",
    "\n",
    "    x_train, x_test, mask_train, mask_test = train_test_split(input_ids, attention_masks, test_size=0.1)\n",
    "    plot_test_data_and_accuracy(x_test[:5], mask_test[:5], model, tokenizer, device)  # Display 5 samples\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = '/workspace/slice-monorepo/thebeast/combined'  # Replace with the path to your folder containing JSON files\n",
    "    main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271db30-e857-421d-aa45-7b81876f8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from JSON files in the specified folder\n",
    "def load_data(folder_path):\n",
    "    texts = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                if isinstance(data, dict) and 'response_content' in data:\n",
    "                    texts.append(data['response_content'])\n",
    "                elif isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        if 'response_content' in item:\n",
    "                            texts.append(item['response_content'])\n",
    "    return texts\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "def preprocess_texts(texts, tokenizer, max_length=512):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    return padded_sequences\n",
    "\n",
    "class TextTokenizer:\n",
    "    def __init__(self, oov_token=\"<OOV>\"):\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {oov_token: 1}\n",
    "        self.index_word = {1: oov_token}\n",
    "        self.word_counts = {}\n",
    "        self.num_words = 2\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                if word not in self.word_counts:\n",
    "                    self.word_counts[word] = 1\n",
    "                else:\n",
    "                    self.word_counts[word] += 1\n",
    "\n",
    "        sorted_words = sorted(self.word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        for word, _ in sorted_words:\n",
    "            self.word_index[word] = self.num_words\n",
    "            self.index_word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequences.append([self.word_index.get(word, self.word_index[self.oov_token]) for word in text.split()])\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for seq in sequences:\n",
    "            texts.append(' '.join([self.index_word.get(idx, self.oov_token) for idx in seq]))\n",
    "        return texts\n",
    "\n",
    "# Padding function\n",
    "def pad_sequences(sequences, maxlen, padding='post', truncating='post'):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'pre':\n",
    "                seq = seq[-maxlen:]\n",
    "            elif truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "        if padding == 'pre':\n",
    "            padded_sequences[i, -len(seq):] = seq\n",
    "        elif padding == 'post':\n",
    "            padded_sequences[i, :len(seq)] = seq\n",
    "    return padded_sequences\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class TextAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, lstm_units=256, max_length=512):\n",
    "        super(TextAutoencoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(embedding_dim, lstm_units, batch_first=True)\n",
    "        self.decoder = nn.LSTM(lstm_units, embedding_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.encoder(embedded)\n",
    "        repeated_hidden = hidden.repeat(self.max_length, 1, 1).permute(1, 0, 2)\n",
    "        decoded, _ = self.decoder(repeated_hidden)\n",
    "        output = self.output_layer(decoded)\n",
    "        return output\n",
    "\n",
    "# Progressive data increment method\n",
    "def progressive_training(model, data, tokenizer, vocab_size, initial_size, increment_ratio, max_epochs, device):\n",
    "    current_size = initial_size\n",
    "    total_size = len(data)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    metrics = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        if current_size > total_size:\n",
    "            break\n",
    "        current_data = data[:current_size]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(current_data, current_data, test_size=0.1)\n",
    "\n",
    "        x_train = torch.tensor(x_train, dtype=torch.long).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        x_test = torch.tensor(x_test, dtype=torch.long).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "        train_dataset = TensorDataset(x_train, y_train)\n",
    "        test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(2)\n",
    "            total += targets.size(0) * targets.size(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        train_accuracy = 100. * correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(2)\n",
    "                total += targets.size(0) * targets.size(1)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        val_accuracy = 100. * correct / total\n",
    "        metrics.append((current_size, train_accuracy, val_accuracy))\n",
    "        current_size += int(current_size * increment_ratio)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{max_epochs} completed with {current_size} samples. Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Main function\n",
    "def main(folder_path):\n",
    "    texts = load_data(folder_path)\n",
    "    tokenizer = TextTokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = preprocess_texts(texts, tokenizer)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TextAutoencoder(vocab_size, embedding_dim=128, lstm_units=256, max_length=512).to(device)\n",
    "\n",
    "    initial_size = 100\n",
    "    increment_ratio = 0.01\n",
    "    max_epochs = 5000\n",
    "\n",
    "    metrics = progressive_training(model, sequences, tokenizer, vocab_size, initial_size, increment_ratio, max_epochs, device)\n",
    "\n",
    "    # Save the metrics to a file\n",
    "    with open('training_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = '/workspace/slice-monorepo/thebeast/data/combined'  # Replace with the path to your folder containing JSON files\n",
    "    main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a67a1-70cc-4d79-b97d-54b0d331c1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de3551-865d-4ef4-b981-0507b0162e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

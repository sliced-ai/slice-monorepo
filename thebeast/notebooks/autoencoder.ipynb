{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ea558-6f2a-4813-b044-8fad7f207973",
   "metadata": {},
   "outputs": [],
   "source": [
    "/workspace/slice-monorepo/thebeast/combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987aa8c-f177-46ac-a904-5f453a64a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn tensorflow transformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f5b06-d2c4-4378-91b9-9e778a7e6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Custom Dataset\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.data = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".json\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    if isinstance(json_data, dict):  # To handle different structures\n",
    "                        json_data = [json_data]\n",
    "                    for item in json_data:\n",
    "                        embedding = item['embedding']\n",
    "                        temp = float(item['configuration']['temperature'])\n",
    "                        top_p = float(item['configuration']['top_p'])\n",
    "                        self.data.append((embedding, temp, top_p))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding, temp, top_p = self.data[idx]\n",
    "        return torch.tensor(embedding, dtype=torch.float32), temp, top_p\n",
    "\n",
    "# Define the Autoencoder model using a pretrained BERT model as the encoder\n",
    "class BertAutoencoder(nn.Module):\n",
    "    def __init__(self, bert_model_name, embedding_dim=3072, hidden_dim=768, lstm_units=256, sequence_length=4):\n",
    "        super(BertAutoencoder, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embedding_adapter = nn.Linear(embedding_dim, hidden_dim * sequence_length)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.decoder = nn.LSTM(hidden_dim, lstm_units, batch_first=True)\n",
    "        self.output_layer = nn.Linear(lstm_units, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_adapter(x)\n",
    "        x = x.view(x.size(0), self.sequence_length, -1)  # Reshape to (batch_size, sequence_length, hidden_dim)\n",
    "        encoder_outputs = self.bert(inputs_embeds=x).last_hidden_state\n",
    "        decoder_outputs, _ = self.decoder(encoder_outputs)\n",
    "        output = self.output_layer(decoder_outputs)\n",
    "        return output\n",
    "\n",
    "# Calculate accuracy as a similarity measure\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    cos = nn.CosineSimilarity(dim=-1)\n",
    "    accuracy = cos(outputs, targets).mean().item()\n",
    "    return accuracy * 100  # Convert to percentage\n",
    "\n",
    "# Progressive data increment method\n",
    "def progressive_training(model, dataset, initial_size, increment_ratio, max_epochs, device):\n",
    "    total_size = len(dataset)\n",
    "    test_size = 0.1\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    metrics = []\n",
    "\n",
    "    # Separate the test data and ensure it's shuffled\n",
    "    train_size = int((1 - test_size) * total_size)\n",
    "    test_size = total_size - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    current_size = initial_size\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        if current_size > train_size:\n",
    "            break\n",
    "\n",
    "        current_train_indices = torch.randperm(train_size)[:current_size]\n",
    "        current_train_dataset = torch.utils.data.Subset(train_dataset, current_train_indices)\n",
    "        train_loader = DataLoader(current_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        for inputs, _, _ in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += calculate_accuracy(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "\n",
    "        train_accuracy /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, _, _ in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += calculate_accuracy(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "\n",
    "        val_accuracy /= len(test_loader)\n",
    "\n",
    "        metrics.append((current_size, train_loss/len(train_loader), val_loss/len(test_loader), train_accuracy, val_accuracy))\n",
    "        current_size += int(current_size * increment_ratio)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{max_epochs} | Data size: {current_size} | Train Loss: {train_loss/len(train_loader):.8f} | Val Loss: {val_loss/len(test_loader):.8f} | Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Plotting function\n",
    "def plot_metrics(metrics):\n",
    "    sizes, train_losses, val_losses, train_accuracies, val_accuracies = zip(*metrics)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(sizes, train_losses, label='Training Loss')\n",
    "    plt.plot(sizes, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss vs Data Size')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(sizes, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(sizes, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy vs Data Size')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot temp/top_p vs accuracy\n",
    "def plot_temp_top_p_vs_accuracy(dataset, model, device):\n",
    "    temps, top_ps, accuracies = [], [], []\n",
    "\n",
    "    test_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, temp, top_p in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            accuracy = calculate_accuracy(outputs, inputs.view(outputs.size(0), model.sequence_length, -1))\n",
    "\n",
    "            temps.append(temp.item())\n",
    "            top_ps.append(top_p.item())\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(temps, accuracies, alpha=0.5)\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Temperature')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(top_ps, accuracies, alpha=0.5)\n",
    "    plt.xlabel('Top_p')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Top_p')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "def main(folder_path):\n",
    "    dataset = EmbeddingDataset(folder_path)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BertAutoencoder('bert-base-uncased', lstm_units=256, hidden_dim=768, sequence_length=4).to(device)\n",
    "\n",
    "    initial_size = 10\n",
    "    increment_ratio = 0.1\n",
    "    max_epochs = 5000\n",
    "\n",
    "    metrics = progressive_training(model, dataset, initial_size, increment_ratio, max_epochs, device)\n",
    "\n",
    "    # Save the metrics to a file\n",
    "    with open('training_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'bert_autoencoder_model.pth')\n",
    "\n",
    "    plot_metrics(metrics)\n",
    "    plot_temp_top_p_vs_accuracy(dataset, model, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = '/workspace/combined'  # Replace with the path to your folder containing JSON files\n",
    "    main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf3faf-44cb-46ac-bb92-6e3fd94a951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50000 | Data size: 101.0 | Train Loss: 10.1777 | Val Loss: 9.9846 | Train Acc: 3.78% | Val Acc: 8.01%\n",
      "Epoch 2/50000 | Data size: 102.01 | Train Loss: 9.9211 | Val Loss: 9.7296 | Train Acc: 8.51% | Val Acc: 9.62%\n",
      "Epoch 3/50000 | Data size: 103.0301 | Train Loss: 9.6456 | Val Loss: 9.4690 | Train Acc: 9.70% | Val Acc: 10.02%\n",
      "Epoch 4/50000 | Data size: 104.060401 | Train Loss: 9.3883 | Val Loss: 9.1993 | Train Acc: 10.68% | Val Acc: 12.35%\n",
      "Epoch 5/50000 | Data size: 105.10100501 | Train Loss: 9.1350 | Val Loss: 8.9235 | Train Acc: 12.72% | Val Acc: 15.40%\n",
      "Epoch 6/50000 | Data size: 106.1520150601 | Train Loss: 8.8413 | Val Loss: 8.6491 | Train Acc: 15.31% | Val Acc: 16.60%\n",
      "Epoch 7/50000 | Data size: 107.213535210701 | Train Loss: 8.5664 | Val Loss: 8.3753 | Train Acc: 16.69% | Val Acc: 17.15%\n",
      "Epoch 8/50000 | Data size: 108.28567056280801 | Train Loss: 8.2726 | Val Loss: 8.1140 | Train Acc: 17.17% | Val Acc: 17.76%\n",
      "Epoch 9/50000 | Data size: 109.36852726843608 | Train Loss: 8.0057 | Val Loss: 7.8586 | Train Acc: 17.86% | Val Acc: 18.49%\n",
      "Epoch 10/50000 | Data size: 110.46221254112044 | Train Loss: 7.7397 | Val Loss: 7.6135 | Train Acc: 18.60% | Val Acc: 19.66%\n",
      "Epoch 11/50000 | Data size: 111.56683466653165 | Train Loss: 7.4632 | Val Loss: 7.3795 | Train Acc: 19.23% | Val Acc: 20.55%\n",
      "Epoch 12/50000 | Data size: 112.68250301319696 | Train Loss: 7.2789 | Val Loss: 7.1561 | Train Acc: 19.75% | Val Acc: 22.06%\n",
      "Epoch 13/50000 | Data size: 113.80932804332893 | Train Loss: 7.0328 | Val Loss: 6.9436 | Train Acc: 20.69% | Val Acc: 23.03%\n",
      "Epoch 14/50000 | Data size: 114.94742132376223 | Train Loss: 6.8245 | Val Loss: 6.7415 | Train Acc: 21.71% | Val Acc: 24.11%\n",
      "Epoch 15/50000 | Data size: 116.09689553699985 | Train Loss: 6.6150 | Val Loss: 6.5501 | Train Acc: 22.88% | Val Acc: 24.94%\n",
      "Epoch 16/50000 | Data size: 117.25786449236985 | Train Loss: 6.4268 | Val Loss: 6.3670 | Train Acc: 24.09% | Val Acc: 25.53%\n",
      "Epoch 17/50000 | Data size: 118.43044313729355 | Train Loss: 6.2086 | Val Loss: 6.1947 | Train Acc: 24.95% | Val Acc: 26.41%\n",
      "Epoch 18/50000 | Data size: 119.61474756866649 | Train Loss: 6.0554 | Val Loss: 6.0296 | Train Acc: 25.76% | Val Acc: 27.52%\n",
      "Epoch 19/50000 | Data size: 120.81089504435315 | Train Loss: 5.8740 | Val Loss: 5.8719 | Train Acc: 26.59% | Val Acc: 28.06%\n",
      "Epoch 20/50000 | Data size: 122.01900399479668 | Train Loss: 5.7077 | Val Loss: 5.7216 | Train Acc: 27.35% | Val Acc: 28.60%\n",
      "Epoch 21/50000 | Data size: 123.23919403474464 | Train Loss: 5.5635 | Val Loss: 5.5778 | Train Acc: 28.16% | Val Acc: 29.82%\n",
      "Epoch 22/50000 | Data size: 124.47158597509208 | Train Loss: 5.4041 | Val Loss: 5.4405 | Train Acc: 29.21% | Val Acc: 31.00%\n",
      "Epoch 23/50000 | Data size: 125.71630183484301 | Train Loss: 5.2695 | Val Loss: 5.3086 | Train Acc: 30.18% | Val Acc: 31.62%\n",
      "Epoch 24/50000 | Data size: 126.97346485319144 | Train Loss: 5.1395 | Val Loss: 5.1822 | Train Acc: 31.05% | Val Acc: 32.61%\n",
      "Epoch 25/50000 | Data size: 128.24319950172335 | Train Loss: 5.0069 | Val Loss: 5.0606 | Train Acc: 32.20% | Val Acc: 33.63%\n",
      "Epoch 26/50000 | Data size: 129.52563149674057 | Train Loss: 4.8703 | Val Loss: 4.9435 | Train Acc: 33.12% | Val Acc: 35.03%\n",
      "Epoch 27/50000 | Data size: 130.820887811708 | Train Loss: 4.7003 | Val Loss: 4.8055 | Train Acc: 34.46% | Val Acc: 35.73%\n",
      "Epoch 28/50000 | Data size: 132.12909668982508 | Train Loss: 4.5422 | Val Loss: 4.6771 | Train Acc: 35.24% | Val Acc: 35.90%\n",
      "Epoch 29/50000 | Data size: 133.45038765672334 | Train Loss: 4.4629 | Val Loss: 4.5496 | Train Acc: 36.06% | Val Acc: 36.99%\n",
      "Epoch 30/50000 | Data size: 134.78489153329056 | Train Loss: 4.2891 | Val Loss: 4.4263 | Train Acc: 36.88% | Val Acc: 37.58%\n",
      "Epoch 31/50000 | Data size: 136.13274044862348 | Train Loss: 4.2339 | Val Loss: 4.3100 | Train Acc: 37.44% | Val Acc: 38.41%\n",
      "Epoch 32/50000 | Data size: 137.49406785310973 | Train Loss: 4.0738 | Val Loss: 4.1992 | Train Acc: 38.39% | Val Acc: 39.23%\n",
      "Epoch 33/50000 | Data size: 138.86900853164082 | Train Loss: 3.9671 | Val Loss: 4.0934 | Train Acc: 39.00% | Val Acc: 39.69%\n",
      "Epoch 34/50000 | Data size: 140.2576986169572 | Train Loss: 3.8057 | Val Loss: 3.9902 | Train Acc: 39.50% | Val Acc: 40.20%\n",
      "Epoch 35/50000 | Data size: 141.6602756031268 | Train Loss: 3.7498 | Val Loss: 3.8940 | Train Acc: 40.26% | Val Acc: 40.65%\n",
      "Epoch 36/50000 | Data size: 143.07687835915806 | Train Loss: 3.6308 | Val Loss: 3.7972 | Train Acc: 40.82% | Val Acc: 41.55%\n",
      "Epoch 37/50000 | Data size: 144.50764714274965 | Train Loss: 3.6006 | Val Loss: 3.7084 | Train Acc: 41.74% | Val Acc: 41.95%\n",
      "Epoch 38/50000 | Data size: 145.95272361417713 | Train Loss: 3.4991 | Val Loss: 3.6174 | Train Acc: 42.20% | Val Acc: 42.89%\n",
      "Epoch 39/50000 | Data size: 147.4122508503189 | Train Loss: 3.3393 | Val Loss: 3.5346 | Train Acc: 42.82% | Val Acc: 43.13%\n",
      "Epoch 40/50000 | Data size: 148.8863733588221 | Train Loss: 3.2808 | Val Loss: 3.4571 | Train Acc: 43.16% | Val Acc: 43.62%\n",
      "Epoch 41/50000 | Data size: 150.37523709241034 | Train Loss: 3.2073 | Val Loss: 3.3729 | Train Acc: 43.67% | Val Acc: 44.78%\n",
      "Epoch 42/50000 | Data size: 151.87898946333445 | Train Loss: 3.1050 | Val Loss: 3.3004 | Train Acc: 44.17% | Val Acc: 45.27%\n",
      "Epoch 43/50000 | Data size: 153.3977793579678 | Train Loss: 3.0085 | Val Loss: 3.2282 | Train Acc: 44.68% | Val Acc: 45.63%\n",
      "Epoch 44/50000 | Data size: 154.9317571515475 | Train Loss: 2.9723 | Val Loss: 3.1553 | Train Acc: 45.36% | Val Acc: 46.75%\n",
      "Epoch 45/50000 | Data size: 156.48107472306296 | Train Loss: 2.8918 | Val Loss: 3.0862 | Train Acc: 46.17% | Val Acc: 47.31%\n",
      "Epoch 46/50000 | Data size: 158.0458854702936 | Train Loss: 2.9021 | Val Loss: 3.0243 | Train Acc: 46.48% | Val Acc: 47.23%\n",
      "Epoch 47/50000 | Data size: 159.62634432499652 | Train Loss: 2.8515 | Val Loss: 2.9587 | Train Acc: 46.86% | Val Acc: 47.99%\n",
      "Epoch 48/50000 | Data size: 161.2226077682465 | Train Loss: 2.7779 | Val Loss: 2.8913 | Train Acc: 47.43% | Val Acc: 48.69%\n",
      "Epoch 49/50000 | Data size: 162.83483384592896 | Train Loss: 2.7219 | Val Loss: 2.8204 | Train Acc: 48.12% | Val Acc: 49.22%\n",
      "Epoch 50/50000 | Data size: 164.46318218438824 | Train Loss: 2.5396 | Val Loss: 2.7557 | Train Acc: 48.61% | Val Acc: 49.59%\n",
      "Epoch 51/50000 | Data size: 166.10781400623213 | Train Loss: 2.5897 | Val Loss: 2.6888 | Train Acc: 49.33% | Val Acc: 50.24%\n",
      "Epoch 52/50000 | Data size: 167.76889214629446 | Train Loss: 2.5566 | Val Loss: 2.6247 | Train Acc: 49.96% | Val Acc: 50.83%\n",
      "Epoch 53/50000 | Data size: 169.4465810677574 | Train Loss: 2.3690 | Val Loss: 2.5637 | Train Acc: 50.37% | Val Acc: 51.27%\n",
      "Epoch 54/50000 | Data size: 171.14104687843496 | Train Loss: 2.3749 | Val Loss: 2.5055 | Train Acc: 50.92% | Val Acc: 51.57%\n",
      "Epoch 55/50000 | Data size: 172.8524573472193 | Train Loss: 2.2885 | Val Loss: 2.4542 | Train Acc: 51.33% | Val Acc: 51.95%\n",
      "Epoch 56/50000 | Data size: 174.5809819206915 | Train Loss: 2.2662 | Val Loss: 2.3903 | Train Acc: 51.92% | Val Acc: 53.10%\n",
      "Epoch 57/50000 | Data size: 176.32679173989843 | Train Loss: 2.2505 | Val Loss: 2.3402 | Train Acc: 52.46% | Val Acc: 53.02%\n",
      "Epoch 58/50000 | Data size: 178.09005965729742 | Train Loss: 2.1727 | Val Loss: 2.2844 | Train Acc: 52.74% | Val Acc: 53.78%\n",
      "Epoch 59/50000 | Data size: 179.87096025387038 | Train Loss: 2.1136 | Val Loss: 2.2405 | Train Acc: 53.57% | Val Acc: 53.72%\n",
      "Epoch 60/50000 | Data size: 181.6696698564091 | Train Loss: 2.0362 | Val Loss: 2.1874 | Train Acc: 54.07% | Val Acc: 54.46%\n",
      "Epoch 61/50000 | Data size: 183.4863665549732 | Train Loss: 1.9864 | Val Loss: 2.1395 | Train Acc: 54.61% | Val Acc: 54.93%\n",
      "Epoch 62/50000 | Data size: 185.32123022052292 | Train Loss: 1.9400 | Val Loss: 2.1007 | Train Acc: 54.64% | Val Acc: 54.84%\n",
      "Epoch 63/50000 | Data size: 187.17444252272816 | Train Loss: 1.9054 | Val Loss: 2.0507 | Train Acc: 54.85% | Val Acc: 56.05%\n",
      "Epoch 64/50000 | Data size: 189.04618694795545 | Train Loss: 1.8560 | Val Loss: 2.0092 | Train Acc: 55.65% | Val Acc: 55.98%\n",
      "Epoch 65/50000 | Data size: 190.936648817435 | Train Loss: 1.8108 | Val Loss: 1.9687 | Train Acc: 56.11% | Val Acc: 56.23%\n",
      "Epoch 66/50000 | Data size: 192.84601530560937 | Train Loss: 1.7680 | Val Loss: 1.9263 | Train Acc: 56.41% | Val Acc: 56.90%\n",
      "Epoch 67/50000 | Data size: 194.77447545866548 | Train Loss: 1.7286 | Val Loss: 1.8997 | Train Acc: 56.99% | Val Acc: 56.65%\n",
      "Epoch 68/50000 | Data size: 196.72222021325214 | Train Loss: 1.6215 | Val Loss: 1.8458 | Train Acc: 57.07% | Val Acc: 57.14%\n",
      "Epoch 69/50000 | Data size: 198.68944241538466 | Train Loss: 1.6102 | Val Loss: 1.8027 | Train Acc: 57.71% | Val Acc: 57.39%\n",
      "Epoch 70/50000 | Data size: 200.67633683953852 | Train Loss: 1.5306 | Val Loss: 1.7606 | Train Acc: 57.78% | Val Acc: 57.82%\n",
      "Epoch 71/50000 | Data size: 202.6831002079339 | Train Loss: 1.5310 | Val Loss: 1.7211 | Train Acc: 58.17% | Val Acc: 57.96%\n",
      "Epoch 72/50000 | Data size: 204.70993121001325 | Train Loss: 1.4830 | Val Loss: 1.6817 | Train Acc: 58.22% | Val Acc: 58.14%\n",
      "Epoch 73/50000 | Data size: 206.75703052211338 | Train Loss: 1.4895 | Val Loss: 1.6423 | Train Acc: 58.54% | Val Acc: 58.55%\n",
      "Epoch 74/50000 | Data size: 208.8246008273345 | Train Loss: 1.4237 | Val Loss: 1.6058 | Train Acc: 58.84% | Val Acc: 58.77%\n",
      "Epoch 75/50000 | Data size: 210.91284683560787 | Train Loss: 1.3965 | Val Loss: 1.5705 | Train Acc: 59.10% | Val Acc: 58.93%\n",
      "Epoch 76/50000 | Data size: 213.02197530396396 | Train Loss: 1.3814 | Val Loss: 1.5359 | Train Acc: 59.24% | Val Acc: 59.16%\n",
      "Epoch 77/50000 | Data size: 215.15219505700358 | Train Loss: 1.3186 | Val Loss: 1.5024 | Train Acc: 59.18% | Val Acc: 59.38%\n",
      "Epoch 78/50000 | Data size: 217.30371700757362 | Train Loss: 1.2706 | Val Loss: 1.4708 | Train Acc: 59.57% | Val Acc: 59.55%\n",
      "Epoch 79/50000 | Data size: 219.47675417764935 | Train Loss: 1.2396 | Val Loss: 1.4393 | Train Acc: 60.01% | Val Acc: 59.74%\n",
      "Epoch 80/50000 | Data size: 221.67152171942584 | Train Loss: 1.2017 | Val Loss: 1.4090 | Train Acc: 60.34% | Val Acc: 59.96%\n",
      "Epoch 81/50000 | Data size: 223.8882369366201 | Train Loss: 1.1692 | Val Loss: 1.3811 | Train Acc: 60.64% | Val Acc: 60.01%\n",
      "Epoch 82/50000 | Data size: 226.1271193059863 | Train Loss: 1.1397 | Val Loss: 1.3522 | Train Acc: 60.72% | Val Acc: 60.22%\n",
      "Epoch 83/50000 | Data size: 228.38839049904615 | Train Loss: 1.0951 | Val Loss: 1.3225 | Train Acc: 61.09% | Val Acc: 60.25%\n",
      "Epoch 84/50000 | Data size: 230.6722744040366 | Train Loss: 1.2006 | Val Loss: 1.2933 | Train Acc: 60.93% | Val Acc: 60.38%\n",
      "Epoch 85/50000 | Data size: 232.97899714807699 | Train Loss: 1.0567 | Val Loss: 1.2634 | Train Acc: 61.19% | Val Acc: 60.65%\n",
      "Epoch 86/50000 | Data size: 235.30878711955776 | Train Loss: 1.0491 | Val Loss: 1.2359 | Train Acc: 61.42% | Val Acc: 60.67%\n",
      "Epoch 87/50000 | Data size: 237.66187499075335 | Train Loss: 0.9876 | Val Loss: 1.2080 | Train Acc: 61.18% | Val Acc: 60.86%\n",
      "Epoch 88/50000 | Data size: 240.03849374066087 | Train Loss: 0.9743 | Val Loss: 1.1827 | Train Acc: 61.12% | Val Acc: 60.93%\n",
      "Epoch 89/50000 | Data size: 242.43887867806748 | Train Loss: 0.9461 | Val Loss: 1.1577 | Train Acc: 61.34% | Val Acc: 61.08%\n",
      "Epoch 90/50000 | Data size: 244.86326746484815 | Train Loss: 0.9326 | Val Loss: 1.1343 | Train Acc: 61.73% | Val Acc: 61.16%\n",
      "Epoch 91/50000 | Data size: 247.31190013949663 | Train Loss: 0.9108 | Val Loss: 1.1105 | Train Acc: 61.69% | Val Acc: 61.32%\n",
      "Epoch 92/50000 | Data size: 249.7850191408916 | Train Loss: 0.8765 | Val Loss: 1.0874 | Train Acc: 61.80% | Val Acc: 61.48%\n",
      "Epoch 93/50000 | Data size: 252.28286933230052 | Train Loss: 0.8637 | Val Loss: 1.0668 | Train Acc: 62.15% | Val Acc: 61.49%\n",
      "Epoch 94/50000 | Data size: 254.8056980256235 | Train Loss: 0.8379 | Val Loss: 1.0454 | Train Acc: 62.16% | Val Acc: 61.66%\n",
      "Epoch 95/50000 | Data size: 257.35375500587975 | Train Loss: 0.8169 | Val Loss: 1.0263 | Train Acc: 62.26% | Val Acc: 61.66%\n",
      "Epoch 96/50000 | Data size: 259.92729255593855 | Train Loss: 0.7375 | Val Loss: 1.0047 | Train Acc: 62.23% | Val Acc: 61.81%\n",
      "Epoch 97/50000 | Data size: 262.5265654814979 | Train Loss: 0.7868 | Val Loss: 0.9849 | Train Acc: 62.41% | Val Acc: 61.89%\n",
      "Epoch 98/50000 | Data size: 265.1518311363129 | Train Loss: 0.7195 | Val Loss: 0.9670 | Train Acc: 62.29% | Val Acc: 61.89%\n",
      "Epoch 99/50000 | Data size: 267.80334944767606 | Train Loss: 0.7269 | Val Loss: 0.9471 | Train Acc: 62.41% | Val Acc: 62.05%\n",
      "Epoch 100/50000 | Data size: 270.48138294215283 | Train Loss: 0.7126 | Val Loss: 0.9276 | Train Acc: 62.54% | Val Acc: 62.13%\n",
      "Epoch 101/50000 | Data size: 273.1861967715744 | Train Loss: 0.6854 | Val Loss: 0.9105 | Train Acc: 62.72% | Val Acc: 62.16%\n",
      "Epoch 102/50000 | Data size: 275.91805873929013 | Train Loss: 0.6617 | Val Loss: 0.8927 | Train Acc: 62.91% | Val Acc: 62.34%\n",
      "Epoch 103/50000 | Data size: 278.67723932668304 | Train Loss: 0.6633 | Val Loss: 0.8772 | Train Acc: 63.19% | Val Acc: 62.31%\n",
      "Epoch 104/50000 | Data size: 281.4640117199499 | Train Loss: 0.6358 | Val Loss: 0.8606 | Train Acc: 63.46% | Val Acc: 62.42%\n",
      "Epoch 105/50000 | Data size: 284.27865183714937 | Train Loss: 0.6179 | Val Loss: 0.8453 | Train Acc: 63.63% | Val Acc: 62.49%\n",
      "Epoch 106/50000 | Data size: 287.12143835552087 | Train Loss: 0.6002 | Val Loss: 0.8301 | Train Acc: 63.57% | Val Acc: 62.59%\n",
      "Epoch 107/50000 | Data size: 289.99265273907605 | Train Loss: 0.5865 | Val Loss: 0.8161 | Train Acc: 63.46% | Val Acc: 62.64%\n",
      "Epoch 108/50000 | Data size: 292.8925792664668 | Train Loss: 0.5517 | Val Loss: 0.8010 | Train Acc: 63.49% | Val Acc: 62.69%\n",
      "Epoch 109/50000 | Data size: 295.8215050591314 | Train Loss: 0.5475 | Val Loss: 0.7861 | Train Acc: 63.43% | Val Acc: 62.82%\n",
      "Epoch 110/50000 | Data size: 298.77972010972275 | Train Loss: 0.5750 | Val Loss: 0.7728 | Train Acc: 63.49% | Val Acc: 62.83%\n",
      "Epoch 111/50000 | Data size: 301.76751731082 | Train Loss: 0.5458 | Val Loss: 0.7597 | Train Acc: 63.56% | Val Acc: 62.86%\n",
      "Epoch 112/50000 | Data size: 304.7851924839282 | Train Loss: 0.5323 | Val Loss: 0.7461 | Train Acc: 63.83% | Val Acc: 62.88%\n",
      "Epoch 113/50000 | Data size: 307.8330444087675 | Train Loss: 0.5306 | Val Loss: 0.7327 | Train Acc: 63.74% | Val Acc: 62.95%\n",
      "Epoch 114/50000 | Data size: 310.91137485285515 | Train Loss: 0.5052 | Val Loss: 0.7190 | Train Acc: 63.90% | Val Acc: 63.06%\n",
      "Epoch 115/50000 | Data size: 314.0204886013837 | Train Loss: 0.5028 | Val Loss: 0.7073 | Train Acc: 63.92% | Val Acc: 63.03%\n",
      "Epoch 116/50000 | Data size: 317.16069348739757 | Train Loss: 0.4929 | Val Loss: 0.6975 | Train Acc: 64.10% | Val Acc: 62.97%\n",
      "Epoch 117/50000 | Data size: 320.33230042227154 | Train Loss: 0.4932 | Val Loss: 0.6831 | Train Acc: 64.13% | Val Acc: 63.11%\n",
      "Epoch 118/50000 | Data size: 323.5356234264943 | Train Loss: 0.4735 | Val Loss: 0.6712 | Train Acc: 64.35% | Val Acc: 63.22%\n",
      "Epoch 119/50000 | Data size: 326.7709796607592 | Train Loss: 0.4456 | Val Loss: 0.6595 | Train Acc: 64.60% | Val Acc: 63.28%\n",
      "Epoch 120/50000 | Data size: 330.03868945736684 | Train Loss: 0.4389 | Val Loss: 0.6485 | Train Acc: 64.76% | Val Acc: 63.30%\n",
      "Epoch 121/50000 | Data size: 333.3390763519405 | Train Loss: 0.4623 | Val Loss: 0.6375 | Train Acc: 64.97% | Val Acc: 63.31%\n",
      "Epoch 122/50000 | Data size: 336.6724671154599 | Train Loss: 0.4413 | Val Loss: 0.6265 | Train Acc: 64.95% | Val Acc: 63.36%\n",
      "Epoch 123/50000 | Data size: 340.0391917866145 | Train Loss: 0.4258 | Val Loss: 0.6150 | Train Acc: 65.08% | Val Acc: 63.44%\n",
      "Epoch 124/50000 | Data size: 343.43958370448064 | Train Loss: 0.4320 | Val Loss: 0.6055 | Train Acc: 64.99% | Val Acc: 63.45%\n",
      "Epoch 125/50000 | Data size: 346.87397954152544 | Train Loss: 0.4161 | Val Loss: 0.5954 | Train Acc: 64.99% | Val Acc: 63.48%\n",
      "Epoch 126/50000 | Data size: 350.3427193369407 | Train Loss: 0.4308 | Val Loss: 0.5848 | Train Acc: 64.99% | Val Acc: 63.52%\n",
      "Epoch 127/50000 | Data size: 353.84614653031014 | Train Loss: 0.4150 | Val Loss: 0.5752 | Train Acc: 64.88% | Val Acc: 63.57%\n",
      "Epoch 128/50000 | Data size: 357.38460799561324 | Train Loss: 0.4151 | Val Loss: 0.5659 | Train Acc: 64.61% | Val Acc: 63.58%\n",
      "Epoch 129/50000 | Data size: 360.95845407556936 | Train Loss: 0.3852 | Val Loss: 0.5586 | Train Acc: 64.71% | Val Acc: 63.58%\n",
      "Epoch 130/50000 | Data size: 364.56803861632505 | Train Loss: 0.3977 | Val Loss: 0.5481 | Train Acc: 64.66% | Val Acc: 63.65%\n",
      "Epoch 131/50000 | Data size: 368.2137190024883 | Train Loss: 0.3717 | Val Loss: 0.5392 | Train Acc: 64.83% | Val Acc: 63.72%\n",
      "Epoch 132/50000 | Data size: 371.8958561925132 | Train Loss: 0.3598 | Val Loss: 0.5307 | Train Acc: 64.86% | Val Acc: 63.75%\n",
      "Epoch 133/50000 | Data size: 375.6148147544383 | Train Loss: 0.3737 | Val Loss: 0.5215 | Train Acc: 65.04% | Val Acc: 63.75%\n",
      "Epoch 134/50000 | Data size: 379.3709629019827 | Train Loss: 0.3847 | Val Loss: 0.5128 | Train Acc: 65.05% | Val Acc: 63.76%\n",
      "Epoch 135/50000 | Data size: 383.1646725310025 | Train Loss: 0.3646 | Val Loss: 0.5046 | Train Acc: 65.13% | Val Acc: 63.78%\n",
      "Epoch 136/50000 | Data size: 386.9963192563125 | Train Loss: 0.3502 | Val Loss: 0.4965 | Train Acc: 65.25% | Val Acc: 63.80%\n",
      "Epoch 137/50000 | Data size: 390.86628244887567 | Train Loss: 0.3445 | Val Loss: 0.4883 | Train Acc: 65.33% | Val Acc: 63.81%\n",
      "Epoch 138/50000 | Data size: 394.77494527336444 | Train Loss: 0.3309 | Val Loss: 0.4821 | Train Acc: 65.21% | Val Acc: 63.82%\n",
      "Epoch 139/50000 | Data size: 398.7226947260981 | Train Loss: 0.3533 | Val Loss: 0.4741 | Train Acc: 65.01% | Val Acc: 63.83%\n",
      "Epoch 140/50000 | Data size: 402.7099216733591 | Train Loss: 0.3197 | Val Loss: 0.4678 | Train Acc: 65.09% | Val Acc: 63.84%\n",
      "Epoch 141/50000 | Data size: 406.73702089009265 | Train Loss: 0.3113 | Val Loss: 0.4605 | Train Acc: 65.04% | Val Acc: 63.86%\n",
      "Epoch 142/50000 | Data size: 410.80439109899356 | Train Loss: 0.3071 | Val Loss: 0.4525 | Train Acc: 65.14% | Val Acc: 63.91%\n",
      "Epoch 143/50000 | Data size: 414.9124350099835 | Train Loss: 0.2939 | Val Loss: 0.4458 | Train Acc: 65.19% | Val Acc: 63.93%\n",
      "Epoch 144/50000 | Data size: 419.0615593600833 | Train Loss: 0.2918 | Val Loss: 0.4391 | Train Acc: 65.18% | Val Acc: 63.96%\n",
      "Epoch 145/50000 | Data size: 423.2521749536842 | Train Loss: 0.2759 | Val Loss: 0.4334 | Train Acc: 65.39% | Val Acc: 63.97%\n",
      "Epoch 146/50000 | Data size: 427.484696703221 | Train Loss: 0.2778 | Val Loss: 0.4271 | Train Acc: 65.58% | Val Acc: 64.00%\n",
      "Epoch 147/50000 | Data size: 431.7595436702532 | Train Loss: 0.2687 | Val Loss: 0.4215 | Train Acc: 65.73% | Val Acc: 64.02%\n",
      "Epoch 148/50000 | Data size: 436.0771391069557 | Train Loss: 0.2491 | Val Loss: 0.4155 | Train Acc: 65.82% | Val Acc: 64.04%\n",
      "Epoch 149/50000 | Data size: 440.43791049802525 | Train Loss: 0.2448 | Val Loss: 0.4108 | Train Acc: 65.76% | Val Acc: 64.05%\n",
      "Epoch 150/50000 | Data size: 444.8422896030055 | Train Loss: 0.2406 | Val Loss: 0.4051 | Train Acc: 65.79% | Val Acc: 64.08%\n",
      "Epoch 151/50000 | Data size: 449.2907124990356 | Train Loss: 0.2299 | Val Loss: 0.4003 | Train Acc: 65.71% | Val Acc: 64.09%\n",
      "Epoch 152/50000 | Data size: 453.78361962402596 | Train Loss: 0.2154 | Val Loss: 0.3950 | Train Acc: 65.51% | Val Acc: 64.12%\n",
      "Epoch 153/50000 | Data size: 458.32145582026624 | Train Loss: 0.2111 | Val Loss: 0.3907 | Train Acc: 65.41% | Val Acc: 64.12%\n",
      "Epoch 154/50000 | Data size: 462.9046703784689 | Train Loss: 0.2208 | Val Loss: 0.3857 | Train Acc: 65.45% | Val Acc: 64.15%\n",
      "Epoch 155/50000 | Data size: 467.5337170822536 | Train Loss: 0.2162 | Val Loss: 0.3810 | Train Acc: 65.46% | Val Acc: 64.15%\n",
      "Epoch 156/50000 | Data size: 472.20905425307615 | Train Loss: 0.2082 | Val Loss: 0.3759 | Train Acc: 65.36% | Val Acc: 64.18%\n",
      "Epoch 157/50000 | Data size: 476.9311447956069 | Train Loss: 0.2069 | Val Loss: 0.3731 | Train Acc: 65.48% | Val Acc: 64.17%\n",
      "Epoch 158/50000 | Data size: 481.70045624356294 | Train Loss: 0.2057 | Val Loss: 0.3689 | Train Acc: 65.62% | Val Acc: 64.18%\n",
      "Epoch 159/50000 | Data size: 486.51746080599855 | Train Loss: 0.1973 | Val Loss: 0.3624 | Train Acc: 65.44% | Val Acc: 64.21%\n",
      "Epoch 160/50000 | Data size: 491.38263541405854 | Train Loss: 0.1919 | Val Loss: 0.3587 | Train Acc: 65.40% | Val Acc: 64.22%\n",
      "Epoch 161/50000 | Data size: 496.29646176819915 | Train Loss: 0.1913 | Val Loss: 0.3541 | Train Acc: 65.45% | Val Acc: 64.22%\n",
      "Epoch 162/50000 | Data size: 501.25942638588117 | Train Loss: 0.1897 | Val Loss: 0.3493 | Train Acc: 65.35% | Val Acc: 64.23%\n",
      "Epoch 163/50000 | Data size: 506.27202064974 | Train Loss: 0.1828 | Val Loss: 0.3452 | Train Acc: 65.34% | Val Acc: 64.24%\n",
      "Epoch 164/50000 | Data size: 511.3347408562374 | Train Loss: 0.1845 | Val Loss: 0.3406 | Train Acc: 65.49% | Val Acc: 64.25%\n",
      "Epoch 165/50000 | Data size: 516.4480882647997 | Train Loss: 0.1779 | Val Loss: 0.3364 | Train Acc: 65.49% | Val Acc: 64.26%\n",
      "Epoch 166/50000 | Data size: 521.6125691474477 | Train Loss: 0.1715 | Val Loss: 0.3334 | Train Acc: 65.52% | Val Acc: 64.26%\n",
      "Epoch 167/50000 | Data size: 526.8286948389223 | Train Loss: 0.1774 | Val Loss: 0.3280 | Train Acc: 65.51% | Val Acc: 64.28%\n",
      "Epoch 168/50000 | Data size: 532.0969817873115 | Train Loss: 0.1721 | Val Loss: 0.3255 | Train Acc: 65.51% | Val Acc: 64.28%\n",
      "Epoch 169/50000 | Data size: 537.4179516051846 | Train Loss: 0.1662 | Val Loss: 0.3207 | Train Acc: 65.75% | Val Acc: 64.29%\n",
      "Epoch 170/50000 | Data size: 542.7921311212365 | Train Loss: 0.1625 | Val Loss: 0.3165 | Train Acc: 65.63% | Val Acc: 64.31%\n",
      "Epoch 171/50000 | Data size: 548.2200524324488 | Train Loss: 0.1571 | Val Loss: 0.3130 | Train Acc: 65.62% | Val Acc: 64.32%\n",
      "Epoch 172/50000 | Data size: 553.7022529567733 | Train Loss: 0.1495 | Val Loss: 0.3101 | Train Acc: 65.63% | Val Acc: 64.32%\n",
      "Epoch 173/50000 | Data size: 559.2392754863411 | Train Loss: 0.1451 | Val Loss: 0.3059 | Train Acc: 65.60% | Val Acc: 64.33%\n",
      "Epoch 174/50000 | Data size: 564.8316682412045 | Train Loss: 0.1385 | Val Loss: 0.3026 | Train Acc: 65.52% | Val Acc: 64.35%\n",
      "Epoch 175/50000 | Data size: 570.4799849236166 | Train Loss: 0.1385 | Val Loss: 0.2992 | Train Acc: 65.32% | Val Acc: 64.36%\n",
      "Epoch 176/50000 | Data size: 576.1847847728527 | Train Loss: 0.1335 | Val Loss: 0.2960 | Train Acc: 65.25% | Val Acc: 64.37%\n",
      "Epoch 177/50000 | Data size: 581.9466326205812 | Train Loss: 0.1265 | Val Loss: 0.2933 | Train Acc: 65.11% | Val Acc: 64.38%\n",
      "Epoch 178/50000 | Data size: 587.766098946787 | Train Loss: 0.1223 | Val Loss: 0.2901 | Train Acc: 64.99% | Val Acc: 64.38%\n",
      "Epoch 179/50000 | Data size: 593.6437599362548 | Train Loss: 0.1353 | Val Loss: 0.2872 | Train Acc: 65.01% | Val Acc: 64.39%\n",
      "Epoch 180/50000 | Data size: 599.5801975356173 | Train Loss: 0.1297 | Val Loss: 0.2837 | Train Acc: 64.90% | Val Acc: 64.41%\n",
      "Epoch 181/50000 | Data size: 605.5759995109735 | Train Loss: 0.1222 | Val Loss: 0.2806 | Train Acc: 64.79% | Val Acc: 64.42%\n",
      "Epoch 182/50000 | Data size: 611.6317595060832 | Train Loss: 0.1187 | Val Loss: 0.2775 | Train Acc: 64.71% | Val Acc: 64.43%\n",
      "Epoch 183/50000 | Data size: 617.7480771011441 | Train Loss: 0.1514 | Val Loss: 0.2742 | Train Acc: 64.73% | Val Acc: 64.45%\n",
      "Epoch 184/50000 | Data size: 623.9255578721555 | Train Loss: 0.1389 | Val Loss: 0.2771 | Train Acc: 64.79% | Val Acc: 64.44%\n",
      "Epoch 185/50000 | Data size: 630.1648134508771 | Train Loss: 0.1376 | Val Loss: 0.2792 | Train Acc: 64.97% | Val Acc: 64.41%\n",
      "Epoch 186/50000 | Data size: 636.4664615853859 | Train Loss: 0.1250 | Val Loss: 0.2849 | Train Acc: 64.82% | Val Acc: 64.44%\n",
      "Epoch 187/50000 | Data size: 642.8311262012397 | Train Loss: 0.1394 | Val Loss: 0.2682 | Train Acc: 64.79% | Val Acc: 64.46%\n",
      "Epoch 188/50000 | Data size: 649.2594374632521 | Train Loss: 0.1179 | Val Loss: 0.2638 | Train Acc: 64.73% | Val Acc: 64.47%\n",
      "Epoch 189/50000 | Data size: 655.7520318378846 | Train Loss: 0.1149 | Val Loss: 0.2589 | Train Acc: 64.91% | Val Acc: 64.48%\n",
      "Epoch 190/50000 | Data size: 662.3095521562634 | Train Loss: 0.1097 | Val Loss: 0.2553 | Train Acc: 64.96% | Val Acc: 64.48%\n",
      "Epoch 191/50000 | Data size: 668.932647677826 | Train Loss: 0.1120 | Val Loss: 0.2529 | Train Acc: 64.98% | Val Acc: 64.49%\n",
      "Epoch 192/50000 | Data size: 675.6219741546042 | Train Loss: 0.1144 | Val Loss: 0.2490 | Train Acc: 64.82% | Val Acc: 64.50%\n",
      "Epoch 193/50000 | Data size: 682.3781938961503 | Train Loss: 0.1097 | Val Loss: 0.2453 | Train Acc: 64.90% | Val Acc: 64.51%\n",
      "Epoch 194/50000 | Data size: 689.2019758351117 | Train Loss: 0.1062 | Val Loss: 0.2431 | Train Acc: 65.03% | Val Acc: 64.51%\n",
      "Epoch 195/50000 | Data size: 696.0939955934629 | Train Loss: 0.1007 | Val Loss: 0.2400 | Train Acc: 65.15% | Val Acc: 64.52%\n",
      "Epoch 196/50000 | Data size: 703.0549355493974 | Train Loss: 0.0993 | Val Loss: 0.2375 | Train Acc: 65.28% | Val Acc: 64.53%\n",
      "Epoch 197/50000 | Data size: 710.0854849048915 | Train Loss: 0.0991 | Val Loss: 0.2350 | Train Acc: 65.29% | Val Acc: 64.54%\n",
      "Epoch 198/50000 | Data size: 717.1863397539404 | Train Loss: 0.1070 | Val Loss: 0.2314 | Train Acc: 65.31% | Val Acc: 64.55%\n",
      "Epoch 199/50000 | Data size: 724.3582031514798 | Train Loss: 0.0977 | Val Loss: 0.2293 | Train Acc: 65.39% | Val Acc: 64.56%\n",
      "Epoch 200/50000 | Data size: 731.6017851829946 | Train Loss: 0.0924 | Val Loss: 0.2267 | Train Acc: 65.39% | Val Acc: 64.58%\n",
      "Epoch 201/50000 | Data size: 738.9178030348246 | Train Loss: 0.0932 | Val Loss: 0.2248 | Train Acc: 65.34% | Val Acc: 64.59%\n",
      "Epoch 202/50000 | Data size: 746.3069810651729 | Train Loss: 0.0878 | Val Loss: 0.2216 | Train Acc: 65.45% | Val Acc: 64.61%\n",
      "Epoch 203/50000 | Data size: 753.7700508758246 | Train Loss: 0.0850 | Val Loss: 0.2196 | Train Acc: 65.38% | Val Acc: 64.62%\n",
      "Epoch 204/50000 | Data size: 761.3077513845828 | Train Loss: 0.0855 | Val Loss: 0.2172 | Train Acc: 65.51% | Val Acc: 64.64%\n",
      "Epoch 205/50000 | Data size: 768.9208288984287 | Train Loss: 0.0882 | Val Loss: 0.2156 | Train Acc: 65.58% | Val Acc: 64.64%\n",
      "Epoch 206/50000 | Data size: 776.610037187413 | Train Loss: 0.0858 | Val Loss: 0.2126 | Train Acc: 65.74% | Val Acc: 64.66%\n",
      "Epoch 207/50000 | Data size: 784.3761375592871 | Train Loss: 0.0822 | Val Loss: 0.2104 | Train Acc: 65.72% | Val Acc: 64.68%\n",
      "Epoch 208/50000 | Data size: 792.21989893488 | Train Loss: 0.0807 | Val Loss: 0.2078 | Train Acc: 65.64% | Val Acc: 64.69%\n",
      "Epoch 209/50000 | Data size: 800.1420979242288 | Train Loss: 0.0796 | Val Loss: 0.2051 | Train Acc: 65.61% | Val Acc: 64.70%\n",
      "Epoch 210/50000 | Data size: 808.1435189034711 | Train Loss: 0.0760 | Val Loss: 0.2032 | Train Acc: 65.52% | Val Acc: 64.71%\n",
      "Epoch 211/50000 | Data size: 816.2249540925058 | Train Loss: 0.0752 | Val Loss: 0.2007 | Train Acc: 65.51% | Val Acc: 64.72%\n",
      "Epoch 212/50000 | Data size: 824.3872036334308 | Train Loss: 0.0746 | Val Loss: 0.1987 | Train Acc: 65.47% | Val Acc: 64.72%\n",
      "Epoch 213/50000 | Data size: 832.6310756697651 | Train Loss: 0.0714 | Val Loss: 0.1966 | Train Acc: 65.63% | Val Acc: 64.73%\n",
      "Epoch 214/50000 | Data size: 840.9573864264628 | Train Loss: 0.0687 | Val Loss: 0.1949 | Train Acc: 65.48% | Val Acc: 64.74%\n",
      "Epoch 215/50000 | Data size: 849.3669602907274 | Train Loss: 0.0668 | Val Loss: 0.1930 | Train Acc: 65.55% | Val Acc: 64.75%\n",
      "Epoch 216/50000 | Data size: 857.8606298936347 | Train Loss: 0.0662 | Val Loss: 0.1909 | Train Acc: 65.51% | Val Acc: 64.76%\n",
      "Epoch 217/50000 | Data size: 866.4392361925711 | Train Loss: 0.0635 | Val Loss: 0.1891 | Train Acc: 65.42% | Val Acc: 64.77%\n",
      "Epoch 218/50000 | Data size: 875.1036285544968 | Train Loss: 0.0608 | Val Loss: 0.1872 | Train Acc: 65.43% | Val Acc: 64.78%\n",
      "Epoch 219/50000 | Data size: 883.8546648400418 | Train Loss: 0.0610 | Val Loss: 0.1854 | Train Acc: 65.43% | Val Acc: 64.79%\n",
      "Epoch 220/50000 | Data size: 892.6932114884422 | Train Loss: 0.0574 | Val Loss: 0.1843 | Train Acc: 65.51% | Val Acc: 64.79%\n",
      "Epoch 221/50000 | Data size: 901.6201436033266 | Train Loss: 0.0552 | Val Loss: 0.1821 | Train Acc: 65.50% | Val Acc: 64.80%\n",
      "Epoch 222/50000 | Data size: 910.6363450393599 | Train Loss: 0.0538 | Val Loss: 0.1812 | Train Acc: 65.54% | Val Acc: 64.80%\n",
      "Epoch 223/50000 | Data size: 919.7427084897535 | Train Loss: 0.0519 | Val Loss: 0.1792 | Train Acc: 65.51% | Val Acc: 64.81%\n",
      "Epoch 224/50000 | Data size: 928.940135574651 | Train Loss: 0.0501 | Val Loss: 0.1777 | Train Acc: 65.58% | Val Acc: 64.81%\n",
      "Epoch 225/50000 | Data size: 938.2295369303974 | Train Loss: 0.0567 | Val Loss: 0.1756 | Train Acc: 65.64% | Val Acc: 64.81%\n",
      "Epoch 226/50000 | Data size: 947.6118322997014 | Train Loss: 0.0549 | Val Loss: 0.1744 | Train Acc: 65.73% | Val Acc: 64.82%\n",
      "Epoch 227/50000 | Data size: 957.0879506226984 | Train Loss: 0.0533 | Val Loss: 0.1728 | Train Acc: 65.79% | Val Acc: 64.83%\n",
      "Epoch 228/50000 | Data size: 966.6588301289254 | Train Loss: 0.0512 | Val Loss: 0.1708 | Train Acc: 65.85% | Val Acc: 64.83%\n",
      "Epoch 229/50000 | Data size: 976.3254184302147 | Train Loss: 0.0513 | Val Loss: 0.1688 | Train Acc: 65.90% | Val Acc: 64.84%\n",
      "Epoch 230/50000 | Data size: 986.0886726145168 | Train Loss: 0.0489 | Val Loss: 0.1680 | Train Acc: 65.93% | Val Acc: 64.84%\n",
      "Epoch 231/50000 | Data size: 995.949559340662 | Train Loss: 0.0480 | Val Loss: 0.1655 | Train Acc: 65.87% | Val Acc: 64.84%\n",
      "Epoch 232/50000 | Data size: 1005.9090549340685 | Train Loss: 0.0493 | Val Loss: 0.1643 | Train Acc: 65.95% | Val Acc: 64.85%\n",
      "Epoch 233/50000 | Data size: 1015.9681454834092 | Train Loss: 0.0486 | Val Loss: 0.1622 | Train Acc: 65.91% | Val Acc: 64.85%\n",
      "Epoch 234/50000 | Data size: 1026.1278269382433 | Train Loss: 0.0463 | Val Loss: 0.1612 | Train Acc: 65.79% | Val Acc: 64.85%\n",
      "Epoch 235/50000 | Data size: 1036.3891052076258 | Train Loss: 0.0431 | Val Loss: 0.1588 | Train Acc: 65.71% | Val Acc: 64.86%\n",
      "Epoch 236/50000 | Data size: 1046.752996259702 | Train Loss: 0.0455 | Val Loss: 0.1583 | Train Acc: 65.71% | Val Acc: 64.87%\n",
      "Epoch 237/50000 | Data size: 1057.220526222299 | Train Loss: 0.0430 | Val Loss: 0.1555 | Train Acc: 65.75% | Val Acc: 64.88%\n",
      "Epoch 238/50000 | Data size: 1067.792731484522 | Train Loss: 0.0441 | Val Loss: 0.1546 | Train Acc: 65.80% | Val Acc: 64.89%\n",
      "Epoch 239/50000 | Data size: 1078.4706587993671 | Train Loss: 0.0420 | Val Loss: 0.1534 | Train Acc: 65.74% | Val Acc: 64.89%\n",
      "Epoch 240/50000 | Data size: 1089.2553653873608 | Train Loss: 0.0408 | Val Loss: 0.1520 | Train Acc: 65.65% | Val Acc: 64.90%\n",
      "Epoch 241/50000 | Data size: 1100.1479190412344 | Train Loss: 0.0385 | Val Loss: 0.1509 | Train Acc: 65.62% | Val Acc: 64.90%\n",
      "Epoch 242/50000 | Data size: 1111.1493982316467 | Train Loss: 0.0394 | Val Loss: 0.1489 | Train Acc: 65.66% | Val Acc: 64.91%\n",
      "Epoch 243/50000 | Data size: 1122.2608922139632 | Train Loss: 0.0384 | Val Loss: 0.1491 | Train Acc: 65.71% | Val Acc: 64.91%\n",
      "Epoch 244/50000 | Data size: 1133.4835011361029 | Train Loss: 0.0359 | Val Loss: 0.1459 | Train Acc: 65.67% | Val Acc: 64.93%\n",
      "Epoch 245/50000 | Data size: 1144.818336147464 | Train Loss: 0.0411 | Val Loss: 0.1451 | Train Acc: 65.67% | Val Acc: 64.94%\n",
      "Epoch 246/50000 | Data size: 1156.2665195089385 | Train Loss: 0.0419 | Val Loss: 0.1437 | Train Acc: 65.67% | Val Acc: 64.94%\n",
      "Epoch 247/50000 | Data size: 1167.829184704028 | Train Loss: 0.0407 | Val Loss: 0.1430 | Train Acc: 65.71% | Val Acc: 64.94%\n",
      "Epoch 248/50000 | Data size: 1179.5074765510683 | Train Loss: 0.0403 | Val Loss: 0.1399 | Train Acc: 65.82% | Val Acc: 64.96%\n",
      "Epoch 249/50000 | Data size: 1191.302551316579 | Train Loss: 0.0392 | Val Loss: 0.1389 | Train Acc: 65.93% | Val Acc: 64.96%\n",
      "Epoch 250/50000 | Data size: 1203.2155768297448 | Train Loss: 0.0375 | Val Loss: 0.1369 | Train Acc: 66.01% | Val Acc: 64.96%\n",
      "Epoch 251/50000 | Data size: 1215.2477325980421 | Train Loss: 0.0364 | Val Loss: 0.1357 | Train Acc: 65.94% | Val Acc: 64.97%\n",
      "Epoch 252/50000 | Data size: 1227.4002099240226 | Train Loss: 0.0347 | Val Loss: 0.1342 | Train Acc: 65.97% | Val Acc: 64.97%\n",
      "Epoch 253/50000 | Data size: 1239.674212023263 | Train Loss: 0.0338 | Val Loss: 0.1332 | Train Acc: 66.08% | Val Acc: 64.98%\n",
      "Epoch 254/50000 | Data size: 1252.0709541434956 | Train Loss: 0.0339 | Val Loss: 0.1321 | Train Acc: 66.07% | Val Acc: 64.98%\n",
      "Epoch 255/50000 | Data size: 1264.5916636849306 | Train Loss: 0.0334 | Val Loss: 0.1301 | Train Acc: 65.99% | Val Acc: 65.00%\n",
      "Epoch 256/50000 | Data size: 1277.23758032178 | Train Loss: 0.0337 | Val Loss: 0.1294 | Train Acc: 66.13% | Val Acc: 65.01%\n",
      "Epoch 257/50000 | Data size: 1290.0099561249976 | Train Loss: 0.0333 | Val Loss: 0.1284 | Train Acc: 66.27% | Val Acc: 65.01%\n",
      "Epoch 258/50000 | Data size: 1302.9100556862477 | Train Loss: 0.0329 | Val Loss: 0.1268 | Train Acc: 66.26% | Val Acc: 65.02%\n",
      "Epoch 259/50000 | Data size: 1315.9391562431101 | Train Loss: 0.0330 | Val Loss: 0.1259 | Train Acc: 66.22% | Val Acc: 65.02%\n",
      "Epoch 260/50000 | Data size: 1329.0985478055413 | Train Loss: 0.0344 | Val Loss: 0.1262 | Train Acc: 66.16% | Val Acc: 65.02%\n",
      "Epoch 261/50000 | Data size: 1342.3895332835968 | Train Loss: 0.0333 | Val Loss: 0.1230 | Train Acc: 66.18% | Val Acc: 65.03%\n",
      "Epoch 262/50000 | Data size: 1355.8134286164327 | Train Loss: 0.0317 | Val Loss: 0.1225 | Train Acc: 66.22% | Val Acc: 65.03%\n",
      "Epoch 263/50000 | Data size: 1369.3715629025971 | Train Loss: 0.0340 | Val Loss: 0.1204 | Train Acc: 66.32% | Val Acc: 65.04%\n",
      "Epoch 264/50000 | Data size: 1383.065278531623 | Train Loss: 0.0318 | Val Loss: 0.1197 | Train Acc: 66.38% | Val Acc: 65.05%\n",
      "Epoch 265/50000 | Data size: 1396.8959313169394 | Train Loss: 0.0305 | Val Loss: 0.1189 | Train Acc: 66.36% | Val Acc: 65.05%\n",
      "Epoch 266/50000 | Data size: 1410.8648906301087 | Train Loss: 0.0323 | Val Loss: 0.1182 | Train Acc: 66.46% | Val Acc: 65.05%\n",
      "Epoch 267/50000 | Data size: 1424.9735395364098 | Train Loss: 0.0309 | Val Loss: 0.1153 | Train Acc: 66.43% | Val Acc: 65.06%\n",
      "Epoch 268/50000 | Data size: 1439.2232749317739 | Train Loss: 0.1216 | Val Loss: 0.1788 | Train Acc: 65.77% | Val Acc: 64.86%\n",
      "Epoch 269/50000 | Data size: 1453.6155076810917 | Train Loss: 0.1270 | Val Loss: 0.1362 | Train Acc: 66.04% | Val Acc: 65.00%\n",
      "Epoch 270/50000 | Data size: 1468.1516627579026 | Train Loss: 0.0523 | Val Loss: 0.1189 | Train Acc: 66.25% | Val Acc: 65.05%\n",
      "Epoch 271/50000 | Data size: 1482.8331793854816 | Train Loss: 0.0390 | Val Loss: 0.1163 | Train Acc: 66.19% | Val Acc: 65.06%\n",
      "Epoch 272/50000 | Data size: 1497.6615111793365 | Train Loss: 0.0348 | Val Loss: 0.1143 | Train Acc: 66.19% | Val Acc: 65.07%\n",
      "Epoch 273/50000 | Data size: 1512.6381262911298 | Train Loss: 0.0338 | Val Loss: 0.1132 | Train Acc: 66.22% | Val Acc: 65.07%\n",
      "Epoch 274/50000 | Data size: 1527.764507554041 | Train Loss: 0.0338 | Val Loss: 0.1119 | Train Acc: 66.29% | Val Acc: 65.08%\n",
      "Epoch 275/50000 | Data size: 1543.0421526295816 | Train Loss: 0.0316 | Val Loss: 0.1105 | Train Acc: 66.36% | Val Acc: 65.08%\n",
      "Epoch 276/50000 | Data size: 1558.4725741558773 | Train Loss: 0.0299 | Val Loss: 0.1096 | Train Acc: 66.31% | Val Acc: 65.09%\n",
      "Epoch 277/50000 | Data size: 1574.057299897436 | Train Loss: 0.0287 | Val Loss: 0.1084 | Train Acc: 66.21% | Val Acc: 65.09%\n",
      "Epoch 278/50000 | Data size: 1589.7978728964104 | Train Loss: 0.0275 | Val Loss: 0.1076 | Train Acc: 66.12% | Val Acc: 65.10%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from JSON files in the specified folder\n",
    "def load_data(folder_path):\n",
    "    texts = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                if isinstance(data, dict) and 'response_content' in data:\n",
    "                    texts.append(data['response_content'])\n",
    "                elif isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        if 'response_content' in item:\n",
    "                            texts.append(item['response_content'])\n",
    "    return texts\n",
    "\n",
    "# Preprocess text data using a tokenizer\n",
    "def preprocess_texts(texts, tokenizer, max_length=512):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    return encodings.input_ids, encodings.attention_mask\n",
    "\n",
    "# Define the Autoencoder model using a pretrained BERT model as the encoder\n",
    "class BertAutoencoder(nn.Module):\n",
    "    def __init__(self, bert_model_name, lstm_units=256, max_length=512):\n",
    "        super(BertAutoencoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.encoder = self.bert.encoder\n",
    "        self.decoder = nn.LSTM(self.bert.config.hidden_size, lstm_units, batch_first=True)\n",
    "        self.output_layer = nn.Linear(lstm_units, self.bert.config.vocab_size)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        encoder_outputs = bert_outputs.last_hidden_state\n",
    "        decoder_outputs, _ = self.decoder(encoder_outputs)\n",
    "        output = self.output_layer(decoder_outputs)\n",
    "        return output\n",
    "\n",
    "# Progressive data increment method\n",
    "def progressive_training(model, data, attention_masks, tokenizer, initial_size, increment_ratio, max_epochs, device):\n",
    "    current_size = initial_size\n",
    "    total_size = len(data)\n",
    "    test_size = 0.1\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    metrics = []\n",
    "    \n",
    "    # Split data into training and test sets\n",
    "    train_indices, test_indices = train_test_split(np.arange(total_size), test_size=test_size, shuffle=True)\n",
    "    \n",
    "    x_test, mask_test = data[test_indices], attention_masks[test_indices]\n",
    "    x_train_full, mask_train_full = data[train_indices], attention_masks[train_indices]\n",
    "    \n",
    "    test_dataset = TensorDataset(x_test, x_test, mask_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        if current_size > len(train_indices):\n",
    "            break\n",
    "            \n",
    "        current_train_indices = train_indices[:int(current_size)]\n",
    "        current_data = data[current_train_indices]\n",
    "        current_masks = attention_masks[current_train_indices]\n",
    "        \n",
    "        x_train, y_train, mask_train = current_data, current_data, current_masks\n",
    "        \n",
    "        train_dataset = TensorDataset(x_train, y_train, mask_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for inputs, targets, masks in train_loader:\n",
    "            inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, masks)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(2)\n",
    "            total += targets.size(0) * targets.size(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        train_accuracy = 100. * correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, masks in test_loader:\n",
    "                inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "                outputs = model(inputs, masks)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(2)\n",
    "                total += targets.size(0) * targets.size(1)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        val_accuracy = 100. * correct / total\n",
    "        metrics.append((current_size, train_loss/len(train_loader), val_loss/len(test_loader), train_accuracy, val_accuracy))\n",
    "        current_size += current_size * increment_ratio\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{max_epochs} | Data size: {current_size} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(test_loader):.4f} | Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Plotting function\n",
    "def plot_metrics(metrics):\n",
    "    sizes, train_losses, val_losses, train_accuracies, val_accuracies = zip(*metrics)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(sizes, train_losses, label='Training Loss')\n",
    "    plt.plot(sizes, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss vs Data Size')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(sizes, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(sizes, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy vs Data Size')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_test_data_and_accuracy(test_data, test_masks, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(TensorDataset(test_data, test_masks), batch_size=1, shuffle=False)\n",
    "\n",
    "    original_texts = tokenizer.batch_decode(test_data, skip_special_tokens=True)\n",
    "    predicted_texts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in test_loader:\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "            outputs = model(inputs, masks)\n",
    "            _, predicted = outputs.max(2)\n",
    "            predicted_texts.extend(tokenizer.batch_decode(predicted.cpu(), skip_special_tokens=True))\n",
    "\n",
    "    for original, predicted in zip(original_texts, predicted_texts):\n",
    "        print(f'Original: {original}')\n",
    "        print(f'Predicted: {predicted}')\n",
    "        print('---')\n",
    "\n",
    "# Main function\n",
    "def main(folder_path):\n",
    "    texts = load_data(folder_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "    input_ids, attention_masks = preprocess_texts(texts, tokenizer)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BertAutoencoder('bert-base-uncased', lstm_units=256, max_length=512).to(device)\n",
    "\n",
    "    initial_size = 100\n",
    "    increment_ratio = 0.01\n",
    "    max_epochs = 50000\n",
    "\n",
    "    metrics = progressive_training(model, input_ids, attention_masks, tokenizer, initial_size, increment_ratio, max_epochs, device)\n",
    "\n",
    "    # Save the metrics to a file\n",
    "    with open('training_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    plot_metrics(metrics)\n",
    "\n",
    "    x_train, x_test, mask_train, mask_test = train_test_split(input_ids, attention_masks, test_size=0.1)\n",
    "    plot_test_data_and_accuracy(x_test[:5], mask_test[:5], model, tokenizer, device)  # Display 5 samples\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = '/workspace/slice-monorepo/thebeast/combined'  # Replace with the path to your folder containing JSON files\n",
    "    main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271db30-e857-421d-aa45-7b81876f8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from JSON files in the specified folder\n",
    "def load_data(folder_path):\n",
    "    texts = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                if isinstance(data, dict) and 'response_content' in data:\n",
    "                    texts.append(data['response_content'])\n",
    "                elif isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        if 'response_content' in item:\n",
    "                            texts.append(item['response_content'])\n",
    "    return texts\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "def preprocess_texts(texts, tokenizer, max_length=512):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    return padded_sequences\n",
    "\n",
    "class TextTokenizer:\n",
    "    def __init__(self, oov_token=\"<OOV>\"):\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {oov_token: 1}\n",
    "        self.index_word = {1: oov_token}\n",
    "        self.word_counts = {}\n",
    "        self.num_words = 2\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                if word not in self.word_counts:\n",
    "                    self.word_counts[word] = 1\n",
    "                else:\n",
    "                    self.word_counts[word] += 1\n",
    "\n",
    "        sorted_words = sorted(self.word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        for word, _ in sorted_words:\n",
    "            self.word_index[word] = self.num_words\n",
    "            self.index_word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequences.append([self.word_index.get(word, self.word_index[self.oov_token]) for word in text.split()])\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for seq in sequences:\n",
    "            texts.append(' '.join([self.index_word.get(idx, self.oov_token) for idx in seq]))\n",
    "        return texts\n",
    "\n",
    "# Padding function\n",
    "def pad_sequences(sequences, maxlen, padding='post', truncating='post'):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'pre':\n",
    "                seq = seq[-maxlen:]\n",
    "            elif truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "        if padding == 'pre':\n",
    "            padded_sequences[i, -len(seq):] = seq\n",
    "        elif padding == 'post':\n",
    "            padded_sequences[i, :len(seq)] = seq\n",
    "    return padded_sequences\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class TextAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, lstm_units=256, max_length=512):\n",
    "        super(TextAutoencoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(embedding_dim, lstm_units, batch_first=True)\n",
    "        self.decoder = nn.LSTM(lstm_units, embedding_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.encoder(embedded)\n",
    "        repeated_hidden = hidden.repeat(self.max_length, 1, 1).permute(1, 0, 2)\n",
    "        decoded, _ = self.decoder(repeated_hidden)\n",
    "        output = self.output_layer(decoded)\n",
    "        return output\n",
    "\n",
    "# Progressive data increment method\n",
    "def progressive_training(model, data, tokenizer, vocab_size, initial_size, increment_ratio, max_epochs, device):\n",
    "    current_size = initial_size\n",
    "    total_size = len(data)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    metrics = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        if current_size > total_size:\n",
    "            break\n",
    "        current_data = data[:current_size]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(current_data, current_data, test_size=0.1)\n",
    "\n",
    "        x_train = torch.tensor(x_train, dtype=torch.long).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        x_test = torch.tensor(x_test, dtype=torch.long).to(device)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "        train_dataset = TensorDataset(x_train, y_train)\n",
    "        test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(2)\n",
    "            total += targets.size(0) * targets.size(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        train_accuracy = 100. * correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(2)\n",
    "                total += targets.size(0) * targets.size(1)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        val_accuracy = 100. * correct / total\n",
    "        metrics.append((current_size, train_accuracy, val_accuracy))\n",
    "        current_size += int(current_size * increment_ratio)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{max_epochs} completed with {current_size} samples. Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Main function\n",
    "def main(folder_path):\n",
    "    texts = load_data(folder_path)\n",
    "    tokenizer = TextTokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = preprocess_texts(texts, tokenizer)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TextAutoencoder(vocab_size, embedding_dim=128, lstm_units=256, max_length=512).to(device)\n",
    "\n",
    "    initial_size = 100\n",
    "    increment_ratio = 0.01\n",
    "    max_epochs = 5000\n",
    "\n",
    "    metrics = progressive_training(model, sequences, tokenizer, vocab_size, initial_size, increment_ratio, max_epochs, device)\n",
    "\n",
    "    # Save the metrics to a file\n",
    "    with open('training_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = '/workspace/slice-monorepo/thebeast/data/combined'  # Replace with the path to your folder containing JSON files\n",
    "    main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a67a1-70cc-4d79-b97d-54b0d331c1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de3551-865d-4ef4-b981-0507b0162e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

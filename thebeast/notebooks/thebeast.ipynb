{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0225e162-4e87-4b2f-b544-98fa1fac6925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.31.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting hdbscan\n",
      "  Downloading hdbscan-0.8.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.7.3-py3-none-any.whl.metadata (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions<5,>=4.7 (from openai)\n",
      "  Downloading typing_extensions-4.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.15.1 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.3.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Collecting cython<3,>=0.27 (from hdbscan)\n",
      "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting numba>=0.51.2 (from umap-learn)\n",
      "  Downloading numba-0.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.12-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Downloading keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Downloading fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llvmlite<0.43,>=0.42.0dev0 (from numba>=0.51.2->umap-learn)\n",
      "  Downloading llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (1.26.13)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading openai-1.31.0-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.1/324.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading hdbscan-0.8.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading fonttools-4.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.1-py3-none-any.whl (37 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, typing-extensions, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, scipy, safetensors, regex, protobuf, opt-einsum, ml-dtypes, mdurl, markdown, llvmlite, kiwisolver, joblib, h5py, h11, grpcio, google-pasta, gast, fsspec, fonttools, cython, cycler, contourpy, astunparse, annotated-types, absl-py, tensorboard, scikit-learn, pydantic-core, optree, numba, matplotlib, markdown-it-py, huggingface-hub, httpcore, tokenizers, rich, pynndescent, pydantic, httpx, hdbscan, umap-learn, transformers, openai, keras, tensorflow, sentence-transformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed absl-py-2.1.0 annotated-types-0.7.0 astunparse-1.6.3 contourpy-1.2.1 cycler-0.12.1 cython-0.29.37 flatbuffers-24.3.25 fonttools-4.53.0 fsspec-2024.6.0 gast-0.5.4 google-pasta-0.2.0 grpcio-1.64.1 h11-0.14.0 h5py-3.11.0 hdbscan-0.8.36 httpcore-1.0.5 httpx-0.27.0 huggingface-hub-0.23.3 joblib-1.4.2 keras-3.3.3 kiwisolver-1.4.5 libclang-18.1.1 llvmlite-0.42.0 markdown-3.6 markdown-it-py-3.0.0 matplotlib-3.9.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 numba-0.59.1 openai-1.31.0 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 pydantic-2.7.3 pydantic-core-2.18.4 pynndescent-0.5.12 regex-2024.5.15 rich-13.7.1 safetensors-0.4.3 scikit-learn-1.5.0 scipy-1.13.1 sentence-transformers-3.0.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 threadpoolctl-3.5.0 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.41.2 typing-extensions-4.12.1 umap-learn-0.5.6 werkzeug-3.0.3 wrapt-1.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai sentence-transformers scikit-learn numpy matplotlib hdbscan umap-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e59b870-bb70-4fa8-83a9-437676246f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Flask\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting blinker\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting Werkzeug>=3.0.0 (from Flask)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Jinja2>=3.1.2 (from Flask)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting click>=8.1.3 (from Flask)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2>=3.1.2->Flask)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Installing collected packages: MarkupSafe, itsdangerous, click, blinker, Werkzeug, Jinja2, Flask\n",
      "Successfully installed Flask-3.0.3 Jinja2-3.1.2 MarkupSafe-2.1.2 Werkzeug-3.0.3 blinker-1.8.2 click-8.1.7 itsdangerous-2.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Flask --ignore-installed blinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80c8d0-2192-4bbf-9255-b96e04d72a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "apt-get update\n",
    "apt-get install lsof\n",
    "kill -9 $(lsof -t -i:4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3cf70f-5549-4944-8859-45e6e29ff22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4804273-9305-44c2-b154-417912d696db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "df['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n",
    "df.to_csv('output/embedded_1k_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e967da9-c765-4a32-a64d-6a612b376735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded. File ID: file-9jyr72r96jaxWQCY1TN3vGNU\n",
      "Batch job created. Batch ID: batch_RreSlmceawjRlFaBLSP6Bv5K\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     Repeated BATCH     ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def upload_file(file_path, purpose):\n",
    "    url = \"https://api.openai.com/v1/files\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    }\n",
    "    files = {\n",
    "        'file': open(file_path, 'rb'),\n",
    "        'purpose': (None, purpose),\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "    return response.json()\n",
    "\n",
    "def create_batch(input_file_id, endpoint, completion_window):\n",
    "    url = \"https://api.openai.com/v1/batches\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    data = {\n",
    "        \"input_file_id\": input_file_id,\n",
    "        \"endpoint\": endpoint,\n",
    "        \"completion_window\": completion_window,\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "def create_jsonl_file(prompt, n, file_path):\n",
    "    data = [{\n",
    "        \"custom_id\": f\"request-{i+1}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "    } for i in range(n)]\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def main():\n",
    "    prompt = \"I want a script that generates an analysis of some text data. Pull the text data from any source of text it doesn't matter what. Get the text data from a source you know of.\"\n",
    "    n = 100  # number of jobs\n",
    "    file_path = \"batch_test.jsonl\"\n",
    "    endpoint = \"/v1/chat/completions\"\n",
    "    completion_window = \"24h\"\n",
    "\n",
    "    # Step 1: Create the file\n",
    "    create_jsonl_file(prompt, n, file_path)\n",
    "    \n",
    "    # Step 2: Upload the file\n",
    "    upload_response = upload_file(file_path, \"batch\")\n",
    "    input_file_id = upload_response['id']\n",
    "    print(f\"File uploaded. File ID: {input_file_id}\")\n",
    "\n",
    "    # Step 3: Create the batch job\n",
    "    batch_response = create_batch(input_file_id, endpoint, completion_window)\n",
    "    batch_id = batch_response['id']\n",
    "    print(f\"Batch job created. Batch ID: {batch_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86630db2-c700-48e5-b043-ca86574c80e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded. File ID: file-2QvkFdNrCoqUU1jZG312gR6h\n",
      "Batch job created. Batch ID: batch_7wAG2AjElDG4RFRGBKqwjyfj\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     HP Sweep BATCH     ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "def upload_file(file_path, purpose):\n",
    "    url = \"https://api.openai.com/v1/files\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    }\n",
    "    files = {\n",
    "        'file': open(file_path, 'rb'),\n",
    "        'purpose': (None, purpose),\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "    return response.json()\n",
    "\n",
    "def create_batch(input_file_id, endpoint, completion_window):\n",
    "    url = \"https://api.openai.com/v1/batches\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    data = {\n",
    "        \"input_file_id\": input_file_id,\n",
    "        \"endpoint\": endpoint,\n",
    "        \"completion_window\": completion_window,\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "def create_jsonl_file(prompts, parameter_combinations, file_path):\n",
    "    data = []\n",
    "    for i, (prompt, params) in enumerate(zip(prompts, parameter_combinations)):\n",
    "        request_data = {\n",
    "            \"custom_id\": f\"request-{i+1}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": params[0],\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": params[1],\n",
    "                \"temperature\": params[2],\n",
    "                \"top_p\": params[3],\n",
    "                \"frequency_penalty\": params[4],\n",
    "                \"presence_penalty\": params[5],\n",
    "                \"stop\": params[6],\n",
    "                \"user\": params[7],\n",
    "                \"logprobs\": True,\n",
    "                \"top_logprobs\": 5\n",
    "            }\n",
    "        }\n",
    "        data.append(request_data)\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def generate_parameter_combinations(models, max_tokens_list, temperatures, top_ps, frequency_penalties, presence_penalties, stops, users):\n",
    "    combinations = list(itertools.product(models, max_tokens_list, temperatures, top_ps, frequency_penalties, presence_penalties, stops, users))\n",
    "    random.shuffle(combinations)  # Randomly shuffle to ensure random selection\n",
    "    return combinations\n",
    "\n",
    "def main():\n",
    "    prompt = \"\"\"You are given an integer array nums and two integers indexDiff and valueDiff.\n",
    "\n",
    "Find a pair of indices (i, j) such that:\n",
    "\n",
    "i != j,\n",
    "abs(i - j) <= indexDiff.\n",
    "abs(nums[i] - nums[j]) <= valueDiff, and\n",
    "Return true if such pair exists or false otherwise.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: nums = [1,2,3,1], indexDiff = 3, valueDiff = 0\n",
    "Output: true\n",
    "Explanation: We can choose (i, j) = (0, 3).\n",
    "We satisfy the three conditions:\n",
    "i != j --> 0 != 3\n",
    "abs(i - j) <= indexDiff --> abs(0 - 3) <= 3\n",
    "abs(nums[i] - nums[j]) <= valueDiff --> abs(1 - 1) <= 0\n",
    "Example 2:\n",
    "\n",
    "Input: nums = [1,5,9,1,5,9], indexDiff = 2, valueDiff = 3\n",
    "Output: false\n",
    "Explanation: After trying all the possible pairs (i, j), we cannot satisfy the three conditions, so we return false.\n",
    " \n",
    "\n",
    "Constraints:\n",
    "\n",
    "2 <= nums.length <= 105\n",
    "-109 <= nums[i] <= 109\n",
    "1 <= indexDiff <= nums.length\n",
    "0 <= valueDiff <= 109\"\"\"\n",
    "    total_combinations = 2000  # Total number of unique parameter combinations to generate\n",
    "    n = 500  # number of jobs to use from the generated combinations\n",
    "    file_path = \"1000_batch_test.jsonl\"\n",
    "    endpoint = \"/v1/chat/completions\"\n",
    "    completion_window = \"24h\"\n",
    "\n",
    "    # Define the parameter ranges for the sweep\n",
    "    models = [\"gpt-3.5-turbo\"]\n",
    "    max_tokens_list = [200, 300, 500, 700]\n",
    "    temperatures = [0.5, 0.7, 0.9, 1.3]\n",
    "    top_ps = [0.8, 0.9, 1.0]\n",
    "    frequency_penalties = [0, 0.5, 1.0]\n",
    "    presence_penalties = [0, 0.5, 1.0]\n",
    "    stops = [[\"\\n\"], [\".\", \"?\", \"!\"], None]\n",
    "    users = [\"user_charles\"]\n",
    "\n",
    "    # Generate a large pool of parameter combinations\n",
    "    parameter_combinations = generate_parameter_combinations(models, max_tokens_list, temperatures, top_ps, frequency_penalties, presence_penalties, stops, users)\n",
    "\n",
    "    # Select `n` random combinations from the pool\n",
    "    selected_combinations = random.sample(parameter_combinations, n)\n",
    "    prompts = [prompt] * len(selected_combinations)\n",
    "\n",
    "    # Step 1: Create the file with the selected parameter combinations\n",
    "    create_jsonl_file(prompts, selected_combinations, file_path)\n",
    "    \n",
    "    # Step 2: Upload the file\n",
    "    upload_response = upload_file(file_path, \"batch\")\n",
    "    input_file_id = upload_response['id']\n",
    "    print(f\"File uploaded. File ID: {input_file_id}\")\n",
    "\n",
    "    # Step 3: Create the batch job\n",
    "    batch_response = create_batch(input_file_id, endpoint, completion_window)\n",
    "    batch_id = batch_response['id']\n",
    "    print(f\"Batch job created. Batch ID: {batch_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed34e21e-7b9e-4952-8c44-3621ab2c9c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch statuses:\n",
      "batch_7wAG2AjElDG4RFRGBKqwjyfj:2024-05-25 18:29:06::Finished\n",
      "batch_tXgwb4a0dqRQ1FlMAOAPZTj5:2024-05-25 18:27:44::Fail\n",
      "batch_TYryq4oxmpN6MxoOtMQZRVLO:2024-05-21 17:43:28::Finished\n",
      "batch_QsQq1inCRe7658QDxPfPop1W:2024-05-21 17:32:09::canceled\n",
      "batch_RreSlmceawjRlFaBLSP6Bv5K:2024-05-21 16:56:52::canceled\n",
      "batch_RcRye5IdrkCDNwNiNZO4FWa1:2024-05-21 16:54:17::Fail\n",
      "batch_GBx1zUqapiftq7vgL1VTltPH:2024-05-21 16:52:03::Fail\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     Status All Batch    ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def list_batches(limit=20, after=None):\n",
    "    url = \"https://api.openai.com/v1/batches\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    params = {\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    if after:\n",
    "        params[\"after\"] = after\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def get_status_label(status):\n",
    "    if status == \"completed\":\n",
    "        return \"Finished\"\n",
    "    elif status == \"failed\":\n",
    "        return \"Fail\"\n",
    "    elif status == \"cancelling\":\n",
    "        return \"canceling\"\n",
    "    elif status == \"cancelled\":\n",
    "        return \"canceled\"\n",
    "    else:\n",
    "        return \"In Progress\"\n",
    "\n",
    "def main():\n",
    "    limit = 100  # Adjust the limit as needed\n",
    "    list_response = list_batches(limit=limit)\n",
    "\n",
    "    print(\"Batch statuses:\")\n",
    "    for batch in list_response['data']:\n",
    "        batch_id = batch['id']\n",
    "        created_at = datetime.utcfromtimestamp(batch['created_at']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        status_label = get_status_label(batch['status'])\n",
    "        print(f\"{batch_id}:{created_at}::{status_label}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6fbacf-0319-4e8f-abaf-c1751e28db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch status and information:\n",
      "{\n",
      "  \"id\": \"batch_7wAG2AjElDG4RFRGBKqwjyfj\",\n",
      "  \"object\": \"batch\",\n",
      "  \"endpoint\": \"/v1/chat/completions\",\n",
      "  \"errors\": null,\n",
      "  \"input_file_id\": \"file-2QvkFdNrCoqUU1jZG312gR6h\",\n",
      "  \"completion_window\": \"24h\",\n",
      "  \"status\": \"completed\",\n",
      "  \"output_file_id\": \"file-vMIQ3g6UuaJBLjOUSAb8qgHf\",\n",
      "  \"error_file_id\": null,\n",
      "  \"created_at\": 1716661746,\n",
      "  \"in_progress_at\": 1716661746,\n",
      "  \"expires_at\": 1716748146,\n",
      "  \"finalizing_at\": 1716668654,\n",
      "  \"completed_at\": 1716668737,\n",
      "  \"failed_at\": null,\n",
      "  \"expired_at\": null,\n",
      "  \"cancelling_at\": null,\n",
      "  \"cancelled_at\": null,\n",
      "  \"request_counts\": {\n",
      "    \"total\": 500,\n",
      "    \"completed\": 500,\n",
      "    \"failed\": 0\n",
      "  },\n",
      "  \"metadata\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     Status 1 BATCH     ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def retrieve_batch(batch_id):\n",
    "    url = f\"https://api.openai.com/v1/batches/{batch_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def main():\n",
    "    batch_id = \"batch_7wAG2AjElDG4RFRGBKqwjyfj\"\n",
    "\n",
    "    # Retrieve the job status and info\n",
    "    batch_status = retrieve_batch(batch_id)\n",
    "    print(\"Batch status and information:\")\n",
    "    print(json.dumps(batch_status, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33bb370c-fd77-4cef-83eb-a796de7ab2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancel batch response:\n",
      "{'id': 'batch_RreSlmceawjRlFaBLSP6Bv5K', 'object': 'batch', 'endpoint': '/v1/chat/completions', 'errors': None, 'input_file_id': 'file-9jyr72r96jaxWQCY1TN3vGNU', 'completion_window': '24h', 'status': 'cancelling', 'output_file_id': None, 'error_file_id': None, 'created_at': 1716310612, 'in_progress_at': 1716310613, 'expires_at': 1716397012, 'finalizing_at': None, 'completed_at': None, 'failed_at': None, 'expired_at': None, 'cancelling_at': 1716316970, 'cancelled_at': None, 'request_counts': {'total': 10, 'completed': 0, 'failed': 0}, 'metadata': None}\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     Cancel BATCH     ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import requests\n",
    "\n",
    "def cancel_batch(batch_id):\n",
    "    url = f\"https://api.openai.com/v1/batches/{batch_id}/cancel\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    response = requests.post(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def main():\n",
    "    batch_id = \"batch_RreSlmceawjRlFaBLSP6Bv5K\"  # replace with the actual batch ID\n",
    "\n",
    "    # Cancel the batch\n",
    "    cancel_response = cancel_batch(batch_id)\n",
    "    print(\"Cancel batch response:\")\n",
    "    print(cancel_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24af460d-d9e3-4c14-9f85-b684184498b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"batch_TYryq4oxmpN6MxoOtMQZRVLO\",\n",
      "  \"object\": \"batch\",\n",
      "  \"endpoint\": \"/v1/chat/completions\",\n",
      "  \"errors\": null,\n",
      "  \"input_file_id\": \"file-Px7HiajPuZtpdeFUXCpDcYaL\",\n",
      "  \"completion_window\": \"24h\",\n",
      "  \"status\": \"completed\",\n",
      "  \"output_file_id\": \"file-JbSzi7Kvb0P1XUcyBLCu6emV\",\n",
      "  \"error_file_id\": null,\n",
      "  \"created_at\": 1716313408,\n",
      "  \"in_progress_at\": 1716313409,\n",
      "  \"expires_at\": 1716399808,\n",
      "  \"finalizing_at\": 1716341796,\n",
      "  \"completed_at\": 1716341827,\n",
      "  \"failed_at\": null,\n",
      "  \"expired_at\": null,\n",
      "  \"cancelling_at\": null,\n",
      "  \"cancelled_at\": null,\n",
      "  \"request_counts\": {\n",
      "    \"total\": 200,\n",
      "    \"completed\": 200,\n",
      "    \"failed\": 0\n",
      "  },\n",
      "  \"metadata\": null\n",
      "}\n",
      "Batch status saved to batch_status.json\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "##     Download BATCH     ##\n",
    "\n",
    "##############################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def retrieve_batch(batch_id):\n",
    "    url = f\"https://api.openai.com/v1/batches/{batch_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def save_to_file(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def main():\n",
    "    batch_id = \"batch_TYryq4oxmpN6MxoOtMQZRVLO\"  # replace with the actual batch ID\n",
    "    output_file = \"batch_status.json\"\n",
    "\n",
    "    # Step 4: Retrieve the job\n",
    "    final_status = retrieve_batch(batch_id)\n",
    "    print(json.dumps(final_status, indent=2))\n",
    "\n",
    "    # Save the output to a file\n",
    "    save_to_file(final_status, output_file)\n",
    "    print(f\"Batch status saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc25882c-be72-47ee-94ee-eacfd2b396b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch info:\n",
      "{\n",
      "  \"id\": \"batch_7wAG2AjElDG4RFRGBKqwjyfj\",\n",
      "  \"object\": \"batch\",\n",
      "  \"endpoint\": \"/v1/chat/completions\",\n",
      "  \"errors\": null,\n",
      "  \"input_file_id\": \"file-2QvkFdNrCoqUU1jZG312gR6h\",\n",
      "  \"completion_window\": \"24h\",\n",
      "  \"status\": \"completed\",\n",
      "  \"output_file_id\": \"file-vMIQ3g6UuaJBLjOUSAb8qgHf\",\n",
      "  \"error_file_id\": null,\n",
      "  \"created_at\": 1716661746,\n",
      "  \"in_progress_at\": 1716661746,\n",
      "  \"expires_at\": 1716748146,\n",
      "  \"finalizing_at\": 1716668654,\n",
      "  \"completed_at\": 1716668737,\n",
      "  \"failed_at\": null,\n",
      "  \"expired_at\": null,\n",
      "  \"cancelling_at\": null,\n",
      "  \"cancelled_at\": null,\n",
      "  \"request_counts\": {\n",
      "    \"total\": 500,\n",
      "    \"completed\": 500,\n",
      "    \"failed\": 0\n",
      "  },\n",
      "  \"metadata\": null\n",
      "}\n",
      "Batch result saved to batch_result.jsonl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def retrieve_batch(batch_id):\n",
    "    url = f\"https://api.openai.com/v1/batches/{batch_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def download_file(file_id, filename):\n",
    "    url = f\"https://api.openai.com/v1/files/{file_id}/content\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "def main():\n",
    "    batch_id = \"batch_7wAG2AjElDG4RFRGBKqwjyfj\"  # replace with the actual batch ID\n",
    "    output_file = \"batch_result.jsonl\"  # file to save the output\n",
    "\n",
    "    # Retrieve the batch status and info\n",
    "    batch_info = retrieve_batch(batch_id)\n",
    "    print(\"Batch info:\")\n",
    "    print(json.dumps(batch_info, indent=2))\n",
    "\n",
    "    # Check if the batch is complete and has an output file\n",
    "    if batch_info['status'] == 'completed' and 'output_file_id' in batch_info:\n",
    "        output_file_id = batch_info['output_file_id']\n",
    "        # Download the output file\n",
    "        download_file(output_file_id, output_file)\n",
    "        print(f\"Batch result saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"Batch is not complete or no output file available.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b40bd-b6af-4a73-a9cb-97ddfef7669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n",
    "model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct').to(device)\n",
    "\n",
    "def single_layer_pool(hidden_states, layer_index):\n",
    "    \"\"\"\n",
    "    Extract embeddings from a specific layer.\n",
    "    \"\"\"\n",
    "    selected_layer = hidden_states[layer_index]\n",
    "    return selected_layer.mean(dim=1)  # Average over the token dimension to get a single vector per sequence\n",
    "\n",
    "def embed_text(input_texts, max_length=500, layer_index=-1, scaling_factor=1.0):\n",
    "    try:\n",
    "        # Tokenize the input texts with truncation and without padding\n",
    "        batch_dict = tokenizer(input_texts, max_length=max_length, padding=False, truncation=True, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Check the length of the tokenized input and ignore if it's less than 100\n",
    "        token_length = batch_dict['input_ids'].shape[1]\n",
    "        if token_length < 100:\n",
    "            return None\n",
    "        \n",
    "        # Set output_hidden_states to True\n",
    "        outputs = model(**batch_dict, output_hidden_states=True)\n",
    "        \n",
    "        # Use the new pooling method with specified layer\n",
    "        embeddings = single_layer_pool(outputs.hidden_states, layer_index)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Scale embeddings\n",
    "        embeddings = embeddings * scaling_factor\n",
    "        \n",
    "        # Convert the embeddings to lists\n",
    "        embedding_lists = embeddings.cpu().tolist()\n",
    "        \n",
    "        return embedding_lists\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            del batch_dict\n",
    "            del outputs\n",
    "            del embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "max_input_text_len = 500  # Set a specific maximum length for truncation\n",
    "min_input_text_len = 100  # Minimum length threshold\n",
    "layer_index = -1  # Use the last layer by default\n",
    "scaling_factor = 1.0  # Scaling factor to amplify the embedding values\n",
    "\n",
    "file_path = './breakingchat.txt'\n",
    "user_list, chatgpt_list = parse_conversations(file_path)\n",
    "\n",
    "print(\"Analyzing User Texts\")\n",
    "embedding_user_list = []\n",
    "for text in user_list:\n",
    "    embeddings = embed_text(text, max_length=max_input_text_len, layer_index=layer_index, scaling_factor=scaling_factor)\n",
    "    if embeddings is not None:\n",
    "        embedding_user_list.append(embeddings)\n",
    "\n",
    "print(\"Analyzing ChatGPT Texts\")\n",
    "embedding_chatgpt_list = []\n",
    "for text in chatgpt_list:\n",
    "    embeddings = embed_text(text, max_length=max_input_text_len, layer_index=layer_index, scaling_factor=scaling_factor)\n",
    "    if embeddings is not None:\n",
    "        embedding_chatgpt_list.append(embeddings)\n",
    "\n",
    "# Convert lists of lists to numpy arrays and reshape to remove the extra dimension\n",
    "embedding_user_array = np.squeeze(np.array(embedding_user_list), axis=1)\n",
    "embedding_chatgpt_array = np.squeeze(np.array(embedding_chatgpt_list), axis=1)\n",
    "\n",
    "# Calculate variances\n",
    "user_variance = np.var(embedding_user_array, axis=0).mean()\n",
    "chatgpt_variance = np.var(embedding_chatgpt_array, axis=0).mean()\n",
    "\n",
    "print(f\"Mean Variance in User Embeddings: {user_variance}\")\n",
    "print(f\"Mean Variance in ChatGPT Embeddings: {chatgpt_variance}\")\n",
    "\n",
    "# Assume the following functions and imports are provided\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_autoencoder(input_data, latent_dim, num_epochs):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    \n",
    "    # Encoder configuration\n",
    "    encoded = tf.keras.layers.Dense(1024, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
    "    encoded = tf.keras.layers.LeakyReLU()(encoded)\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dropout(0.3)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
    "    encoded = tf.keras.layers.LeakyReLU()(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    # Decoder configuration\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.BatchNormalization()(decoded)\n",
    "    decoded = tf.keras.layers.LeakyReLU()(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dropout(0.3)(decoded)\n",
    "    decoded = tf.keras.layers.Dense(1024, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.BatchNormalization()(decoded)\n",
    "    decoded = tf.keras.layers.LeakyReLU()(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    # Autoencoder connecting encoder and decoder\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    # Move the model to GPU if available\n",
    "    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "        # Train the autoencoder\n",
    "        autoencoder.fit(input_data, input_data, epochs=num_epochs, batch_size=32, verbose=1)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    # Applying Gaussian filter to smooth the grids\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=2) for grid in encoded_2d_grid])\n",
    "\n",
    "    # Setting up the figure and 3D axis\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Creating a meshgrid for the x and y coordinates\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    \n",
    "    # Plotting each grid with an increasing offset in Z\n",
    "    offset = 0.0\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i] + i * offset  # Offset each grid\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    # Setting labels and title\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Free up GPU memory by deleting the model\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train the autoencoder and visualize embeddings\n",
    "grid_size = 50\n",
    "latent_dim = grid_size * grid_size\n",
    "num_epochs = 500\n",
    "\n",
    "combined_input = embedding_chatgpt_array\n",
    "encoder = train_autoencoder(combined_input, latent_dim, num_epochs)\n",
    "\n",
    "# Predict using the trained encoder\n",
    "encoded_2d_grid = encoder.predict(combined_input).reshape(-1, grid_size, grid_size)\n",
    "print(\"Variance in Encoded Outputs for ChatGPT Data:\", np.var(encoded_2d_grid, axis=0).mean())\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "# Now predict using user data\n",
    "combined_input = embedding_user_array\n",
    "encoded_2d_grid = encoder.predict(combined_input).reshape(-1, grid_size, grid_size)\n",
    "print(\"Variance in Encoded Outputs for User Data:\", np.var(encoded_2d_grid, axis=0).mean())\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f761a2-994b-44eb-9190-fe636f5675bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_user_list = []\n",
    "embedding_chatgpt_list = []\n",
    "max_input_text_len = 1000\n",
    "for text in user_list:\n",
    "    embedding_user_list.append(embed_text(text[0:max_input_text_len]))\n",
    "for text in chatgpt_list:\n",
    "    embedding_chatgpt_list.append(embed_text(text[0:max_input_text_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad085c7-0a80-4db2-b974-f58c787bc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    return embeddings\n",
    "# Note: You might need to limit the number of texts processed at once if the list is very large\n",
    "embedding_user_list = get_embeddings(user_list[:max_input_text_len])\n",
    "embedding_chatgpt_list = get_embeddings(chatgpt_list[:max_input_text_len])\n",
    "import numpy as np\n",
    "\n",
    "# Calculate variance for each set of embeddin gs\n",
    "user_variance = np.var(embedding_user_list, axis=0).mean()\n",
    "chatgpt_variance = np.var(embedding_chatgpt_list, axis=0).mean()\n",
    "\n",
    "print(f\"Mean Variance in User Embeddings: {user_variance}\")\n",
    "print(f\"Mean Variance in ChatGPT Embeddings: {chatgpt_variance}\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "# Assuming `user_list` and `chatgpt_list` contain your texts\n",
    "embedding_user_array = get_embeddings(user_list[:max_input_text_len])\n",
    "embedding_chatgpt_array = get_embeddings(chatgpt_list[:max_input_text_len])\n",
    "\n",
    "# Ensure both arrays are of the same shape\n",
    "if embedding_user_array.shape[1] != embedding_chatgpt_array.shape[1]:\n",
    "    raise ValueError(\"Embedding dimensions do not match and cannot be concatenated.\")\n",
    "\n",
    "# Here we use only ChatGPT array for training as an example\n",
    "combined_input = embedding_chatgpt_array\n",
    "\n",
    "grid_size = 30\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Train the autoencoder\n",
    "encoder = train_autoencoder(combined_input, latent_dim, 30)\n",
    "\n",
    "# Predict using the trained encoder\n",
    "encoded_2d_grid = encoder.predict(combined_input).reshape(-1, grid_size, grid_size)\n",
    "print(\"Variance in Encoded Outputs for ChatGPT Data:\", np.var(encoded_2d_grid, axis=0).mean())\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "# Now predict using user data\n",
    "combined_input = embedding_user_array\n",
    "encoded_2d_grid = encoder.predict(combined_input).reshape(-1, grid_size, grid_size)\n",
    "print(\"Variance in Encoded Outputs for User Data:\", np.var(encoded_2d_grid, axis=0).mean())\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddce106-3920-4209-ba65-c8a68ac8cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def single_layer_pool(hidden_states, layer_index):\n",
    "    \"\"\"\n",
    "    Extract embeddings from a specific layer.\n",
    "    \"\"\"\n",
    "    selected_layer = hidden_states[layer_index]\n",
    "    return selected_layer.mean(dim=1)  # Average over the token dimension to get a single vector per sequence\n",
    "\n",
    "def embed_text(input_texts, max_length=500, layer_index=-1, scaling_factor=1.0):\n",
    "    try:\n",
    "        # Tokenize the input texts with truncation and without padding\n",
    "        batch_dict = tokenizer(input_texts, max_length=max_length, padding=False, truncation=True, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Check the length of the tokenized input and ignore if it's less than 100\n",
    "        token_length = batch_dict['input_ids'].shape[1]\n",
    "        if token_length < 100:\n",
    "            return None\n",
    "        \n",
    "        # Set output_hidden_states to True\n",
    "        outputs = model(**batch_dict, output_hidden_states=True)\n",
    "        \n",
    "        # Use the new pooling method with specified layer\n",
    "        embeddings = single_layer_pool(outputs.hidden_states, layer_index)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Scale embeddings\n",
    "        embeddings = embeddings * scaling_factor\n",
    "        \n",
    "        # Convert the embeddings to lists\n",
    "        embedding_lists = embeddings.cpu().tolist()\n",
    "        \n",
    "        return embedding_lists\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            del batch_dict\n",
    "            del outputs\n",
    "            del embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "max_input_text_len = 500  # Set a specific maximum length for truncation\n",
    "min_input_text_len = 100  # Minimum length threshold\n",
    "layer_index = -1  # Use the last layer by default\n",
    "scaling_factor = 1000.0  # Scaling factor to amplify the embedding values\n",
    "\n",
    "print(\"Analyzing User Texts\")\n",
    "embedding_user_list = []\n",
    "for text in user_list:\n",
    "    embeddings = embed_text(text, max_length=max_input_text_len, layer_index=layer_index, scaling_factor=scaling_factor)\n",
    "    if embeddings is not None:\n",
    "        embedding_user_list.append(embeddings)\n",
    "\n",
    "print(\"Analyzing ChatGPT Texts\")\n",
    "embedding_chatgpt_list = []\n",
    "for text in chatgpt_list:\n",
    "    embeddings = embed_text(text, max_length=max_input_text_len, layer_index=layer_index, scaling_factor=scaling_factor)\n",
    "    if embeddings is not None:\n",
    "        embedding_chatgpt_list.append(embeddings)\n",
    "\n",
    "# Convert lists of lists to numpy arrays\n",
    "embedding_user_array = np.array(embedding_user_list)\n",
    "embedding_chatgpt_array = np.array(embedding_chatgpt_list)\n",
    "\n",
    "# Calculate variances\n",
    "user_variance = np.var(embedding_user_array, axis=0).mean()\n",
    "chatgpt_variance = np.var(embedding_chatgpt_array, axis=0).mean()\n",
    "\n",
    "print(f\"Mean Variance in User Embeddings: {user_variance}\")\n",
    "print(f\"Mean Variance in ChatGPT Embeddings: {chatgpt_variance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825bdaf7-7f23-4002-a661-f02a353ef633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=1000, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=10, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "def extract_token_probs(data):\n",
    "    # Initialize an empty list to store the token probabilities\n",
    "    token_probs_list = []\n",
    "    \n",
    "    # Iterate over each entry in the data list\n",
    "    for entry in data:\n",
    "        # Check if 'token_probs' key exists in the dictionary\n",
    "        if 'token_probs' in entry:\n",
    "            # Append the numpy array of token probabilities to the list\n",
    "            token_probs_list.append(entry['token_probs'])\n",
    "    \n",
    "    # Return the list of token probabilities\n",
    "    return token_probs_list\n",
    "    \n",
    "def pad_token_probs(probabilities, pad_value=0):\n",
    "    # Find the maximum length of the token probabilities list\n",
    "    max_length = max(len(probs) for probs in probabilities)\n",
    "    # Pad each probabilities list to the maximum length\n",
    "    padded_probabilities = np.array([np.pad(probs, (0, max_length - len(probs)), 'constant', constant_values=pad_value) for probs in probabilities])\n",
    "    return padded_probabilities\n",
    "\n",
    "\n",
    "prompt = \"I want a script that generates an analysis of some text data. Pull the text data from any source of text it doesn't matter what.\"\n",
    "n = 10\n",
    "responses, log_probs = get_responses(prompt, n)\n",
    "embeddings = get_embeddings(responses)\n",
    "\n",
    "probability_map = analyze_log_probs(log_probs)\n",
    "token_probs_list = extract_token_probs(probability_map)\n",
    "\n",
    "token_probabilities = pad_token_probs([entry['token_probs'] for entry in probability_map])\n",
    "\n",
    "# Combine embeddings and token probabilities\n",
    "combined_input = np.hstack((embeddings, token_probabilities))\n",
    "\n",
    "grid_size = 30\n",
    "latent_dim = grid_size * grid_size\n",
    "encoder = train_autoencoder(combined_input, latent_dim)\n",
    "\n",
    "encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093deb9e-9491-4c15-8d48-2ae831e7a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad the token probabilities to the same length\n",
    "def pad_token_probs(probabilities, pad_value=0):\n",
    "    # Find the maximum length of the token probabilities list\n",
    "    max_length = max(len(probs) for probs in probabilities)\n",
    "    # Pad each probabilities list to the maximum length\n",
    "    padded_probabilities = np.array([np.pad(probs, (0, max_length - len(probs)), 'constant', constant_values=pad_value) for probs in probabilities])\n",
    "    return padded_probabilities\n",
    "\n",
    "# Usage in your main workflow\n",
    "token_probabilities = pad_token_probs([entry['token_probs'] for entry in probability_map])\n",
    "\n",
    "combined_input = np.hstack((embeddings, token_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b3087-e335-4c31-8041-8d62202260bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=50, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=5) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "grid_size = 100\n",
    "latent_dim = grid_size * grid_size\n",
    "encoder = train_autoencoder(combined_input, latent_dim)\n",
    "encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf38c31-c8f0-489b-a621-46af1135ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=30, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    # Applying Gaussian filter to smooth the grids\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "    # Setting up the figure and 3D axis\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Creating a meshgrid for the x and y coordinates\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    \n",
    "    # Plotting each grid with an increasing offset in Z\n",
    "    offset = 0.5\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i] + i * offset  # Offset each grid\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    # Setting labels and title\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "grid_size = 30\n",
    "latent_dim = grid_size * grid_size\n",
    "encoder = train_autoencoder(combined_input, latent_dim)\n",
    "encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "visualize_2d_grid(encoded_2d_grid, grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a79e1-10e2-4efb-ba53-bf96679356ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Set output_hidden_states to True\n",
    "outputs = model(**batch_dict, output_hidden_states=True)\n",
    "# Use the new pooling method\n",
    "embeddings = multi_layer_pool(outputs.hidden_states, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Convert the embeddings to lists\n",
    "embedding_lists = embeddings.cpu().tolist()\n",
    "for i, embedding in enumerate(embedding_lists):\n",
    "    print(f'Embedding for input {i+1}: {embedding}')\n",
    "\n",
    "# To print the similarity scores\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(\"Similarity scores:\", scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba15aee8-d7c8-46c5-9f7c-86d749800f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "# Each query must come with a one-sentence instruction that describes the task\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "queries = [\n",
    "    get_detailed_instruct(task, 'how much protein should a female eat'),\n",
    "    get_detailed_instruct(task, 'summit define')\n",
    "]\n",
    "# No need to add instruction for retrieval documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n",
    "]\n",
    "input_texts = queries + documents\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n",
    "model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc899c-6a9f-41c6-88ff-cc60defcee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_pool(hidden_states: list, attention_mask: Tensor, num_layers: int = 1) -> Tensor:\n",
    "    \"\"\"\n",
    "    Extract embeddings from the last 'num_layers' layers and concatenate them.\n",
    "    \"\"\"\n",
    "    # Get the last num_layers layers\n",
    "    all_layers = hidden_states[-num_layers:]  \n",
    "    # Concatenate them on the embedding dimension\n",
    "    concatenated_layers = torch.cat(all_layers, dim=-1)  \n",
    "\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return concatenated_layers[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = concatenated_layers.shape[0]\n",
    "        return concatenated_layers[torch.arange(batch_size, device=concatenated_layers.device), sequence_lengths]\n",
    "\n",
    "max_length = 4096\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Set output_hidden_states to True\n",
    "outputs = model(**batch_dict, output_hidden_states=True)\n",
    "# Use the new pooling method\n",
    "embeddings = multi_layer_pool(outputs.hidden_states, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Convert the embeddings to lists\n",
    "embedding_lists = embeddings.cpu().tolist()\n",
    "\n",
    "print(len(embedding_lists[0]))\n",
    "print(embedding_lists[0][0:100])\n",
    "\n",
    "# To print the similarity scores\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(\"Similarity scores:\", scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7939c53-084c-4ccb-a858-ca34c4a0b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "def get_full_response(prompt, n=1, max_tokens=1000, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  # Replace with your model ID as needed\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True,  # Enable logprobs\n",
    "            top_logprobs=10  # Specify number of top log probabilities to return\n",
    "        )\n",
    "\n",
    "        # Print the entire response object\n",
    "        return response\n",
    "\n",
    "# Example usage\n",
    "response = get_full_response(\"Tell me a joke\", n=1)\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_response_info(response):\n",
    "    # Extract the conversation response\n",
    "    response_match = re.search(r'content=\"(.*?)\"', str(response), re.DOTALL)\n",
    "    response_text = response_match.group(1) if response_match else \"\"\n",
    "\n",
    "    # Extract the logprobs of the response\n",
    "    logprobs_match = re.findall(r'logprob=(-?\\d+\\.\\d+)', str(response))\n",
    "    logprobs = [float(logprob) for logprob in logprobs_match]\n",
    "\n",
    "    return response_text, logprobs\n",
    "    \n",
    "reply, logprobs= extract_response_info(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95aec0c-f714-441c-bf0e-6db95ec12f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chat_completion_data(response):\n",
    "    # Data structure to hold the results\n",
    "    data = {\n",
    "        \"Response Content\": \"\",\n",
    "        \"Logprobs\": [],\n",
    "        \"Top Logprob Words\": [],\n",
    "        \"Top Logprob Values\": []\n",
    "    }\n",
    "    \n",
    "    # Assume the first choice for simplification; adapt as needed for multiple choices\n",
    "    if response.choices:\n",
    "        choice = response.choices[0]\n",
    "        data[\"Response Content\"] = choice.message.content\n",
    "        \n",
    "        # Extract token logprob information\n",
    "        for token_logprob in choice.logprobs.content:\n",
    "            # Append the logprob of the current token to the list\n",
    "            data[\"Logprobs\"].append(token_logprob.logprob)\n",
    "            \n",
    "            # For collecting top logprob words and their values\n",
    "            top_words = []\n",
    "            top_values = []\n",
    "            \n",
    "            # Extract top logprob details\n",
    "            for top_logprob in token_logprob.top_logprobs:\n",
    "                top_words.append(top_logprob.token)\n",
    "                top_values.append(top_logprob.logprob)\n",
    "            \n",
    "            # Append each token's top logprob words and values\n",
    "            data[\"Top Logprob Words\"].append(top_words)\n",
    "            data[\"Top Logprob Values\"].append(top_values)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a response object from the OpenAI API\n",
    "# print(extract_chat_completion_data(response))\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a response object from the OpenAI API\n",
    "print(extract_chat_completion_data(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33261ab-5276-4b73-944a-e9e19416773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(response))\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cb8fa-310c-4a0f-9cce-b835d9a2d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conversations(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    conversations = content.split('\\n\\n')\n",
    "    user_messages = []\n",
    "    chatgpt_messages = []\n",
    "\n",
    "    current_label = None\n",
    "    current_message = []\n",
    "\n",
    "    for conversation in conversations:\n",
    "        lines = conversation.strip().split('\\n')\n",
    "        if len(lines) >= 1:\n",
    "            if lines[0] == '#USER':\n",
    "                if current_label == '##ChatGPT':\n",
    "                    chatgpt_messages.append(' '.join(current_message))\n",
    "                current_label = '#USER'\n",
    "                current_message = lines[1:]\n",
    "            elif lines[0] == '##ChatGPT':\n",
    "                if current_label == '#USER':\n",
    "                    user_messages.append(' '.join(current_message))\n",
    "                current_label = '##ChatGPT'\n",
    "                current_message = lines[1:]\n",
    "            else:\n",
    "                current_message.extend(lines)\n",
    "\n",
    "    if current_label == '#USER':\n",
    "        user_messages.append(' '.join(current_message))\n",
    "    elif current_label == '##ChatGPT':\n",
    "        chatgpt_messages.append(' '.join(current_message))\n",
    "\n",
    "    return user_messages, chatgpt_messages\n",
    "\n",
    "# Example usage\n",
    "file_path = './breakingchat.txt'\n",
    "user_list, chatgpt_list = parse_conversations(file_path)\n",
    "\n",
    "\"\"\"\n",
    "print(\"User Messages:\")\n",
    "for message in user_list:\n",
    "    print(message)\n",
    "    print()\n",
    "\n",
    "print(\"ChatGPT Messages:\")\n",
    "for message in chatgpt_list:\n",
    "    print(message)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8eda05-7d2d-45ae-a5a8-3c04c69eb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91f16e-7de6-4886-b912-f06499a700db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "        token_probs = softmax(token_logprobs)\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs\n",
    "        })\n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "    autoencoder.fit(input_data, input_data, epochs=20, batch_size=8, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 20\n",
    "    responses, log_probs = get_responses(prompt, n)\n",
    "    embeddings = get_embeddings(responses)\n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    token_probs_list = np.array([entry['token_probs'] for entry in probability_map])\n",
    "    combined_input = np.hstack((embeddings, token_probs_list))\n",
    "    grid_size = 30\n",
    "    latent_dim = grid_size * grid_size\n",
    "    encoder = train_autoencoder(combined_input, latent_dim)\n",
    "    encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "    visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13554d8c-06f4-4b9a-80ba-c5ed5ca5dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Prepare the Text\n",
    "sample_text = \"Natural language processing (NLP) involves the interaction between computers and humans through language. It enables machines to read, understand, and derive meaning from human languages.\"\n",
    "\n",
    "# Step 2: Vectorize the Text using Universal Sentence Encoder from TensorFlow Hub\n",
    "# Load Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "# Convert the sample text into an embedding\n",
    "text_embedding = embedding_layer([sample_text])\n",
    "print(\"Original Embedding Shape:\", text_embedding.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder Model to reduce dimensionality to a 2D array\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embedding.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(32, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(2 * 2, activation='linear')(encoded)  # 2x2 array output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(2 * 2,))\n",
    "decoded = tf.keras.layers.Dense(32, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(64, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embedding.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "# Here, we use the text embedding itself as the target (autoencoder learns to reconstruct its input)\n",
    "autoencoder.fit(text_embedding, text_embedding, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "# Predict the 2D coordinates\n",
    "encoded_2d_array = encoder(text_embedding)\n",
    "print(\"2D Array for the Input Text:\", encoded_2d_array.numpy().reshape(2, 2))\n",
    "\n",
    "# Step 4: Visualize the 2D Array Result\n",
    "encoded_2d_array_reshaped = encoded_2d_array.numpy().reshape(2, 2)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(encoded_2d_array_reshaped, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('2D Array Embedding of the Text')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e2dcd-9cc3-4b69-b51c-a06be4fcacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(encoded_2d_grid.shape[0]):\n",
    "    Z = encoded_2d_grid[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "\n",
    "ax.set_title('3D Visualization of Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca1aa2-814f-4937-b562-8baab5e7c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc7ccd-fb40-4d5b-9c3e-997014f238b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:1000]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 30  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dropout(0.5)(encoded)  # Adding dropout\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56558db2-81c1-485f-8b58-e3dcbcd7616f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33c2f9-0276-42d9-b09d-47817c28d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Step 1: Prepare Multiple Text Samples\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "texts = newsgroups.data[:10]  # Limit to 1000 samples for this example\n",
    "\n",
    "# Step 2: Convert Text Samples into Embeddings using Universal Sentence Encoder\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "text_embeddings = embedding_layer(texts)\n",
    "print(\"Original Embedding Shape:\", text_embeddings.shape)\n",
    "\n",
    "# Step 3: Build and Train an Autoencoder to reduce dimensionality to a large 2D grid\n",
    "grid_size = 10  # 30x30 grid\n",
    "latent_dim = grid_size * grid_size\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_input = tf.keras.Input(shape=(text_embeddings.shape[-1],))\n",
    "encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)  # Larger 2D grid output\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "decoded_output = tf.keras.layers.Dense(text_embeddings.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "# Combine encoder and decoder into an autoencoder model\n",
    "autoencoder_input = encoder_input\n",
    "encoded_embedding = encoder(autoencoder_input)\n",
    "decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(text_embeddings, text_embeddings, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "# Step 4: Visualize the 2D Grid in 3D with Smoothing\n",
    "encoded_2d_grid = encoder(text_embeddings).numpy().reshape(-1, grid_size, grid_size)\n",
    "\n",
    "# Apply Gaussian smoothing to each sample's 2D grid\n",
    "smoothed_grids = np.array([gaussian_filter(grid, sigma=5) for grid in encoded_2d_grid])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "for i in range(smoothed_grids.shape[0]):\n",
    "    Z = smoothed_grids[i]\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Embedding Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557eeaf-cf81-4563-bf13-82b9e9eded62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai sentence-transformers scikit-learn numpy matplotlib hdbscan umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb824ca-eb85-425a-b975-0a4686dfe97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to cluster embeddings (HDBSCAN or K-Means)\n",
    "def cluster_embeddings(embeddings, method='hdbscan'):\n",
    "    if method == 'hdbscan':\n",
    "        clusterer = HDBSCAN(min_cluster_size=15, cluster_selection_method='leaf')\n",
    "        cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    else:\n",
    "        clusterer = KMeans(n_clusters=5)\n",
    "        cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    return cluster_labels\n",
    "\n",
    "# Function to visualize embeddings using UMAP\n",
    "def visualize_embeddings(embeddings, labels):\n",
    "    n_neighbors = min(15, len(embeddings) - 1)\n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=n_neighbors)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='Spectral')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def plot_raw_logits(probability_map):\n",
    "    for entry in probability_map:\n",
    "        tokens = entry['tokens']\n",
    "        log_probs = entry['token_logprobs']  # Assuming this key will be added to the dictionary\n",
    "        \n",
    "        # Ensure we have valid data\n",
    "        if tokens and log_probs and len(tokens) == len(log_probs):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(tokens, log_probs)\n",
    "            plt.xlabel('Tokens')\n",
    "            plt.ylabel('Log Probabilities')\n",
    "            plt.title('Raw Log Probabilities of Tokens')\n",
    "            plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "def plot_probabilities(probability_map):\n",
    "    for entry in probability_map:\n",
    "        tokens = entry['tokens']\n",
    "        token_probs = entry['token_probs']\n",
    "        \n",
    "        # Ensure we have valid data\n",
    "        if tokens and token_probs is not None and len(tokens) == len(token_probs):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(tokens, token_probs)\n",
    "            plt.xlabel('Tokens')\n",
    "            plt.ylabel('Probabilities')\n",
    "            plt.title('Token Probabilities')\n",
    "            plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 10\n",
    "    \n",
    "    responses, log_probs = get_responses(\n",
    "    prompt=prompt,\n",
    "    n=5,\n",
    "    max_tokens=100,\n",
    "    temperature=2,\n",
    "    top_p=0.3,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.3\n",
    "    )\n",
    "\n",
    "    embeddings = get_embeddings(responses)\n",
    "    \n",
    "    # Choose clustering method ('hdbscan' or 'kmeans')\n",
    "    clustering_method = 'hdbscan'\n",
    "    cluster_labels = cluster_embeddings(embeddings, method=clustering_method)\n",
    "    \n",
    "    #print(\"Clustering Results:\")\n",
    "    #for i, response in enumerate(responses):\n",
    "        #print(f\"Cluster {cluster_labels[i]}: {response}\")\n",
    "    \n",
    "    visualize_embeddings(embeddings, cluster_labels)\n",
    "\n",
    "    print(log_probs)\n",
    "    \n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    print(probability_map)\n",
    "    plot_probabilities(probability_map)\n",
    "    plot_raw_logits(probability_map)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22516b0f-490e-430d-bbbe-280df6483956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=\"sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP\")\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# Function to get responses from GPT-4o\n",
    "def get_responses(prompt, n, max_tokens=100, temperature=0.7, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    responses = []\n",
    "    log_probs = []\n",
    "    for _ in range(n):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            logprobs=True\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "        log_probs.append(response.choices[0].logprobs)\n",
    "    return responses, log_probs\n",
    "\n",
    "# Function to get embeddings for the responses\n",
    "def get_embeddings(responses):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = model.encode(responses)\n",
    "    return embeddings\n",
    "\n",
    "# Function to convert log probabilities to probabilities using softmax\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def analyze_log_probs(log_probs):\n",
    "    probability_map = []\n",
    "    for choice in log_probs:\n",
    "        # Access the 'content' attribute which is the list of 'ChatCompletionTokenLogprob'\n",
    "        tokens = [token_logprob.token for token_logprob in choice.content]\n",
    "        token_logprobs = [token_logprob.logprob for token_logprob in choice.content]\n",
    "\n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        token_probs = softmax(token_logprobs)\n",
    "\n",
    "        probability_map.append({\n",
    "            'tokens': tokens,\n",
    "            'token_probs': token_probs,\n",
    "            'token_logprobs': token_logprobs  # Store raw log probabilities for plotting\n",
    "        })\n",
    "    \n",
    "    return probability_map\n",
    "\n",
    "# Function to train an autoencoder\n",
    "def train_autoencoder(input_data, latent_dim):\n",
    "    encoder_input = tf.keras.Input(shape=(input_data.shape[-1],))\n",
    "    encoded = tf.keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "    encoded = tf.keras.layers.Dropout(0.5)(encoded)\n",
    "    encoded = tf.keras.layers.Dense(256, activation='relu')(encoded)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    encoded_output = tf.keras.layers.Dense(latent_dim, activation='linear')(encoded)\n",
    "\n",
    "    encoder = tf.keras.Model(encoder_input, encoded_output, name='encoder')\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim,))\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "    decoded = tf.keras.layers.Dense(256, activation='relu')(decoded)\n",
    "    decoded = tf.keras.layers.Dense(512, activation='relu')(decoded)\n",
    "    decoded_output = tf.keras.layers.Dense(input_data.shape[-1], activation='sigmoid')(decoded)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoded_output, name='decoder')\n",
    "\n",
    "    autoencoder_input = encoder_input\n",
    "    encoded_embedding = encoder(autoencoder_input)\n",
    "    decoded_embedding = decoder(encoded_embedding)\n",
    "\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, decoded_embedding, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    autoencoder.fit(input_data, input_data, epochs=10, batch_size=16, verbose=1)\n",
    "    return encoder\n",
    "\n",
    "# Function to visualize 2D grid in 3D with smoothing\n",
    "def visualize_2d_grid(encoded_2d_grid, grid_size):\n",
    "    smoothed_grids = np.array([gaussian_filter(grid, sigma=1) for grid in encoded_2d_grid])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(grid_size), range(grid_size))\n",
    "    for i in range(smoothed_grids.shape[0]):\n",
    "        Z = smoothed_grids[i]\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    ax.set_title('3D Visualization of Smoothed Text Embeddings')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Embedding Value')\n",
    "    plt.show()\n",
    "\n",
    "def extract_token_probs(data):\n",
    "    # Initialize an empty list to store the token probabilities\n",
    "    token_probs_list = []\n",
    "    \n",
    "    # Iterate over each entry in the data list\n",
    "    for entry in data:\n",
    "        # Check if 'token_probs' key exists in the dictionary\n",
    "        if 'token_probs' in entry:\n",
    "            # Append the numpy array of token probabilities to the list\n",
    "            token_probs_list.append(entry['token_probs'])\n",
    "    \n",
    "    # Return the list of token probabilities\n",
    "    return token_probs_list\n",
    "\n",
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    prompt = \"make me a short unique poem about aliens fighting on venus over earths iranian goat population gaining sentience\"\n",
    "    n = 1\n",
    "    responses, log_probs = get_responses(prompt, n)\n",
    "    embeddings = get_embeddings(responses)\n",
    "\n",
    "    probability_map = analyze_log_probs(log_probs)\n",
    "    token_probs_list = extract_token_probs(probability_map)\n",
    "    \n",
    "    # Extract token probabilities from the probability map\n",
    "    token_probabilities = np.array([entry['token_probs'] for entry in probability_map])\n",
    "    \n",
    "    # Combine embeddings and token probabilities\n",
    "    combined_input = np.hstack((embeddings, token_probabilities))\n",
    "\n",
    "    grid_size = 30\n",
    "    latent_dim = grid_size * grid_size\n",
    "    encoder = train_autoencoder(combined_input, latent_dim)\n",
    "\n",
    "    encoded_2d_grid = encoder(combined_input).numpy().reshape(-1, grid_size, grid_size)\n",
    "    visualize_2d_grid(encoded_2d_grid, grid_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d7942-5a5f-40bf-b0e3-9f97fc97bf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff08afb-1aae-4bb6-8506-c83fb2cac7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b450094-4bad-44fc-b044-6974e04d016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration\n",
    "MODEL_DIR = \"results/LAURA_1\"  # Update with your specific directory\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_NEW_TOKENS = 200\n",
    "FOLDER_PATH = '/workspace/slice-monorepo/cl_cr3/aligneddata'\n",
    "TARGET_NAME = \"LAURA\"\n",
    "NUM_EXAMPLES = 5\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = GPTNeoXForCausalLM.from_pretrained(MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Function to load real examples from the aligneddata dataset\n",
    "def load_real_examples_from_aligneddata(folder_path, target_name, num_examples):\n",
    "    utterances = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path) as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    for document in data:\n",
    "                        turns = document['TURNS']\n",
    "                        for i in range(1, len(turns)):\n",
    "                            prev_turn = turns[i-1]\n",
    "                            curr_turn = turns[i]\n",
    "                            if target_name in curr_turn['NAMES']:\n",
    "                                instruction = f\"{prev_turn['NAMES'][0]}: \" + \" \".join(prev_turn['UTTERANCES'])\n",
    "                                response = f\"{curr_turn['NAMES'][0]}: \" + \" \".join(curr_turn['UTTERANCES'])\n",
    "                                utterances.append({\"instruction\": instruction, \"expected_response\": response})\n",
    "                                if len(utterances) >= num_examples:\n",
    "                                    return utterances\n",
    "    return utterances\n",
    "\n",
    "# Function to preprocess and load real examples from the dolly15k dataset\n",
    "def load_real_examples_from_dolly15k(num_examples):\n",
    "    dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "    test_dataset = dataset['train'].train_test_split(test_size=0.1)['test']\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        instruction = examples['instruction']\n",
    "        context = examples.get('context', \"\")\n",
    "        response = examples['response']\n",
    "\n",
    "        if isinstance(instruction, list):\n",
    "            instruction = [\" \".join(ins) if isinstance(ins, list) else ins for ins in instruction]\n",
    "        if isinstance(context, list):\n",
    "            context = [\" \".join(con) if isinstance(con, list) else con for con in context]\n",
    "\n",
    "        return {\"instruction\": instruction, \"context\": context, \"response\": response}\n",
    "    \n",
    "    test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n",
    "    \n",
    "    examples = [{\"instruction\": f\"{row['instruction']} {row['context']}\".strip(), \"expected_response\": row['response']} for row in test_dataset]\n",
    "    \n",
    "    return random.sample(examples, num_examples)\n",
    "\n",
    "# Load real examples from both datasets\n",
    "real_examples_aligneddata = load_real_examples_from_aligneddata(FOLDER_PATH, TARGET_NAME, NUM_EXAMPLES)\n",
    "real_examples_dolly15k = load_real_examples_from_dolly15k(NUM_EXAMPLES)\n",
    "\n",
    "# Combine real examples from both datasets\n",
    "real_examples = real_examples_aligneddata + real_examples_dolly15k\n",
    "\n",
    "# Function to generate a response from the model\n",
    "def generate_response(prompt, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(DEVICE)\n",
    "    attention_mask = torch.ones(input_ids.shape, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "# Perform inference and print the results\n",
    "print(\"\\nInference on real examples from both datasets:\")\n",
    "for example in real_examples:\n",
    "    response = generate_response(example[\"instruction\"])\n",
    "    model_response = response.replace(example[\"instruction\"], \"\").strip()\n",
    "    print(f\"Instruction: {example['instruction']}\")\n",
    "    print(f\"Expected Response: {example['expected_response']}\")\n",
    "    print(f\"Model Response: {model_response}\\n\")\n",
    "\n",
    "    # Save test results to file\n",
    "    with open(os.path.join(MODEL_DIR, \"test_results.txt\"), \"a\") as file:\n",
    "        file.write(f\"Instruction: {example['instruction']}\\n\")\n",
    "        file.write(f\"Expected Response: {example['expected_response']}\\n\")\n",
    "        file.write(f\"Model Response: {model_response}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89badc96-636e-471c-89b3-d85f74e148e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Directory where the model and tokenizer are saved\n",
    "directory_name = \"results/EleutherAI-pythia-70m_4\"  # Update with your specific directory\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = GPTNeoXForCausalLM.from_pretrained(directory_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(directory_name)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "test_dataset = dataset['train'].train_test_split(test_size=0.1)['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    instruction = examples['instruction']\n",
    "    context = examples.get('context', \"\")\n",
    "    response = examples['response']\n",
    "\n",
    "    if isinstance(instruction, list):\n",
    "        instruction = [\" \".join(ins) if isinstance(ins, list) else ins for ins in instruction]\n",
    "    if isinstance(context, list):\n",
    "        context = [\" \".join(con) if isinstance(con, list) else con for con in context]\n",
    "    \n",
    "    text = [f\"{ins} {con}\".strip() for ins, con in zip(instruction, context)]\n",
    "    labels = tokenizer(response, truncation=True, padding='max_length', max_length=1000).input_ids\n",
    "\n",
    "    tokenized = tokenizer(text, truncation=True, padding='max_length', max_length=1000)\n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Perform inference on 5 random samples from the test set\n",
    "model.to('cuda:0')\n",
    "model.eval()\n",
    "\n",
    "print(\"\\nFinal Test on 5 random samples from the test set:\")\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    input_ids = batch['input_ids'].to('cuda:0')\n",
    "    attention_mask = batch['attention_mask'].to('cuda:0')\n",
    "    labels = batch['labels'].to('cuda:0')\n",
    "\n",
    "    # Adjust max_new_tokens to ensure generation\n",
    "    output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=200, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    decoded_input = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the input part from the output\n",
    "    model_response = decoded_output[len(decoded_input):].strip()\n",
    "    \n",
    "    expected_output = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Input: {decoded_input}\")\n",
    "    print(f\"Expected Output: {expected_output}\")\n",
    "    print(f\"Model Output: {model_response}\\n\")\n",
    "\n",
    "    # Save test results to file\n",
    "    with open(os.path.join(directory_name, \"test_results.txt\"), \"a\") as file:\n",
    "        file.write(f\"Input: {decoded_input}\\n\")\n",
    "        file.write(f\"Expected Output: {expected_output}\\n\")\n",
    "        file.write(f\"Model Output: {model_response}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50795561-1c5c-4507-943e-b8ca785f9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "# Configuration\n",
    "MODEL_DIR = \"/workspace/slice-monorepo/cl_cr3/llms/gpt/results/LAURA_1\"\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_NEW_TOKENS = 50\n",
    "FOLDER_PATH = '/workspace/slice-monorepo/cl_cr3/aligneddata'\n",
    "TARGET_NAME = \"LAURA\"\n",
    "NUM_EXAMPLES = 5\n",
    "\n",
    "# Function to load real examples from the dataset\n",
    "def load_real_examples(folder_path, target_name, num_examples):\n",
    "    utterances = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path) as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    for document in data:\n",
    "                        turns = document['TURNS']\n",
    "                        for i in range(1, len(turns)):\n",
    "                            prev_turn = turns[i-1]\n",
    "                            curr_turn = turns[i]\n",
    "                            if target_name in curr_turn['NAMES']:\n",
    "                                instruction = f\"{prev_turn['NAMES'][0]}: \" + \" \".join(prev_turn['UTTERANCES'])\n",
    "                                response = f\"{curr_turn['NAMES'][0]}: \" + \" \".join(curr_turn['UTTERANCES'])\n",
    "                                utterances.append({\"instruction\": instruction, \"expected_response\": response})\n",
    "                                if len(utterances) >= num_examples:\n",
    "                                    return utterances\n",
    "    return utterances\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = GPTNeoXForCausalLM.from_pretrained(MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Function to generate a response from the model\n",
    "def generate_response(prompt, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(DEVICE)\n",
    "    attention_mask = torch.ones(input_ids.shape, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "# Load real examples from the dataset\n",
    "real_examples = load_real_examples(FOLDER_PATH, TARGET_NAME, NUM_EXAMPLES)\n",
    "\n",
    "# Perform inference and print the results\n",
    "for example in real_examples:\n",
    "    response = generate_response(example[\"instruction\"])\n",
    "    model_response = response.replace(example[\"instruction\"], \"\").strip()\n",
    "    print(f\"Instruction: {example['instruction']}\")\n",
    "    print(f\"Expected Response: {example['expected_response']}\")\n",
    "    print(f\"Model Response: {model_response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7f499-7887-4d52-b24a-c20132b138e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c07e3d-5220-47a6-a752-4dc21d1efcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302688\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.634319\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.408328\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.149765\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.149948\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.205779\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.114301\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.282829\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.210197\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.078287\n",
      "\n",
      "Test set: Average loss: 0.1201, Accuracy: 9641/10000 (96.41%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.082678\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.096619\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.117954\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.107355\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.094308\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.091134\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.034649\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.164856\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.136344\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.095649\n",
      "\n",
      "Test set: Average loss: 0.0918, Accuracy: 9692/10000 (96.92%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.049815\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.035422\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.066550\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.212461\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.045411\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.034198\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.010644\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.058905\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.158743\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.250405\n",
      "\n",
      "Test set: Average loss: 0.0823, Accuracy: 9734/10000 (97.34%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.039231\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.110178\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.004586\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.013926\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.013537\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.068489\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.017750\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.075106\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.067172\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.099467\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9738/10000 (97.38%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.005192\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.009433\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.014461\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.127292\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.082802\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.050251\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.070470\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.008278\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.022956\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.026744\n",
      "\n",
      "Test set: Average loss: 0.0866, Accuracy: 9735/10000 (97.35%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "config = {\n",
    "    'batch_size': 64,\n",
    "    'test_batch_size': 1000,\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_comp_neurons': 1,  # Number of computation neurons in CustomNeuron\n",
    "    'log_interval': 100,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "\n",
    "class CustomNeuron(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_comp_neurons=1, bias=True):\n",
    "        super(CustomNeuron, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_comp_neurons = num_comp_neurons\n",
    "        self.bias = bias\n",
    "\n",
    "        if self.num_comp_neurons < 1:\n",
    "            raise ValueError(\"num_comp_neurons must be at least 1.\")\n",
    "\n",
    "        # Selection layer: decides which computation neuron to activate\n",
    "        self.selection_layer = nn.Linear(in_features, out_features * self.num_comp_neurons, bias=bias)\n",
    "\n",
    "        # Computation neurons: multiple linear transformations\n",
    "        self.comp_weights = nn.Parameter(torch.Tensor(self.num_comp_neurons, out_features, in_features))\n",
    "        if bias:\n",
    "            self.comp_biases = nn.Parameter(torch.Tensor(self.num_comp_neurons, out_features))\n",
    "        else:\n",
    "            self.comp_biases = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize computation neurons\n",
    "        nn.init.xavier_uniform_(self.comp_weights)\n",
    "        if self.comp_biases is not None:\n",
    "            nn.init.zeros_(self.comp_biases)\n",
    "        # Initialize selection layer\n",
    "        nn.init.xavier_uniform_(self.selection_layer.weight)\n",
    "        if self.selection_layer.bias is not None:\n",
    "            nn.init.zeros_(self.selection_layer.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: Tensor of shape [batch_size, in_features]\n",
    "        Returns:\n",
    "            output: Tensor of shape [batch_size, out_features]\n",
    "        \"\"\"\n",
    "        batch_size, in_features = input.size()\n",
    "\n",
    "        # Detach input for selection neuron to prevent gradients from flowing back to previous layers\n",
    "        input_detached = input.detach()\n",
    "\n",
    "        # Compute selection logits using detached input\n",
    "        selection_logits = self.selection_layer(input_detached)  # [b, o*n]\n",
    "        selection_logits = selection_logits.view(batch_size, self.out_features, self.num_comp_neurons)  # [b, o, n]\n",
    "\n",
    "        # Compute selection probabilities using softmax over computation neurons\n",
    "        selection_probs = F.softmax(selection_logits, dim=-1)  # [b, o, n]\n",
    "\n",
    "        # Hard selection using argmax\n",
    "        selected_idx = torch.argmax(selection_probs, dim=-1)  # [b, o]\n",
    "        selected_mask_hard = F.one_hot(selected_idx, num_classes=self.num_comp_neurons).float()  # [b, o, n]\n",
    "\n",
    "        # Use Straight-Through Estimator (STE)\n",
    "        selected_mask = (selected_mask_hard - selection_probs).detach() + selection_probs  # [b, o, n]\n",
    "\n",
    "        # Compute outputs from all computation neurons using the original input\n",
    "        comp_weights_transposed = self.comp_weights.permute(0, 2, 1)  # [n, in_features, o]\n",
    "        outputs_all = torch.einsum('bi,nio->bno', input, comp_weights_transposed)  # [b, n, o]\n",
    "\n",
    "        # Add biases if present\n",
    "        if self.comp_biases is not None:\n",
    "            comp_biases = self.comp_biases.unsqueeze(0)  # [1, n, o]\n",
    "            outputs_all = outputs_all + comp_biases  # [b, n, o]\n",
    "\n",
    "        # Apply selected mask\n",
    "        selected_mask_transposed = selected_mask.permute(0, 2, 1)  # [b, n, o]\n",
    "        output = torch.sum(outputs_all * selected_mask_transposed, dim=1)  # Sum over n -> [b, o]\n",
    "\n",
    "        return output  # [b, out_features]\n",
    "\n",
    "# ---------------------------\n",
    "# MNIST Dataset\n",
    "# ---------------------------\n",
    "def load_data(config):\n",
    "    # Data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # Mean and std for MNIST\n",
    "    ])\n",
    "\n",
    "    # Training and test datasets\n",
    "    train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['test_batch_size'], shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ---------------------------\n",
    "# MLP Model with Custom Neuron\n",
    "# ---------------------------\n",
    "class MLPWithCustomNeuron(nn.Module):\n",
    "    def __init__(self, num_comp_neurons):\n",
    "        super(MLPWithCustomNeuron, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = CustomNeuron(28 * 28, 256, num_comp_neurons=num_comp_neurons)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = CustomNeuron(256, 128, num_comp_neurons=num_comp_neurons)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = CustomNeuron(128, 10, num_comp_neurons=num_comp_neurons)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)       # [batch_size, 784]\n",
    "        x = self.fc1(x)           # [batch_size, 256]\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)           # [batch_size, 128]\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)           # [batch_size, 10]\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# Training and Testing Functions\n",
    "# ---------------------------\n",
    "def train(model, device, train_loader, optimizer, epoch, config):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  # [batch_size, 10]\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % config['log_interval'] == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')  # Sum the loss over the batch\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)  # [batch_size, 10]\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.2f}%)\\n')\n",
    "\n",
    "# ---------------------------\n",
    "# Main Function\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # Load data\n",
    "    train_loader, test_loader = load_data(config)\n",
    "\n",
    "    # Initialize model\n",
    "    model = MLPWithCustomNeuron(num_comp_neurons=config['num_comp_neurons']).to(config['device'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train(model, config['device'], train_loader, optimizer, epoch, config)\n",
    "        test(model, config['device'], test_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c77f7de-7939-4241-8b12-e3c08424cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302202\tAvg Entropy: 3.0677\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.350034\tAvg Entropy: 1.2301\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.269357\tAvg Entropy: 1.0444\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.220820\tAvg Entropy: 0.9445\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.141563\tAvg Entropy: 0.8833\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.174156\tAvg Entropy: 0.8388\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.251661\tAvg Entropy: 0.7972\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.113283\tAvg Entropy: 0.7647\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.132022\tAvg Entropy: 0.7378\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.076917\tAvg Entropy: 0.7140\n",
      "Epoch 1 Selection Statistics:\n",
      "  Computation Neuron 0: Selected 8002692 times\n",
      "  Computation Neuron 1: Selected 7804890 times\n",
      "  Computation Neuron 2: Selected 7832418 times\n",
      "\n",
      "Test set: Average loss: 0.1655, Accuracy: 9512/10000 (95.12%)\tAvg Entropy: 0.4662\n",
      "\n",
      "Test Set Selection Statistics:\n",
      "  Computation Neuron 0: Selected 1419877 times\n",
      "  Computation Neuron 1: Selected 1237716 times\n",
      "  Computation Neuron 2: Selected 1282407 times\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.095846\tAvg Entropy: 0.4905\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.158178\tAvg Entropy: 0.4831\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.213001\tAvg Entropy: 0.4867\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.143242\tAvg Entropy: 0.4936\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.143228\tAvg Entropy: 0.4956\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.097755\tAvg Entropy: 0.4854\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.089974\tAvg Entropy: 0.4824\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.097004\tAvg Entropy: 0.4774\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.078130\tAvg Entropy: 0.4775\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.060620\tAvg Entropy: 0.4774\n",
      "Epoch 2 Selection Statistics:\n",
      "  Computation Neuron 0: Selected 8415331 times\n",
      "  Computation Neuron 1: Selected 7569419 times\n",
      "  Computation Neuron 2: Selected 7655250 times\n",
      "\n",
      "Test set: Average loss: 0.1368, Accuracy: 9568/10000 (95.68%)\tAvg Entropy: 0.4476\n",
      "\n",
      "Test Set Selection Statistics:\n",
      "  Computation Neuron 0: Selected 1400437 times\n",
      "  Computation Neuron 1: Selected 1270953 times\n",
      "  Computation Neuron 2: Selected 1268610 times\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.205431\tAvg Entropy: 0.4568\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.155977\tAvg Entropy: 0.4608\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.073934\tAvg Entropy: 0.4538\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.199371\tAvg Entropy: 0.4513\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.311069\tAvg Entropy: 0.4491\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.133628\tAvg Entropy: 0.4492\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.209955\tAvg Entropy: 0.4491\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.011510\tAvg Entropy: 0.4471\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.105126\tAvg Entropy: 0.4446\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.048408\tAvg Entropy: 0.4448\n",
      "Epoch 3 Selection Statistics:\n",
      "  Computation Neuron 0: Selected 8628825 times\n",
      "  Computation Neuron 1: Selected 7504886 times\n",
      "  Computation Neuron 2: Selected 7506289 times\n",
      "\n",
      "Test set: Average loss: 0.1168, Accuracy: 9635/10000 (96.35%)\tAvg Entropy: 0.4511\n",
      "\n",
      "Test Set Selection Statistics:\n",
      "  Computation Neuron 0: Selected 1415290 times\n",
      "  Computation Neuron 1: Selected 1267823 times\n",
      "  Computation Neuron 2: Selected 1256887 times\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.019828\tAvg Entropy: 0.4602\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.025551\tAvg Entropy: 0.4461\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.083538\tAvg Entropy: 0.4354\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.123278\tAvg Entropy: 0.4322\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.021663\tAvg Entropy: 0.4337\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.108297\tAvg Entropy: 0.4335\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.010309\tAvg Entropy: 0.4336\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.060255\tAvg Entropy: 0.4321\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.403915\tAvg Entropy: 0.4288\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.065133\tAvg Entropy: 0.4253\n",
      "Epoch 4 Selection Statistics:\n",
      "  Computation Neuron 0: Selected 8446019 times\n",
      "  Computation Neuron 1: Selected 7541677 times\n",
      "  Computation Neuron 2: Selected 7652304 times\n",
      "\n",
      "Test set: Average loss: 0.1120, Accuracy: 9643/10000 (96.43%)\tAvg Entropy: 0.3721\n",
      "\n",
      "Test Set Selection Statistics:\n",
      "  Computation Neuron 0: Selected 1394213 times\n",
      "  Computation Neuron 1: Selected 1265986 times\n",
      "  Computation Neuron 2: Selected 1279801 times\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.069817\tAvg Entropy: 0.3681\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.028985\tAvg Entropy: 0.3780\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.108341\tAvg Entropy: 0.3749\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.009190\tAvg Entropy: 0.3735\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.278640\tAvg Entropy: 0.3736\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.169236\tAvg Entropy: 0.3748\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.030502\tAvg Entropy: 0.3759\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.116379\tAvg Entropy: 0.3758\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.050312\tAvg Entropy: 0.3752\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.150478\tAvg Entropy: 0.3742\n",
      "Epoch 5 Selection Statistics:\n",
      "  Computation Neuron 0: Selected 8291885 times\n",
      "  Computation Neuron 1: Selected 7585663 times\n",
      "  Computation Neuron 2: Selected 7762452 times\n",
      "\n",
      "Test set: Average loss: 0.1112, Accuracy: 9672/10000 (96.72%)\tAvg Entropy: 0.3537\n",
      "\n",
      "Test Set Selection Statistics:\n",
      "  Computation Neuron 0: Selected 1370731 times\n",
      "  Computation Neuron 1: Selected 1265106 times\n",
      "  Computation Neuron 2: Selected 1304163 times\n",
      "\n",
      "Performing Gradient Verification:\n",
      "Gradient Check - fc1.comp_weights: 83.46% zeros in gradients\n",
      "Gradient Check - fc1.comp_biases: 83.46% zeros in gradients\n",
      "Gradient Check - fc1.selection_layer.weight: Gradient norm = 0.202712\n",
      "Gradient Check - fc1.selection_layer.bias: Gradient norm = 0.007878\n",
      "Gradient Check - fc2.comp_weights: 90.23% zeros in gradients\n",
      "Gradient Check - fc2.comp_biases: 50.78% zeros in gradients\n",
      "Gradient Check - fc2.selection_layer.weight: Gradient norm = 0.175042\n",
      "Gradient Check - fc2.selection_layer.bias: Gradient norm = 0.006315\n",
      "Gradient Check - fc3.comp_weights: 60.52% zeros in gradients\n",
      "Gradient Check - fc3.comp_biases: 46.67% zeros in gradients\n",
      "Gradient Check - fc3.selection_layer.weight: Gradient norm = 0.014383\n",
      "Gradient Check - fc3.selection_layer.bias: Gradient norm = 0.000708\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration Parameters\n",
    "# ---------------------------\n",
    "config = {\n",
    "    'batch_size': 64,\n",
    "    'test_batch_size': 1000,\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_comp_neurons': 3,  # Number of computation neurons in CustomNeuron\n",
    "    'log_interval': 100,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "\n",
    "class CustomNeuron(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_comp_neurons=1, bias=True):\n",
    "        super(CustomNeuron, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_comp_neurons = num_comp_neurons\n",
    "        self.bias = bias\n",
    "\n",
    "        if self.num_comp_neurons < 1:\n",
    "            raise ValueError(\"num_comp_neurons must be at least 1.\")\n",
    "\n",
    "        # Selection layer: decides which computation neuron to activate\n",
    "        self.selection_layer = nn.Linear(in_features, out_features * self.num_comp_neurons, bias=bias)\n",
    "\n",
    "        # Computation neurons: multiple linear transformations\n",
    "        # Corrected shape: [n, i, o] instead of [n, o, i]\n",
    "        self.comp_weights = nn.Parameter(torch.Tensor(self.num_comp_neurons, in_features, out_features))\n",
    "        if bias:\n",
    "            self.comp_biases = nn.Parameter(torch.Tensor(self.num_comp_neurons, out_features))\n",
    "        else:\n",
    "            self.comp_biases = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        # Variables to store selection information for verification\n",
    "        self.last_selection_probs = None\n",
    "        self.last_selected_idx = None\n",
    "        self.last_entropy = None  # Initialize entropy\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize computation neurons\n",
    "        nn.init.xavier_uniform_(self.comp_weights)\n",
    "        if self.comp_biases is not None:\n",
    "            nn.init.zeros_(self.comp_biases)\n",
    "        # Initialize selection layer\n",
    "        nn.init.xavier_uniform_(self.selection_layer.weight)\n",
    "        if self.selection_layer.bias is not None:\n",
    "            nn.init.zeros_(self.selection_layer.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: Tensor of shape [batch_size, in_features]\n",
    "        Returns:\n",
    "            output: Tensor of shape [batch_size, out_features]\n",
    "        \"\"\"\n",
    "        batch_size, in_features = input.size()\n",
    "\n",
    "        # Debugging: Print input shape\n",
    "        #print(f'[DEBUG] Input shape: {input.shape}')  # Should be [b, in_features]\n",
    "\n",
    "        # Detach input for selection neuron to prevent gradients from flowing back to previous layers\n",
    "        input_detached = input.detach()\n",
    "\n",
    "        # Compute selection logits using detached input\n",
    "        selection_logits = self.selection_layer(input_detached)  # [b, o*n]\n",
    "        selection_logits = selection_logits.view(batch_size, self.out_features, self.num_comp_neurons)  # [b, o, n]\n",
    "\n",
    "        # Compute selection probabilities using softmax over computation neurons\n",
    "        selection_probs = F.softmax(selection_logits, dim=-1)  # [b, o, n]\n",
    "        self.last_selection_probs = selection_probs  # Store for verification\n",
    "\n",
    "        # Compute entropy of selection probabilities\n",
    "        entropy = -torch.sum(selection_probs * torch.log(selection_probs + 1e-12), dim=-1)  # [b, o]\n",
    "        self.last_entropy = entropy.mean().item()  # Average entropy over batch and output features\n",
    "\n",
    "        # Hard selection using argmax\n",
    "        selected_idx = torch.argmax(selection_probs, dim=-1)  # [b, o]\n",
    "        self.last_selected_idx = selected_idx  # Store for verification\n",
    "\n",
    "        selected_mask_hard = F.one_hot(selected_idx, num_classes=self.num_comp_neurons).float()  # [b, o, n]\n",
    "\n",
    "        # Use Straight-Through Estimator (STE)\n",
    "        selected_mask = (selected_mask_hard - selection_probs).detach() + selection_probs  # [b, o, n]\n",
    "\n",
    "\n",
    "\n",
    "        # Compute outputs from all computation neurons using the original input\n",
    "        # Using torch.einsum for correct tensor operations\n",
    "        # input: [b, i]\n",
    "        # self.comp_weights: [n, i, o]\n",
    "        # outputs_all: [b, n, o] via 'bi,nio->bno'\n",
    "        #print(f'[DEBUG] comp_weights shape: {self.comp_weights.shape}')  # Should be [n, i, o]\n",
    "\n",
    "        # Check if in_features match\n",
    "        expected_in_features = self.in_features\n",
    "        actual_in_features = self.comp_weights.shape[1]  # Index 1 corresponds to 'i'\n",
    "        if actual_in_features != expected_in_features:\n",
    "            raise ValueError(f'In CustomNeuron: comp_weights has in_features={actual_in_features}, '\n",
    "                             f'but expected {expected_in_features}.')\n",
    "\n",
    "        # Perform tensor contraction using einsum\n",
    "        outputs_all = torch.einsum('bi,nio->bno', input, self.comp_weights)  # [b, n, o]\n",
    "\n",
    "        # Add biases if present\n",
    "        if self.comp_biases is not None:\n",
    "            comp_biases = self.comp_biases.unsqueeze(0)  # [1, n, o]\n",
    "            outputs_all = outputs_all + comp_biases  # [b, n, o]\n",
    "\n",
    "        # Apply selected mask\n",
    "        selected_mask_transposed = selected_mask.permute(0, 2, 1)  # [b, n, o]\n",
    "        output = torch.sum(outputs_all * selected_mask_transposed, dim=1)  # Sum over n -> [b, o]\n",
    "\n",
    "        return output  # [b, out_features]\n",
    "\n",
    "# ---------------------------\n",
    "# MNIST Dataset\n",
    "# ---------------------------\n",
    "def load_data(config):\n",
    "    # Data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # Mean and std for MNIST\n",
    "    ])\n",
    "\n",
    "    # Training and test datasets\n",
    "    train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['test_batch_size'], shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ---------------------------\n",
    "# MLP Model with Custom Neuron\n",
    "# ---------------------------\n",
    "class MLPWithCustomNeuron(nn.Module):\n",
    "    def __init__(self, num_comp_neurons):\n",
    "        super(MLPWithCustomNeuron, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = CustomNeuron(in_features=28 * 28, out_features=256, num_comp_neurons=num_comp_neurons)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = CustomNeuron(in_features=256, out_features=128, num_comp_neurons=num_comp_neurons)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = CustomNeuron(in_features=128, out_features=10, num_comp_neurons=num_comp_neurons)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)       # [batch_size, 784]\n",
    "        x = self.fc1(x)           # [batch_size, 256]\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)           # [batch_size, 128]\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)           # [batch_size, 10]\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# Training and Testing Functions\n",
    "# ---------------------------\n",
    "def train(model, device, train_loader, optimizer, epoch, config):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize counters for verification\n",
    "    total_entropy = 0.0\n",
    "    total_samples = 0\n",
    "    comp_neuron_selection = [0] * config['num_comp_neurons']  # To track selection frequency\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  # [batch_size, 10]\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Verification: Access the last entropy and selection indices from each CustomNeuron\n",
    "        entropy_sum = 0.0\n",
    "        for layer in model.children():\n",
    "            if isinstance(layer, CustomNeuron):\n",
    "                entropy_sum += layer.last_entropy\n",
    "                # Count selection frequencies\n",
    "                selected_indices = layer.last_selected_idx.cpu().numpy().flatten()\n",
    "                for idx in selected_indices:\n",
    "                    comp_neuron_selection[idx] += 1\n",
    "\n",
    "        batch_size = data.size(0)\n",
    "        total_entropy += entropy_sum * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        if batch_idx % config['log_interval'] == 0:\n",
    "            avg_entropy = total_entropy / total_samples\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\\t'\n",
    "                  f'Avg Entropy: {avg_entropy:.4f}')\n",
    "\n",
    "    # After each epoch, print selection statistics\n",
    "    print(f'Epoch {epoch} Selection Statistics:')\n",
    "    for i, count in enumerate(comp_neuron_selection):\n",
    "        print(f'  Computation Neuron {i}: Selected {count} times')\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')  # Sum the loss over the batch\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # Variables for verification\n",
    "    total_entropy = 0.0\n",
    "    total_samples = 0\n",
    "    comp_neuron_selection = [0] * config['num_comp_neurons']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)  # [batch_size, 10]\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            # Verification: Access the last entropy and selection indices from each CustomNeuron\n",
    "            entropy_sum = 0.0\n",
    "            for layer in model.children():\n",
    "                if isinstance(layer, CustomNeuron):\n",
    "                    entropy_sum += layer.last_entropy\n",
    "                    # Count selection frequencies\n",
    "                    selected_indices = layer.last_selected_idx.cpu().numpy().flatten()\n",
    "                    for idx in selected_indices:\n",
    "                        comp_neuron_selection[idx] += 1\n",
    "\n",
    "            batch_size = data.size(0)\n",
    "            total_entropy += entropy_sum * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    avg_entropy = total_entropy / total_samples\n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.2f}%)\\t'\n",
    "          f'Avg Entropy: {avg_entropy:.4f}\\n')\n",
    "\n",
    "    # After testing, print selection statistics\n",
    "    print(f'Test Set Selection Statistics:')\n",
    "    for i, count in enumerate(comp_neuron_selection):\n",
    "        print(f'  Computation Neuron {i}: Selected {count} times')\n",
    "\n",
    "# ---------------------------\n",
    "# Gradient Verification Hooks\n",
    "# ---------------------------\n",
    "def verify_gradients(model):\n",
    "    \"\"\"\n",
    "    Verifies that gradients are flowing only through the selected computation neurons\n",
    "    and the selection neurons. This function should be called after backward() and before\n",
    "    optimizer.step().\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'comp_weights' in name or 'comp_biases' in name:\n",
    "            if param.grad is not None:\n",
    "                # Check if gradient is sparse or has many zeros, indicating inactive neurons\n",
    "                num_zero_grads = torch.sum(param.grad == 0).item()\n",
    "                total_elements = param.grad.numel()\n",
    "                zero_grad_ratio = num_zero_grads / total_elements\n",
    "                print(f'Gradient Check - {name}: {zero_grad_ratio*100:.2f}% zeros in gradients')\n",
    "        elif 'selection_layer' in name:\n",
    "            if param.grad is not None:\n",
    "                # Ensure that gradients are flowing through selection layer\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                print(f'Gradient Check - {name}: Gradient norm = {grad_norm:.6f}')\n",
    "\n",
    "# ---------------------------\n",
    "# Main Function\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # Load data\n",
    "    train_loader, test_loader = load_data(config)\n",
    "\n",
    "    # Initialize model\n",
    "    model = MLPWithCustomNeuron(num_comp_neurons=config['num_comp_neurons']).to(config['device'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train(model, config['device'], train_loader, optimizer, epoch, config)\n",
    "        test(model, config['device'], test_loader)\n",
    "\n",
    "    # After training, perform gradient verification on a sample batch\n",
    "    model.train()\n",
    "    sample_data, sample_target = next(iter(train_loader))\n",
    "    sample_data, sample_target = sample_data.to(config['device']), sample_target.to(config['device'])\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_data)\n",
    "    loss = nn.CrossEntropyLoss()(output, sample_target)\n",
    "    loss.backward()\n",
    "\n",
    "    print('\\nPerforming Gradient Verification:')\n",
    "    verify_gradients(model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196a389-b005-47fc-85ef-820c2a8b4e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

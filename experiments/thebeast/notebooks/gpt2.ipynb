{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9842df0-0b6a-48bb-9f98-0f6b8f79b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7458c-f4f9-49f9-a019-f9f36791ef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0a354759dd46d4b7b296c562233391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded openwebtext dataset with 8013769 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7f79d07dbd412cbc6dff512f43601e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8013769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import GPT2Tokenizer\n",
    "import pickle\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, directory):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        # Read all files in the specified directory\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.json'):\n",
    "                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                    data = json.load(file)\n",
    "                    for item in data:\n",
    "                        encoded = tokenizer(item['response_content'], truncation=True, max_length=512, padding='max_length', return_tensors='pt')\n",
    "                        self.examples.append({key: val.squeeze(0) for key, val in encoded.items()})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Function to tokenize dataset\n",
    "def tokenize_function(examples, text_column):\n",
    "    tokens = tokenizer(examples[text_column], truncation=True, max_length=512, padding='max_length', return_tensors='pt')\n",
    "    tokens = {k: v.squeeze(0) for k, v in tokens.items()}  # Remove extra dimension\n",
    "    return tokens\n",
    "\n",
    "# Function to load and tokenize dataset using a very small subset\n",
    "def load_and_tokenize_dataset(name, split, text_column):\n",
    "    dataset = load_dataset(name, split=f\"{split}\", trust_remote_code=True)\n",
    "    print(f\"Loaded {name} dataset with {len(dataset)} samples\")\n",
    "    tokenized_dataset = dataset.map(lambda x: tokenize_function(x, text_column), batched=True, remove_columns=[text_column])\n",
    "    print(f\"Tokenized {name} dataset: {tokenized_dataset}\")\n",
    "    return tokenized_dataset\n",
    "\n",
    "# Specify the directory containing the JSON files\n",
    "directory_path = '/workspace/slice-monorepo/thebeast/notebooks/combined'\n",
    "\n",
    "# Load the custom dataset\n",
    "custom_dataset = CustomTextDataset(tokenizer, directory_path)\n",
    "\n",
    "# Load other datasets\n",
    "datasets = []\n",
    "datasets.append(load_and_tokenize_dataset('openwebtext', 'train', 'text'))\n",
    "datasets.append(load_and_tokenize_dataset('bookcorpus', 'train', 'text'))\n",
    "datasets.append(load_and_tokenize_dataset('gigaword', 'train', 'document'))\n",
    "\n",
    "# Combine datasets\n",
    "all_datasets = [custom_dataset] + [d['input_ids'] for d in datasets]\n",
    "combined_dataset = concatenate_datasets(all_datasets)\n",
    "print(f\"Combined dataset has {len(combined_dataset)} samples\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(combined_dataset))\n",
    "test_size = len(combined_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(combined_dataset, [train_size, test_size])\n",
    "\n",
    "# Save datasets\n",
    "with open('train_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "with open('test_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dataset, f)\n",
    "print(\"Datasets saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192c9e2-3593-4f37-87b4-ce5f2b2045f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Load datasets\n",
    "with open('train_dataset.pkl', 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "with open('test_dataset.pkl', 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Option to initialize the model from scratch\n",
    "def initialize_model(from_pretrained=True):\n",
    "    if from_pretrained:\n",
    "        model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "    else:\n",
    "        config = GPT2LMHeadModel.config_class()\n",
    "        model = GPT2LMHeadModel(config)\n",
    "    model.resize_token_embeddings(len(tokenizer))  # Adjust the model's embedding size to account for new tokens\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = initialize_model(from_pretrained=True)  # Change to False to train from scratch\n",
    "\n",
    "# Set device and DataParallel\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_dataloader) * 3  # Assuming 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "model.train()\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss.mean()  # Aggregate the loss to a scalar value\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "total_eval_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss.mean()  # Aggregate the loss to a scalar value\n",
    "        total_eval_loss += loss.item()\n",
    "    avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
    "    eval_losses.append(avg_eval_loss)\n",
    "    print(f\"Average Evaluation Loss: {avg_eval_loss}\")\n",
    "\n",
    "# Save the model\n",
    "model_path = 'distilgpt2-trained'\n",
    "if isinstance(model, nn.DataParallel):\n",
    "    model.module.save_pretrained(model_path)\n",
    "else:\n",
    "    model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Plotting the training and evaluation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(eval_losses, label='Evaluation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Simple validation\n",
    "model.eval()\n",
    "sample_text = \"Once upon a time\"\n",
    "encoded_input = tokenizer(sample_text, return_tensors='pt').to(device)\n",
    "if isinstance(model, nn.DataParallel):\n",
    "    output = model.module.generate(encoded_input['input_ids'], max_length=50)\n",
    "else:\n",
    "    output = model.generate(encoded_input['input_ids'], max_length=50)\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Sample Output: {decoded_output}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac898c29-d6d1-4548-a474-c94dde635eb4",
   "metadata": {},
   "source": [
    "###############################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4388cc-f151-4bd4-affc-984dde61c772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9201884fa78e468fbae6ebf1e28abd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea51f035fb964724b6d2377af1546b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee4d2a5ae9e44f5910451c70ea8b200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b071dde4f53143a78cd027b48e758690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56dcdd5c533c408db0d91738c7d61006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc63493cd9a447a58a845e2e178e4a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5717959510d64e70b8fb58fd0ff1f717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/21 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cc72906a89432db742608f309da106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8013769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded openwebtext dataset with 80138 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6f239b8eec41699eb9a8f3e1c7f27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized openwebtext dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 80138\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd20f49c361e46c6b498ad821b4a8d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19ee0acae534ead8d5e9907eded1a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/18.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb5bab16d0d489b8e0666755c425576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa0b2e46fe941fdb178ed86784ca401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/74004228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token\n",
    "\n",
    "# Function to tokenize dataset\n",
    "def tokenize_function(examples, text_column):\n",
    "    tokens = tokenizer(examples[text_column], padding='max_length', truncation=True, max_length=128)\n",
    "    return tokens\n",
    "\n",
    "# Function to load and tokenize dataset\n",
    "def load_and_tokenize_dataset(name, split, text_column):\n",
    "    dataset = load_dataset(name, split=split, trust_remote_code=True)\n",
    "    print(f\"Loaded {name} dataset with {len(dataset)} samples\")\n",
    "    tokenized_dataset = dataset.map(lambda x: tokenize_function(x, text_column), batched=True, remove_columns=[text_column])\n",
    "    print(f\"Tokenized {name} dataset: {tokenized_dataset}\")\n",
    "    return tokenized_dataset\n",
    "\n",
    "# Load datasets\n",
    "datasets = []\n",
    "\n",
    "# Uncomment the datasets you want to use\n",
    "# Wikipedia\n",
    "# datasets.append(load_and_tokenize_dataset('wikipedia', '20220301.en[:1%]', 'text'))\n",
    "# OpenWebText\n",
    "datasets.append(load_and_tokenize_dataset('openwebtext', 'train[:1%]', 'text'))\n",
    "# BooksCorpus\n",
    "datasets.append(load_and_tokenize_dataset('bookcorpus', 'train[:1%]', 'text'))\n",
    "# English Gigaword\n",
    "datasets.append(load_and_tokenize_dataset('gigaword', 'train[:1%]', 'document'))\n",
    "\n",
    "# Combine datasets\n",
    "combined_dataset = concatenate_datasets(datasets)\n",
    "print(f\"Combined dataset has {len(combined_dataset)} samples\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(combined_dataset))\n",
    "test_size = len(combined_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(combined_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Print dataset and dataloader information\n",
    "print(f\"Train dataset has {len(train_dataset)} samples\")\n",
    "print(f\"Test dataset has {len(test_dataset)} samples\")\n",
    "print(\"DataLoader configuration:\")\n",
    "print(f\"Train DataLoader: {len(train_dataloader)} batches\")\n",
    "print(f\"Test DataLoader: {len(test_dataloader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ab884-eb9a-4208-bccd-c3a131e066e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Save the combined dataset and DataLoader configuration\n",
    "save_path = 'combined_dataset.pkl'\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'combined_dataset': combined_dataset,\n",
    "        'train_dataset': train_dataset,\n",
    "        'test_dataset': test_dataset,\n",
    "        'train_dataloader': train_dataloader,\n",
    "        'test_dataloader': test_dataloader,\n",
    "    }, f)\n",
    "\n",
    "print(f\"Saved combined dataset and DataLoader configuration to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e822c3-286c-4ca4-9c6f-34f03c183c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Load the combined dataset and DataLoader configuration\n",
    "load_path = 'combined_dataset.pkl'\n",
    "\n",
    "with open(load_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "combined_dataset = data['combined_dataset']\n",
    "train_dataset = data['train_dataset']\n",
    "test_dataset = data['test_dataset']\n",
    "train_dataloader = data['train_dataloader']\n",
    "test_dataloader = data['test_dataloader']\n",
    "\n",
    "print(f\"Loaded combined dataset with {len(combined_dataset)} samples\")\n",
    "print(f\"Train dataset has {len(train_dataset)} samples\")\n",
    "print(f\"Test dataset has {len(test_dataset)} samples\")\n",
    "print(\"DataLoader configuration:\")\n",
    "print(f\"Train DataLoader: {len(train_dataloader)} batches\")\n",
    "print(f\"Test DataLoader: {len(test_dataloader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68566b2e-6e36-4766-a60b-f2c3d2633923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Initialize model\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Ensure padding token is added\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize embeddings to accommodate the new pad token\n",
    "\n",
    "# Set device to CUDA or CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Improved custom collate function to handle None entries and ensure all elements are tensors\n",
    "def custom_collate(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    collated_batch = {}\n",
    "    for key in batch[0]:\n",
    "        if key == 'summary':\n",
    "            continue\n",
    "        filtered = [b[key] for b in batch if b[key] is not None]\n",
    "        if len(filtered) > 0:\n",
    "            if isinstance(filtered[0], list):\n",
    "                filtered = [torch.tensor(f) for f in filtered]\n",
    "            collated_batch[key] = torch.stack(filtered, dim=0)\n",
    "        else:\n",
    "            collated_batch[key] = None\n",
    "    return collated_batch\n",
    "\n",
    "# Create DataLoader with custom collate function\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, collate_fn=custom_collate, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=2, collate_fn=custom_collate, shuffle=False)\n",
    "\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_dataloader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1):  # Run for 3 epochs\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        if batch is None:  # Skip empty batches\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        if batch is None:  # Skip empty batches\n",
    "            continue\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = input_ids.clone().to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./distilgpt2-trained')\n",
    "tokenizer.save_pretrained('./distilgpt2-trained')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

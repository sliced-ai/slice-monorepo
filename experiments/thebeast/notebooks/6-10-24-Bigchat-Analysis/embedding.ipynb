{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8488bb3a-f632-4d4f-965b-2801e2984ce5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CubicSpline, interp1d\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[1;32m      7\u001b[0m EMBEDDING_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from scipy.interpolate import CubicSpline, interp1d\n",
    "from itertools import combinations\n",
    "\n",
    "EMBEDDING_SIZE = 500\n",
    "\n",
    "# Load the embeddings from the JSON file\n",
    "def load_embeddings(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Interpolation method to bring all embeddings to the same size\n",
    "def interpolate_embeddings(embeddings, target_dim=EMBEDDING_SIZE, method='linear'):\n",
    "    interpolated_embeddings = []\n",
    "    for embed in embeddings:\n",
    "        original_indices = np.linspace(0, 1, num=len(embed))\n",
    "        target_indices = np.linspace(0, 1, num=target_dim)\n",
    "        \n",
    "        if method == 'linear':\n",
    "            interpolated_embed = np.interp(target_indices, original_indices, embed)\n",
    "        elif method == 'cubic':\n",
    "            cs = CubicSpline(original_indices, embed)\n",
    "            interpolated_embed = cs(target_indices)\n",
    "        elif method == 'quadratic':\n",
    "            quadratic_interp = interp1d(original_indices, embed, kind='quadratic')\n",
    "            interpolated_embed = quadratic_interp(target_indices)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported interpolation method: {method}\")\n",
    "        \n",
    "        interpolated_embeddings.append(interpolated_embed)\n",
    "    interpolated_embeddings = np.array(interpolated_embeddings)\n",
    "    return interpolated_embeddings\n",
    "\n",
    "# Function to compare embeddings\n",
    "def compare_embeddings(embeddings1, embeddings2):\n",
    "    similarities = []\n",
    "    for embed1, embed2 in zip(embeddings1, embeddings2):\n",
    "        similarity = np.dot(embed1, embed2) / (np.linalg.norm(embed1) * np.linalg.norm(embed2))\n",
    "        similarities.append(similarity)\n",
    "    return similarities\n",
    "\n",
    "# Function to process a single file and compare interpolation methods\n",
    "def compare_methods(filepath, methods=['linear', 'cubic', 'quadratic']):\n",
    "    data = load_embeddings(filepath)\n",
    "    embeddings = [item['embedding'] for item in data['embeddings']]\n",
    "    \n",
    "    comparisons = {\n",
    "        'original': compare_embeddings_combinations(embeddings)\n",
    "    }\n",
    "    \n",
    "    for method in methods:\n",
    "        interpolated_embeddings = interpolate_embeddings(embeddings, target_dim=EMBEDDING_SIZE, method=method)\n",
    "        method_similarities = []\n",
    "        for original_embed in embeddings:\n",
    "            original_embed_interp = interpolate_embeddings([original_embed], target_dim=EMBEDDING_SIZE, method=method)[0]\n",
    "            method_similarities.append(compare_embeddings([original_embed], [original_embed_interp])[0])\n",
    "        comparisons[method] = method_similarities\n",
    "    \n",
    "    return comparisons\n",
    "\n",
    "# Function to compare all combinations of original embeddings\n",
    "def compare_embeddings_combinations(embeddings):\n",
    "    similarities = []\n",
    "    for (embed1, embed2) in combinations(embeddings, 2):\n",
    "        similarity = np.dot(embed1, embed2) / (np.linalg.norm(embed1) * np.linalg.norm(embed2))\n",
    "        similarities.append(similarity)\n",
    "    return similarities\n",
    "\n",
    "# Example usage to compare different interpolation methods for all files in a directory\n",
    "def compare_all_files(directory, methods=['linear', 'cubic', 'quadratic']):\n",
    "    all_comparisons = {'original': []}\n",
    "    for method in methods:\n",
    "        all_comparisons[method] = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            comparisons = compare_methods(filepath, methods=methods)\n",
    "            for key in comparisons:\n",
    "                all_comparisons[key].extend(comparisons[key])\n",
    "    \n",
    "    # Calculate cumulative metrics\n",
    "    cumulative_metrics = {}\n",
    "    for key in all_comparisons:\n",
    "        cumulative_metrics[key] = {\n",
    "            'mean_similarity': np.mean(all_comparisons[key]),\n",
    "            'std_similarity': np.std(all_comparisons[key])\n",
    "        }\n",
    "    \n",
    "    return cumulative_metrics\n",
    "\n",
    "# Path to the directory containing the JSON files\n",
    "directory_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/embeddings'\n",
    "\n",
    "# Compare all files in the directory\n",
    "cumulative_metrics = compare_all_files(directory_path)\n",
    "\n",
    "# Display the cumulative metrics\n",
    "print(\"Cumulative Metrics:\")\n",
    "for method, metrics in cumulative_metrics.items():\n",
    "    print(f\"Method: {method}\")\n",
    "    print(f\"  Mean Similarity: {metrics['mean_similarity']:.4f}\")\n",
    "    print(f\"  Std Similarity: {metrics['std_similarity']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98660e86-12b5-426d-9a7c-7a3dbc17ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/embeddings/193f9423-65e4-4ca2-a05d-5ef9414a65c1.json']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "EMBEDDING_SIZE = 500\n",
    "\n",
    "# Load the embeddings from the JSON file\n",
    "def load_embeddings(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Interpolation method to bring all embeddings to the same size\n",
    "def interpolate_embeddings(embeddings, target_dim=EMBEDDING_SIZE):\n",
    "    interpolated_embeddings = []\n",
    "    for embed in embeddings:\n",
    "        original_indices = np.linspace(0, 1, num=len(embed))\n",
    "        target_indices = np.linspace(0, 1, num=target_dim)\n",
    "        interpolated_embed = np.interp(target_indices, original_indices, embed)\n",
    "        interpolated_embeddings.append(interpolated_embed)\n",
    "    interpolated_embeddings = np.array(interpolated_embeddings)\n",
    "    return interpolated_embeddings\n",
    "\n",
    "# Mean pooling function\n",
    "def mean_pooling(embeddings, target_dim=EMBEDDING_SIZE):\n",
    "    interpolated_embeddings = interpolate_embeddings(embeddings, target_dim)\n",
    "    return np.mean(interpolated_embeddings, axis=0)\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(filepath):\n",
    "    data = load_embeddings(filepath)\n",
    "    embeddings = [item['embedding'] for item in data['embeddings']]\n",
    "    \n",
    "    # Apply Mean Pooling\n",
    "    mean_pooled_embedding = mean_pooling(embeddings, target_dim=EMBEDDING_SIZE)\n",
    "    \n",
    "    # Add combined embeddings to the data\n",
    "    data['mean_pooled'] = mean_pooled_embedding.tolist()\n",
    "    \n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Function to process all files in a directory\n",
    "def process_directory(directory):\n",
    "    updated_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            updated_filepath = process_file(filepath)\n",
    "            updated_files.append(updated_filepath)\n",
    "    return updated_files\n",
    "\n",
    "# Path to the directory containing the JSON files\n",
    "directory_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/embeddings'\n",
    "\n",
    "# Process all files in the directory\n",
    "updated_files = process_directory(directory_path)\n",
    "\n",
    "# Display the updated files\n",
    "print(updated_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ded0f8-6fa0-4a8f-b72e-01f78cd47112",
   "metadata": {},
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate fake embedding dataset with different lengths\n",
    "def generate_fake_embeddings(num_groups=10, min_len=5, max_len=20, embedding_dim=50):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    embeddings = []\n",
    "    for _ in range(num_groups):\n",
    "        group_len = np.random.randint(min_len, max_len)\n",
    "        group_embeddings = [np.random.rand(np.random.randint(embedding_dim//2, embedding_dim*2)) for _ in range(group_len)]\n",
    "        embeddings.append(group_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Interpolation method to bring all embeddings to the same size\n",
    "def interpolate_embeddings(embeddings, target_dim=EMBEDDING_SIZE):\n",
    "    interpolated_embeddings = []\n",
    "    for embed in embeddings:\n",
    "        original_indices = np.linspace(0, 1, num=len(embed))\n",
    "        target_indices = np.linspace(0, 1, num=target_dim)\n",
    "        interpolated_embed = np.interp(target_indices, original_indices, embed)\n",
    "        interpolated_embeddings.append(interpolated_embed)\n",
    "    return interpolated_embeddings\n",
    "\n",
    "# PCA function to project embeddings to a fixed size\n",
    "def apply_pca(embeddings, target_dim=EMBEDDING_SIZE):\n",
    "    interpolated_embeddings = interpolate_embeddings(embeddings, target_dim)\n",
    "    pca = PCA(n_components=min(target_dim, len(interpolated_embeddings)))\n",
    "    pca_embeddings = pca.fit_transform(interpolated_embeddings)\n",
    "    return pca_embeddings\n",
    "\n",
    "# Mean pooling function\n",
    "def mean_pooling(embeddings, target_dim=EMBEDDING_SIZE):\n",
    "    interpolated_embeddings = interpolate_embeddings(embeddings, target_dim)\n",
    "    return np.mean(interpolated_embeddings, axis=0)\n",
    "\n",
    "# Hybrid approach: PCA then Mean pooling\n",
    "def hybrid_pca_mean_pooling(embeddings, target_dim=EMBEDDING_SIZE):\n",
    "    pca_embeddings = apply_pca(embeddings, target_dim)\n",
    "    return mean_pooling(pca_embeddings, target_dim)\n",
    "\n",
    "# Generate fake dataset\n",
    "embeddings = generate_fake_embeddings()\n",
    "\n",
    "# Apply PCA\n",
    "pca_embeddings = [apply_pca(group) for group in embeddings]\n",
    "\n",
    "# Apply Mean Pooling\n",
    "mean_pooled_embeddings = [mean_pooling(group) for group in embeddings]\n",
    "\n",
    "# Apply Hybrid PCA then Mean Pooling\n",
    "hybrid_embeddings = [hybrid_pca_mean_pooling(group) for group in embeddings]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame for better display\n",
    "result_df = pd.DataFrame({\n",
    "    'PCA Embedding': [emb.tolist() for emb in pca_embeddings],\n",
    "    'Mean Pooling Embedding': [emb.tolist() for emb in mean_pooled_embeddings],\n",
    "    'Hybrid Embedding': [emb.tolist() for emb in hybrid_embeddings]\n",
    "})\n",
    "\n",
    "\n",
    "result_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1cb57-1f3f-43ff-8189-ddf077dcd7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import os\n",
    "\n",
    "EMBEDDING_SIZE = 500\n",
    "\n",
    "# Load the embeddings from the JSON file\n",
    "def load_embeddings(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Normalization function\n",
    "def normalize_embeddings(embeddings):\n",
    "    mean = np.mean(embeddings, axis=0)\n",
    "    std = np.std(embeddings, axis=0)\n",
    "    std[std == 0] = 1  # Avoid division by zero\n",
    "    normalized_embeddings = (embeddings - mean) / std\n",
    "    return normalized_embeddings\n",
    "\n",
    "# Interpolation method to bring all embeddings to the same size\n",
    "def interpolate_embeddings(embeddings, target_dim=EMBEDDING_SIZE):\n",
    "    interpolated_embeddings = []\n",
    "    for embed in embeddings:\n",
    "        original_indices = np.linspace(0, 1, num=len(embed))\n",
    "        target_indices = np.linspace(0, 1, num=target_dim)\n",
    "        interpolated_embed = np.interp(target_indices, original_indices, embed)\n",
    "        interpolated_embeddings.append(interpolated_embed)\n",
    "    interpolated_embeddings = np.array(interpolated_embeddings)\n",
    "    return interpolated_embeddings\n",
    "\n",
    "# PCA function to project embeddings to a fixed size\n",
    "def apply_pca(embeddings, target_dim=EMBEDDING_SIZE, min_components=10, variance_threshold=0.95):\n",
    "    interpolated_embeddings = interpolate_embeddings(embeddings, target_dim)\n",
    "    normalized_embeddings = normalize_embeddings(interpolated_embeddings)\n",
    "    \n",
    "    n_samples, n_features = len(normalized_embeddings), len(normalized_embeddings[0])\n",
    "    max_components = min(n_samples, n_features)\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(normalized_embeddings)\n",
    "    \n",
    "    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = max(np.argmax(explained_variance >= variance_threshold) + 1, min_components)\n",
    "    pca = PCA(n_components=min(n_components, max_components))\n",
    "    pca_embeddings = pca.fit_transform(normalized_embeddings)\n",
    "    \n",
    "    print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_[:5]}\")\n",
    "    print(f\"PCA components shape: {pca.components_.shape}\")\n",
    "    print(f\"PCA number of components selected: {n_components}\")\n",
    "    print(f\"PCA first 5 points: {pca_embeddings[:5]}\")\n",
    "    \n",
    "    return pca_embeddings\n",
    "\n",
    "# Mean pooling function\n",
    "def mean_pooling(embeddings, target_dim=EMBEDDING_SIZE):\n",
    "    interpolated_embeddings = interpolate_embeddings(embeddings, target_dim)\n",
    "    return np.mean(interpolated_embeddings, axis=0)\n",
    "\n",
    "# Hybrid approach: PCA then Mean pooling\n",
    "def hybrid_pca_mean_pooling(embeddings, target_dim=EMBEDDING_SIZE):\n",
    "    pca_embeddings = apply_pca(embeddings, target_dim)\n",
    "    mean_pooled = np.mean(pca_embeddings, axis=0)\n",
    "    return mean_pooled\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(filepath):\n",
    "    data = load_embeddings(filepath)\n",
    "    embeddings = [item['embedding'] for item in data['embeddings']]\n",
    "\n",
    "    # Apply PCA\n",
    "    pca_embeddings = apply_pca(embeddings, target_dim=EMBEDDING_SIZE)\n",
    "    pca_mean = np.mean(pca_embeddings, axis=0)\n",
    "\n",
    "    # Apply Mean Pooling\n",
    "    mean_pooled_embedding = mean_pooling(embeddings, target_dim=EMBEDDING_SIZE)\n",
    "\n",
    "    # Apply Hybrid PCA then Mean Pooling\n",
    "    hybrid_embedding = hybrid_pca_mean_pooling(embeddings, target_dim=EMBEDDING_SIZE)\n",
    "\n",
    "    # Add combined embeddings to the data\n",
    "    data['combined'] = {\n",
    "        'PCA': pca_mean.tolist(),\n",
    "        'MeanPooling': mean_pooled_embedding.tolist(),\n",
    "        'Hybrid': hybrid_embedding.tolist()\n",
    "    }\n",
    "\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Function to process all files in a directory\n",
    "def process_directory(directory):\n",
    "    updated_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            updated_filepath = process_file(filepath)\n",
    "            updated_files.append(updated_filepath)\n",
    "    return updated_files\n",
    "\n",
    "# Variance and range checks with more detailed debug\n",
    "def check_embedding_statistics(original_embeddings, processed_embeddings):\n",
    "    original_var = np.var(original_embeddings, axis=0)\n",
    "    processed_var = np.var(processed_embeddings, axis=0)\n",
    "    \n",
    "    original_range = np.ptp(original_embeddings, axis=0)\n",
    "    processed_range = np.ptp(processed_embeddings, axis=0)\n",
    "    \n",
    "    var_ratio = processed_var / original_var\n",
    "    range_ratio = processed_range / original_range\n",
    "    \n",
    "    print(f\"Original variance: {original_var.mean()}\")\n",
    "    print(f\"Processed variance: {processed_var.mean()}\")\n",
    "    print(f\"Variance ratio: {var_ratio.mean()}\")\n",
    "    print(f\"Original range: {original_range.mean()}\")\n",
    "    print(f\"Processed range: {processed_range.mean()}\")\n",
    "    print(f\"Range ratio: {range_ratio.mean()}\")\n",
    "    \n",
    "    assert 0.5 < var_ratio.mean() < 2.0, \"Processed variance is not within acceptable range\"\n",
    "    assert 0.5 < range_ratio.mean() < 2.0, \"Processed range is not within acceptable range\"\n",
    "\n",
    "# Function to test the structure and shape of the final file\n",
    "def test_final_file(filepath, expected_shape=500):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    assert 'combined' in data, \"Missing 'combined' key in JSON file\"\n",
    "    combined = data['combined']\n",
    "    \n",
    "    original_embeddings = [item['embedding'] for item in data['embeddings']]\n",
    "    original_embeddings = interpolate_embeddings(original_embeddings, target_dim=expected_shape)\n",
    "\n",
    "    for key in ['MeanPooling', 'Hybrid']:\n",
    "        assert key in combined, f\"Missing '{key}' key in 'combined'\"\n",
    "        processed_embeddings = np.array(combined[key])\n",
    "        assert len(processed_embeddings) == expected_shape, f\"Incorrect shape for '{key}' embedding\"\n",
    "        check_embedding_statistics(original_embeddings, processed_embeddings)\n",
    "\n",
    "    # Handle PCA embedding separately\n",
    "    assert 'PCA' in combined, f\"Missing 'PCA' key in 'combined'\"\n",
    "    pca_embedding_shape = len(combined['PCA'])\n",
    "    assert pca_embedding_shape > 0, f\"Incorrect shape for 'PCA' embedding\"\n",
    "    pca_embeddings = np.array(combined['PCA'])\n",
    "    check_embedding_statistics(original_embeddings, pca_embeddings)\n",
    "\n",
    "    print(f\"File {filepath} passed the tests.\")\n",
    "\n",
    "# Path to the directory containing the JSON files\n",
    "directory_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/embeddings'\n",
    "\n",
    "# Process all files in the directory\n",
    "updated_files = process_directory(directory_path)\n",
    "\n",
    "# Test the first updated file to ensure it is correctly formatted\n",
    "test_final_file(updated_files[0])\n",
    "\n",
    "# Display the combined embeddings\n",
    "updated_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aefa6e-d69f-45e3-a889-24c40fd9d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, AdamW\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "NUM_TEXTS = 160\n",
    "MAX_LENGTH = 300\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Step 1: Generate Fake Data\n",
    "fake_embeddings = [torch.tensor(np.random.rand(EMBEDDING_DIM), dtype=torch.float32) for _ in range(NUM_TEXTS * 2)]\n",
    "\n",
    "# Generate random text sequences from the tokenizer's vocabulary\n",
    "def generate_random_texts(tokenizer, num_texts, max_length):\n",
    "    texts = []\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    for _ in range(num_texts):\n",
    "        random_tokens = np.random.randint(0, vocab_size, size=(max_length,))\n",
    "        texts.append(tokenizer.decode(random_tokens, skip_special_tokens=True))\n",
    "    return texts\n",
    "\n",
    "# Step 2: Initialize Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the padding token\n",
    "\n",
    "fake_texts = generate_random_texts(tokenizer, num_texts=NUM_TEXTS, max_length=MAX_LENGTH)\n",
    "\n",
    "# Convert Text to Tokens\n",
    "tokenized_texts = [tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=MAX_LENGTH)['input_ids'].squeeze(0) for text in fake_texts]\n",
    "\n",
    "# Step 3: Create Dataset\n",
    "class EmbeddingTextDataset(Dataset):\n",
    "    def __init__(self, embeddings, tokenized_texts):\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.tokenized_texts[idx]\n",
    "\n",
    "dataset = EmbeddingTextDataset(fake_embeddings, tokenized_texts)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(\"dataset made\")\n",
    "# Step 4: Define the Model\n",
    "class SimpleTextGenerator(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim, max_length):\n",
    "        super(SimpleTextGenerator, self).__init__()\n",
    "        self.embedding_projection = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size * max_length)\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, embedding):\n",
    "        projected_embedding = self.embedding_projection(embedding)\n",
    "        output = self.fc(projected_embedding)\n",
    "        output = output.view(-1, self.max_length, self.vocab_size)\n",
    "        return output\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = SimpleTextGenerator(EMBEDDING_DIM, vocab_size, HIDDEN_DIM, MAX_LENGTH)\n",
    "print(\"model made\")\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Step 5: Train the Model\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for embeddings, tokenized_texts in dataloader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            tokenized_texts = tokenized_texts.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(embeddings)\n",
    "            loss = criterion(logits.view(-1, vocab_size), tokenized_texts.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "train(model, dataloader, optimizer, criterion, device, NUM_EPOCHS)\n",
    "\n",
    "# Step 6: Generate and Print Text in One Go\n",
    "def generate_text_from_embedding(model, embedding, tokenizer, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = embedding.to(device)\n",
    "        logits = model(embedding)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        generated_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "# Test the model with a new embedding\n",
    "test_embedding = torch.tensor(np.random.rand(EMBEDDING_DIM), dtype=torch.float32).to(device)\n",
    "generated_text = generate_text_from_embedding(model, test_embedding, tokenizer, MAX_LENGTH)\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd793256-9ccf-4fec-9d2e-0e6a8c1f2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def extract_embeddings(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    uuid = data.get('uuid')\n",
    "    combined = data.get('combined', {})\n",
    "    mean_pooling = combined.get('MeanPooling', [])\n",
    "    hybrid = combined.get('Hybrid', [])\n",
    "\n",
    "    return uuid, mean_pooling, hybrid\n",
    "\n",
    "def process_directory(directory):\n",
    "    embeddings_dict = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('_updated.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            uuid, mean_pooling, hybrid = extract_embeddings(filepath)\n",
    "            if uuid is not None:\n",
    "                embeddings_dict[uuid] = {\n",
    "                    'MeanPooling': mean_pooling,\n",
    "                    'Hybrid': hybrid\n",
    "                }\n",
    "        break\n",
    "    return embeddings_dict\n",
    "\n",
    "def load_response_list(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def combine_data(embeddings_dict, response_list):\n",
    "    combined_dict = {}\n",
    "    for response in response_list:\n",
    "        uuid = response.get('uuid')\n",
    "        if uuid in embeddings_dict:\n",
    "            combined_dict[uuid] = {\n",
    "                'response_content': response.get('response_content'),\n",
    "                'MeanPooling': embeddings_dict[uuid]['MeanPooling'],\n",
    "                'Hybrid': embeddings_dict[uuid]['Hybrid']\n",
    "            }\n",
    "    return combined_dict\n",
    "\n",
    "# Path to the directory containing the updated JSON files\n",
    "directory_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/test'\n",
    "\n",
    "# Path to the uuid_response_list.json file\n",
    "response_list_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/uuid_response_list.json'\n",
    "\n",
    "# Process all files in the directory to extract embeddings\n",
    "embeddings_dict = process_directory(directory_path)\n",
    "\n",
    "# Load the response list\n",
    "response_list = load_response_list(response_list_path)\n",
    "\n",
    "# Combine the embeddings with the response content\n",
    "combined_data = combine_data(embeddings_dict, response_list)\n",
    "\n",
    "# Display the resulting combined dictionary\n",
    "print(combined_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbfa74-8964-4326-b7bd-d9542af09048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, AdamW\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration variables\n",
    "SEED = 42\n",
    "NUM_TEXTS = 160\n",
    "MAX_LENGTH = 50\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "EMBEDDING_TYPE = 'Hybrid'  # Options: 'MeanPooling', 'Hybrid'\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# Step 1: Load Embeddings and Response Content\n",
    "def extract_embeddings(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    uuid = data.get('uuid')\n",
    "    combined = data.get('combined', {})\n",
    "    mean_pooling = combined.get('MeanPooling', [])\n",
    "    hybrid = combined.get('Hybrid', [])\n",
    "\n",
    "    return uuid, mean_pooling, hybrid\n",
    "\n",
    "def process_directory(directory):\n",
    "    embeddings_dict = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('_updated.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            uuid, mean_pooling, hybrid = extract_embeddings(filepath)\n",
    "            if uuid is not None:\n",
    "                embeddings_dict[uuid] = {\n",
    "                    'MeanPooling': mean_pooling,\n",
    "                    'Hybrid': hybrid\n",
    "                }\n",
    "    return embeddings_dict\n",
    "\n",
    "def load_response_list(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def combine_data(embeddings_dict, response_list):\n",
    "    combined_dict = {}\n",
    "    for response in response_list:\n",
    "        uuid = response.get('uuid')\n",
    "        if uuid in embeddings_dict:\n",
    "            combined_dict[uuid] = {\n",
    "                'response_content': response.get('response_content'),\n",
    "                'MeanPooling': embeddings_dict[uuid]['MeanPooling'],\n",
    "                'Hybrid': embeddings_dict[uuid]['Hybrid']\n",
    "            }\n",
    "    return combined_dict\n",
    "\n",
    "def load_combined_data(embeddings_path, response_list_path):\n",
    "    embeddings_dict = process_directory(embeddings_path)\n",
    "    response_list = load_response_list(response_list_path)\n",
    "    combined_data = combine_data(embeddings_dict, response_list)\n",
    "    return combined_data\n",
    "\n",
    "def print_intermediate_info(combined_data):\n",
    "    print(f\"Number of data points found: {len(combined_data)}\")\n",
    "    if combined_data:\n",
    "        example_uuid, example_data = next(iter(combined_data.items()))\n",
    "        print(f\"Example UUID: {example_uuid}\")\n",
    "        print(f\"Example Response Content: {example_data['response_content']}\")\n",
    "        print(f\"Example Embedding (MeanPooling): {example_data['MeanPooling'][:5]}...\")  # print first 5 elements\n",
    "        print(f\"Example Embedding (Hybrid): {example_data['Hybrid'][:5]}...\")  # print first 5 elements\n",
    "\n",
    "# Step 2: Initialize Tokenizer\n",
    "def initialize_tokenizer():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set the padding token\n",
    "    return tokenizer\n",
    "\n",
    "def process_texts(texts, tokenizer, max_length):\n",
    "    tokenized_texts = [tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)['input_ids'].squeeze(0) for text in texts]\n",
    "    return tokenized_texts\n",
    "\n",
    "# Step 3: Create Dataset\n",
    "class EmbeddingTextDataset(Dataset):\n",
    "    def __init__(self, combined_data, tokenized_texts, embedding_type):\n",
    "        self.combined_data = combined_data\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.embedding_type = embedding_type\n",
    "        self.keys = list(combined_data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        uuid = self.keys[idx]\n",
    "        embedding = self.combined_data[uuid][self.embedding_type]\n",
    "        embedding = torch.tensor(embedding, dtype=torch.float32)\n",
    "        tokenized_text = self.tokenized_texts[idx]\n",
    "        return embedding, tokenized_text\n",
    "\n",
    "def create_dataloader(combined_data, tokenizer, embedding_type, batch_size):\n",
    "    texts = [item['response_content'] for item in combined_data.values()]\n",
    "    tokenized_texts = process_texts(texts, tokenizer, MAX_LENGTH)\n",
    "    dataset = EmbeddingTextDataset(combined_data, tokenized_texts, embedding_type)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# Step 4: Define the Model\n",
    "class SimpleTextGenerator(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim, max_length):\n",
    "        super(SimpleTextGenerator, self).__init__()\n",
    "        self.embedding_projection = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size * max_length)\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, embedding):\n",
    "        projected_embedding = self.embedding_projection(embedding)\n",
    "        output = self.fc(projected_embedding)\n",
    "        output = output.view(-1, self.max_length, self.vocab_size)\n",
    "        return output\n",
    "\n",
    "def initialize_model(embedding_dim, vocab_size, hidden_dim, max_length):\n",
    "    model = SimpleTextGenerator(embedding_dim, vocab_size, hidden_dim, max_length)\n",
    "    return model\n",
    "\n",
    "def move_model_to_device(model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return model, device\n",
    "\n",
    "# Step 5: Train the Model\n",
    "def train(model, dataloader, optimizer, criterion, device, epochs, vocab_size):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for embeddings, tokenized_texts in dataloader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            tokenized_texts = tokenized_texts.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(embeddings)\n",
    "            loss = criterion(logits.view(-1, vocab_size), tokenized_texts.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "# Step 6: Generate and Print Text in One Go\n",
    "def generate_text_from_embedding(model, embedding, tokenizer, max_length, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = embedding.to(device)\n",
    "        logits = model(embedding)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        generated_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "# Step 7: Tokenizer Test\n",
    "def tokenizer_test(tokenizer, example_text, max_length):\n",
    "    print(f\"Original text: {example_text}\")\n",
    "    tokenized_text = tokenizer(example_text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)['input_ids'].squeeze(0)\n",
    "    decoded_text = tokenizer.decode(tokenized_text, skip_special_tokens=True)\n",
    "    print(f\"Tokenized and decoded text: {decoded_text}\")\n",
    "\n",
    "def split_data(combined_data, test_size=0.2):\n",
    "    keys = list(combined_data.keys())\n",
    "    train_keys, test_keys = train_test_split(keys, test_size=test_size, random_state=SEED)\n",
    "    train_data = {key: combined_data[key] for key in train_keys}\n",
    "    test_data = {key: combined_data[key] for key in test_keys}\n",
    "    return train_data, test_data\n",
    "\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # Load combined data\n",
    "    combined_data = load_combined_data(embeddings_path, response_list_path)\n",
    "    print_intermediate_info(combined_data)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    train_data, test_data = split_data(combined_data)\n",
    "    print(f\"Training data points: {len(train_data)}\")\n",
    "    print(f\"Test data points: {len(test_data)}\")\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = initialize_tokenizer()\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # Tokenizer test with an example text\n",
    "    example_text = next(iter(combined_data.values()))['response_content']\n",
    "    tokenizer_test(tokenizer, example_text, MAX_LENGTH)\n",
    "\n",
    "    # Create dataloader for training data\n",
    "    train_dataloader = create_dataloader(train_data, tokenizer, EMBEDDING_TYPE, BATCH_SIZE)\n",
    "\n",
    "    # Initialize and move model to device\n",
    "    model = initialize_model(EMBEDDING_DIM, vocab_size, HIDDEN_DIM, MAX_LENGTH)\n",
    "    model, device = move_model_to_device(model)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    train(model, train_dataloader, optimizer, criterion, device, NUM_EPOCHS, vocab_size)\n",
    "\n",
    "    # Evaluate the model with the test set\n",
    "    test_embeddings = [torch.tensor(test_data[key][EMBEDDING_TYPE], dtype=torch.float32).to(device) for key in test_data]\n",
    "    for i, test_embedding in enumerate(test_embeddings):\n",
    "        generated_text = generate_text_from_embedding(model, test_embedding, tokenizer, MAX_LENGTH, device)\n",
    "        original_text = test_data[list(test_data.keys())[i]]['response_content']\n",
    "        print(f\"Original text: {original_text}\")\n",
    "        print(f\"Generated text: {generated_text}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    embeddings_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/test'\n",
    "    response_list_path = '/workspace/slice-monorepo/thebeast/chat_pipeline/data/test/step_2/uuid_response_list.json'\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9a59a-60e4-406d-b303-94120dcaf31f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

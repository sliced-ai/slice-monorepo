{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12745559-fc2a-4fca-a5e3-73da055995a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Total data-model pairs loaded: 4299\n",
      "\n",
      "Training data size: 4256, Test data size: 43\n",
      "\n",
      "Epoch [1/100] - Loss: 2.390843, Train Acc: 11.62%, Test Acc: 13.33%\n",
      "Epoch [2/100] - Loss: 2.411651, Train Acc: 13.31%, Test Acc: 12.37%\n",
      "Epoch [3/100] - Loss: 2.498167, Train Acc: 11.94%, Test Acc: 10.79%\n",
      "Epoch [4/100] - Loss: 2.554593, Train Acc: 11.06%, Test Acc: 10.47%\n",
      "Epoch [5/100] - Loss: 2.583582, Train Acc: 10.46%, Test Acc: 9.53%\n",
      "Epoch [6/100] - Loss: 2.594849, Train Acc: 10.27%, Test Acc: 9.49%\n",
      "Epoch [7/100] - Loss: 2.601420, Train Acc: 10.11%, Test Acc: 9.49%\n",
      "Epoch [8/100] - Loss: 2.601428, Train Acc: 9.99%, Test Acc: 10.02%\n",
      "Epoch [9/100] - Loss: 2.599147, Train Acc: 10.20%, Test Acc: 9.51%\n",
      "Epoch [10/100] - Loss: 2.594085, Train Acc: 10.20%, Test Acc: 9.74%\n",
      "Epoch [11/100] - Loss: 2.584065, Train Acc: 10.71%, Test Acc: 11.37%\n",
      "Epoch [12/100] - Loss: 2.510952, Train Acc: 11.37%, Test Acc: 11.98%\n",
      "Epoch [13/100] - Loss: 2.358377, Train Acc: 14.26%, Test Acc: 15.35%\n",
      "Epoch [14/100] - Loss: 2.317825, Train Acc: 17.31%, Test Acc: 15.47%\n",
      "Epoch [15/100] - Loss: 2.203647, Train Acc: 21.60%, Test Acc: 23.49%\n",
      "Epoch [16/100] - Loss: 2.109163, Train Acc: 25.26%, Test Acc: 20.93%\n",
      "Epoch [17/100] - Loss: 2.174482, Train Acc: 25.09%, Test Acc: 23.88%\n",
      "Epoch [18/100] - Loss: 2.127449, Train Acc: 27.28%, Test Acc: 25.37%\n",
      "Epoch [19/100] - Loss: 2.235998, Train Acc: 27.88%, Test Acc: 25.19%\n",
      "Epoch [20/100] - Loss: 2.345284, Train Acc: 28.48%, Test Acc: 25.16%\n",
      "Epoch [21/100] - Loss: 2.421576, Train Acc: 29.40%, Test Acc: 26.23%\n",
      "Epoch [22/100] - Loss: 2.506537, Train Acc: 30.07%, Test Acc: 25.53%\n",
      "Epoch [23/100] - Loss: 2.692670, Train Acc: 30.22%, Test Acc: 27.23%\n",
      "Epoch [24/100] - Loss: 2.829764, Train Acc: 30.61%, Test Acc: 27.81%\n",
      "Epoch [25/100] - Loss: 2.946404, Train Acc: 31.33%, Test Acc: 28.19%\n",
      "Epoch [26/100] - Loss: 3.157971, Train Acc: 31.33%, Test Acc: 28.16%\n",
      "Epoch [27/100] - Loss: 3.217993, Train Acc: 31.90%, Test Acc: 28.35%\n",
      "Epoch [28/100] - Loss: 3.447336, Train Acc: 31.90%, Test Acc: 30.05%\n",
      "Epoch [29/100] - Loss: 3.505996, Train Acc: 32.51%, Test Acc: 29.00%\n",
      "Epoch [30/100] - Loss: 3.650544, Train Acc: 32.87%, Test Acc: 27.33%\n",
      "Epoch [31/100] - Loss: 3.792270, Train Acc: 32.94%, Test Acc: 28.51%\n",
      "Epoch [32/100] - Loss: 3.881388, Train Acc: 33.80%, Test Acc: 29.37%\n",
      "Epoch [33/100] - Loss: 3.971841, Train Acc: 33.97%, Test Acc: 28.40%\n",
      "Epoch [34/100] - Loss: 4.122498, Train Acc: 34.18%, Test Acc: 28.88%\n",
      "Epoch [35/100] - Loss: 4.107695, Train Acc: 34.91%, Test Acc: 29.30%\n",
      "Epoch [36/100] - Loss: 4.233380, Train Acc: 35.33%, Test Acc: 28.95%\n",
      "Epoch [37/100] - Loss: 4.244115, Train Acc: 35.94%, Test Acc: 28.65%\n",
      "Epoch [38/100] - Loss: 4.349825, Train Acc: 36.23%, Test Acc: 29.14%\n",
      "Epoch [39/100] - Loss: 4.478069, Train Acc: 36.41%, Test Acc: 30.16%\n",
      "Epoch [40/100] - Loss: 4.406354, Train Acc: 37.29%, Test Acc: 27.44%\n",
      "Epoch [41/100] - Loss: 4.580289, Train Acc: 37.06%, Test Acc: 26.47%\n",
      "Epoch [42/100] - Loss: 4.551538, Train Acc: 37.79%, Test Acc: 28.70%\n",
      "Epoch [43/100] - Loss: 4.644368, Train Acc: 38.11%, Test Acc: 27.91%\n",
      "Epoch [44/100] - Loss: 4.646682, Train Acc: 38.54%, Test Acc: 28.28%\n",
      "Epoch [45/100] - Loss: 4.635106, Train Acc: 38.96%, Test Acc: 28.14%\n",
      "Epoch [46/100] - Loss: 4.741110, Train Acc: 39.30%, Test Acc: 28.00%\n",
      "Epoch [47/100] - Loss: 4.762569, Train Acc: 39.75%, Test Acc: 27.26%\n",
      "Epoch [48/100] - Loss: 4.830906, Train Acc: 39.94%, Test Acc: 28.47%\n",
      "Epoch [49/100] - Loss: 4.798142, Train Acc: 40.39%, Test Acc: 27.72%\n",
      "Epoch [50/100] - Loss: 4.905622, Train Acc: 40.46%, Test Acc: 28.02%\n",
      "Epoch [51/100] - Loss: 4.907014, Train Acc: 40.94%, Test Acc: 28.93%\n",
      "Epoch [52/100] - Loss: 4.871667, Train Acc: 41.58%, Test Acc: 28.47%\n",
      "Epoch [53/100] - Loss: 4.870797, Train Acc: 41.73%, Test Acc: 29.35%\n",
      "Epoch [54/100] - Loss: 4.954224, Train Acc: 42.11%, Test Acc: 27.79%\n",
      "Epoch [55/100] - Loss: 5.012205, Train Acc: 42.02%, Test Acc: 28.56%\n",
      "Epoch [56/100] - Loss: 4.918881, Train Acc: 42.53%, Test Acc: 28.42%\n",
      "Epoch [57/100] - Loss: 4.937222, Train Acc: 43.18%, Test Acc: 29.37%\n",
      "Epoch [58/100] - Loss: 5.062031, Train Acc: 42.84%, Test Acc: 28.12%\n",
      "Epoch [59/100] - Loss: 5.069927, Train Acc: 43.32%, Test Acc: 28.37%\n",
      "Epoch [60/100] - Loss: 5.019379, Train Acc: 43.83%, Test Acc: 28.74%\n",
      "Epoch [61/100] - Loss: 5.048512, Train Acc: 44.15%, Test Acc: 27.86%\n",
      "Epoch [62/100] - Loss: 5.079359, Train Acc: 44.22%, Test Acc: 27.81%\n",
      "Epoch [63/100] - Loss: 5.069130, Train Acc: 44.62%, Test Acc: 28.35%\n",
      "Epoch [64/100] - Loss: 5.128862, Train Acc: 44.85%, Test Acc: 28.07%\n",
      "Epoch [65/100] - Loss: 5.083365, Train Acc: 45.05%, Test Acc: 28.07%\n",
      "Epoch [66/100] - Loss: 5.114029, Train Acc: 45.22%, Test Acc: 27.37%\n",
      "Epoch [67/100] - Loss: 5.180489, Train Acc: 45.22%, Test Acc: 27.88%\n",
      "Epoch [68/100] - Loss: 5.173381, Train Acc: 45.16%, Test Acc: 27.16%\n",
      "Epoch [69/100] - Loss: 5.170333, Train Acc: 45.62%, Test Acc: 27.47%\n",
      "Epoch [70/100] - Loss: 5.079576, Train Acc: 46.22%, Test Acc: 27.30%\n",
      "Epoch [71/100] - Loss: 5.138236, Train Acc: 46.28%, Test Acc: 27.37%\n",
      "Epoch [72/100] - Loss: 5.126407, Train Acc: 46.75%, Test Acc: 27.58%\n",
      "Epoch [73/100] - Loss: 5.184323, Train Acc: 46.62%, Test Acc: 26.95%\n",
      "Epoch [74/100] - Loss: 5.203759, Train Acc: 46.64%, Test Acc: 28.26%\n",
      "Epoch [75/100] - Loss: 5.168226, Train Acc: 46.85%, Test Acc: 27.02%\n",
      "Epoch [76/100] - Loss: 5.201519, Train Acc: 46.93%, Test Acc: 27.58%\n",
      "Epoch [77/100] - Loss: 5.220388, Train Acc: 47.03%, Test Acc: 26.63%\n",
      "Epoch [78/100] - Loss: 5.282206, Train Acc: 47.01%, Test Acc: 26.72%\n",
      "Epoch [79/100] - Loss: 5.134473, Train Acc: 47.71%, Test Acc: 27.44%\n",
      "Epoch [80/100] - Loss: 5.179965, Train Acc: 47.67%, Test Acc: 27.51%\n",
      "Epoch [81/100] - Loss: 5.186997, Train Acc: 47.79%, Test Acc: 26.88%\n",
      "Epoch [82/100] - Loss: 5.244954, Train Acc: 48.04%, Test Acc: 26.16%\n",
      "Epoch [83/100] - Loss: 5.172408, Train Acc: 48.22%, Test Acc: 27.67%\n",
      "Epoch [84/100] - Loss: 5.218888, Train Acc: 48.04%, Test Acc: 25.74%\n",
      "Epoch [85/100] - Loss: 5.173190, Train Acc: 48.52%, Test Acc: 26.70%\n",
      "Epoch [86/100] - Loss: 5.202220, Train Acc: 48.54%, Test Acc: 26.72%\n",
      "Epoch [87/100] - Loss: 5.172655, Train Acc: 48.64%, Test Acc: 24.81%\n",
      "Epoch [88/100] - Loss: 5.197569, Train Acc: 48.70%, Test Acc: 26.86%\n",
      "Epoch [89/100] - Loss: 5.143587, Train Acc: 49.21%, Test Acc: 26.30%\n",
      "Epoch [90/100] - Loss: 5.207310, Train Acc: 49.06%, Test Acc: 25.70%\n",
      "Epoch [91/100] - Loss: 5.249694, Train Acc: 49.04%, Test Acc: 25.81%\n",
      "Epoch [92/100] - Loss: 5.258606, Train Acc: 48.93%, Test Acc: 27.77%\n",
      "Epoch [93/100] - Loss: 5.229696, Train Acc: 49.20%, Test Acc: 26.67%\n",
      "Epoch [94/100] - Loss: 5.250189, Train Acc: 48.99%, Test Acc: 26.56%\n",
      "Epoch [95/100] - Loss: 5.200392, Train Acc: 49.31%, Test Acc: 25.47%\n",
      "Epoch [96/100] - Loss: 5.268780, Train Acc: 49.37%, Test Acc: 26.49%\n",
      "Epoch [97/100] - Loss: 5.290183, Train Acc: 49.73%, Test Acc: 25.49%\n",
      "Epoch [98/100] - Loss: 5.316651, Train Acc: 49.40%, Test Acc: 27.26%\n",
      "Epoch [99/100] - Loss: 5.260441, Train Acc: 49.61%, Test Acc: 25.16%\n",
      "Epoch [100/100] - Loss: 5.196487, Train Acc: 49.81%, Test Acc: 26.02%\n",
      "\n",
      "Hypernetwork training completed and saved to 'trained_multi_branch_hypernetwork.pt'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Configuration Parameters\n",
    "# ---------------------------\n",
    "DATA_FOLDER = './saved_examples'   # Folder containing the data-model pairs\n",
    "TRAIN_RATIO = 0.99         # Ratio of data to use for training\n",
    "NUM_EPOCHS = 100          # Number of training epochs\n",
    "LEARNING_RATE = 1e-4      # Learning rate for the optimizer\n",
    "BATCH_SIZE = 512            # Number of subsets per batch\n",
    "\n",
    "# Target Model Configuration\n",
    "INPUT_SIZE = 784          # Input size (28x28 images flattened)\n",
    "HIDDEN_SIZE = 8           # Hidden size in the target models\n",
    "OUTPUT_SIZE = 10          # Number of classes\n",
    "SUBSET_SIZE = 100         # Number of examples per subset\n",
    "\n",
    "# Calculated Parameters\n",
    "PER_EXAMPLE_INPUT_SIZE = INPUT_SIZE + OUTPUT_SIZE  # 784 + 10 = 794\n",
    "TOTAL_INPUT_SIZE = SUBSET_SIZE * PER_EXAMPLE_INPUT_SIZE  # 100 * 794 = 79,400\n",
    "\n",
    "# Target Model Parameter Size\n",
    "# SimpleNN: (784 * 8 + 8) + (8 * 10 + 10) = 6,280 + 90 = 6,370\n",
    "PARAM_SIZE = (INPUT_SIZE * HIDDEN_SIZE + HIDDEN_SIZE) + (HIDDEN_SIZE * OUTPUT_SIZE + OUTPUT_SIZE)  # 6370\n",
    "\n",
    "# Hypernetwork Configuration\n",
    "NUM_EXAMPLES = SUBSET_SIZE       # Number of examples per subset\n",
    "EMBED_DIM = 256                   # Embedding dimension per example\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Model Definitions\n",
    "# ---------------------------\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Target neural network whose parameters are to be predicted by the hypernetwork.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=784, hidden_size=8, num_layers=1, output_size=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class EmbeddingBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Dedicated embedding branch for each input example.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=794, embed_dim=256):\n",
    "        super(EmbeddingBranch, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)  # Output shape: (batch_size, embed_dim)\n",
    "\n",
    "class FusionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Fusion layer to combine embeddings from all examples using attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=256, num_examples=100):\n",
    "        super(FusionLayer, self).__init__()\n",
    "        self.num_examples = num_examples\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads=8, dropout=0.1)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (sequence_length=num_examples, batch_size, embed_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (sequence_length=num_examples, batch_size, embed_dim)\n",
    "        \"\"\"\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.layer_norm(x + self.dropout(attn_output))\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.layer_norm(x + self.dropout(ff_output))\n",
    "        return x  # Shape: same as input\n",
    "\n",
    "class ParameterGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generates the parameter vector from the fused embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=256, param_size=6370):\n",
    "        super(ParameterGenerator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, param_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)  # Shape: (batch_size, param_size)\n",
    "\n",
    "class MultiBranchHypernetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Hypernetwork with dedicated embedding branches for each input example.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_examples=100, per_example_input_size=794, embed_dim=256, param_size=6370):\n",
    "        super(MultiBranchHypernetwork, self).__init__()\n",
    "        self.num_examples = num_examples\n",
    "        self.per_example_input_size = per_example_input_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Create a list of embedding branches, one for each example\n",
    "        self.embedding_branches = nn.ModuleList([\n",
    "            EmbeddingBranch(input_size=per_example_input_size, embed_dim=embed_dim) for _ in range(num_examples)\n",
    "        ])\n",
    "        \n",
    "        # Fusion layer to combine all embeddings\n",
    "        self.fusion_layer = FusionLayer(embed_dim=embed_dim, num_examples=num_examples)\n",
    "        \n",
    "        # Parameter generator\n",
    "        self.param_generator = ParameterGenerator(embed_dim=embed_dim, param_size=param_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, num_examples * per_example_input_size)\n",
    "        Returns:\n",
    "            param_vector: Tensor of shape (batch_size, param_size)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        # Split the input into individual examples\n",
    "        x = x.view(batch_size, self.num_examples, self.per_example_input_size)  # Shape: (batch_size, num_examples, per_example_input_size)\n",
    "        \n",
    "        # Pass each example through its dedicated embedding branch\n",
    "        embeddings = []\n",
    "        for i in range(self.num_examples):\n",
    "            emb = self.embedding_branches[i](x[:, i, :])  # Shape: (batch_size, embed_dim)\n",
    "            embeddings.append(emb)\n",
    "        \n",
    "        # Stack embeddings to form a sequence for the fusion layer\n",
    "        embeddings = torch.stack(embeddings, dim=0)  # Shape: (num_examples, batch_size, embed_dim)\n",
    "        \n",
    "        # Apply fusion layer (e.g., attention-based)\n",
    "        fused = self.fusion_layer(embeddings)  # Shape: (num_examples, batch_size, embed_dim)\n",
    "        \n",
    "        # Aggregate transformer outputs: mean pooling over the sequence\n",
    "        aggregated = fused.mean(dim=0)  # Shape: (batch_size, embed_dim)\n",
    "        \n",
    "        # Generate parameter vector\n",
    "        param_vector = self.param_generator(aggregated)  # Shape: (batch_size, param_size)\n",
    "        \n",
    "        return param_vector\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Dataset and DataLoader\n",
    "# ---------------------------\n",
    "\n",
    "class HypernetworkDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Hypernetwork.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_list (list): List of tuples (input_vector, param_vector).\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_vector, param_vector = self.data_list[idx]\n",
    "        return input_vector, param_vector\n",
    "\n",
    "def load_data(data_folder):\n",
    "    \"\"\"\n",
    "    Loads the data-model pairs from the specified folder.\n",
    "\n",
    "    Args:\n",
    "        data_folder (str): Directory containing the saved examples.\n",
    "\n",
    "    Returns:\n",
    "        data_list (list): List of tuples (input_vector, param_vector).\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for file_name in os.listdir(data_folder):\n",
    "        if file_name.endswith('.pt'):\n",
    "            file_path = os.path.join(data_folder, file_name)\n",
    "            checkpoint = torch.load(file_path, map_location='cpu')  # Load on CPU to avoid CUDA issues in workers\n",
    "            data = checkpoint['data']   # Tensor of shape (100, 784)\n",
    "            targets = checkpoint['targets']   # Tensor of shape (100)\n",
    "            model_state_dict = checkpoint['model_state_dict']  # Dictionary of model parameters\n",
    "\n",
    "            # Initialize the model and load state_dict\n",
    "            model = SimpleNN(input_size=784, hidden_size=8, num_layers=1, output_size=10).to('cpu')  # Initialize on CPU\n",
    "            try:\n",
    "                model.load_state_dict(model_state_dict)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error loading state_dict for file {file_name}: {e}\")\n",
    "                continue  # Skip this file if there's an error\n",
    "\n",
    "            # Flatten model parameters into a single vector\n",
    "            param_vector = flatten_model_parameters(model)  # Shape: (6370,)\n",
    "\n",
    "            # Flatten data and labels into a single input vector\n",
    "            labels_one_hot = torch.nn.functional.one_hot(targets, num_classes=10).float()  # Shape: (100, 10)\n",
    "            input_vector = torch.cat([data, labels_one_hot], dim=1).view(-1)  # Shape: (100 * 794,) = (79,400,)\n",
    "\n",
    "            data_list.append((input_vector, param_vector))\n",
    "    return data_list\n",
    "\n",
    "def split_data(data_list, train_ratio=TRAIN_RATIO):\n",
    "    \"\"\"\n",
    "    Splits the data into training and test sets.\n",
    "\n",
    "    Args:\n",
    "        data_list (list): List of data-model pairs.\n",
    "        train_ratio (float): Ratio of data to use for training.\n",
    "\n",
    "    Returns:\n",
    "        train_data (list): Training data.\n",
    "        test_data (list): Test data.\n",
    "    \"\"\"\n",
    "    random.shuffle(data_list)\n",
    "    split_idx = int(len(data_list) * train_ratio)\n",
    "    train_data = data_list[:split_idx]\n",
    "    test_data = data_list[split_idx:]\n",
    "    return train_data, test_data\n",
    "\n",
    "def flatten_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Flattens all parameters of the model into a single vector.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "\n",
    "    Returns:\n",
    "        flat_params (Tensor): Flattened parameter vector.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        params.append(param.view(-1))\n",
    "    return torch.cat(params)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Training and Evaluation Functions\n",
    "# ---------------------------\n",
    "\n",
    "def train_hypernetwork(train_loader, hypernetwork, optimizer, mse_criterion, ce_criterion):\n",
    "    \"\"\"\n",
    "    Trains the hypernetwork for one epoch using a combined loss function.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        hypernetwork (nn.Module): Hypernetwork to generate model parameters.\n",
    "        optimizer (Optimizer): Optimizer for the hypernetwork.\n",
    "        mse_criterion (Loss): Mean Squared Error loss for parameter regression.\n",
    "        ce_criterion (Loss): Cross-Entropy loss for model performance.\n",
    "\n",
    "    Returns:\n",
    "        avg_loss (float): Average combined loss over the epoch.\n",
    "        avg_train_acc (float): Average training accuracy over the epoch.\n",
    "    \"\"\"\n",
    "    hypernetwork.train()\n",
    "    total_loss = 0.0\n",
    "    total_train_acc = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    for batch_idx, (input_batch, param_batch) in enumerate(train_loader):\n",
    "        input_batch = input_batch.to(device)       # Shape: (B, 79,400)\n",
    "        param_batch = param_batch.to(device)       # Shape: (B, 6370)\n",
    "        \n",
    "        # Generate predicted parameters\n",
    "        predicted_params = hypernetwork(input_batch)        # Shape: (B, 6370)\n",
    "        \n",
    "        # Define MSE loss between predicted_params and true params\n",
    "        mse_loss = mse_criterion(predicted_params, param_batch)\n",
    "        \n",
    "        # Initialize a list to hold Cross-Entropy losses\n",
    "        ce_losses = []\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        # Forward pass through each subset in the batch\n",
    "        for i in range(predicted_params.size(0)):\n",
    "            # Load predicted parameters into a new SimpleNN model\n",
    "            generated_model = SimpleNN(input_size=784, hidden_size=8, num_layers=1, output_size=10).to(device)\n",
    "            load_parameters_into_model(generated_model, predicted_params[i])  # Shape: (6370,)\n",
    "            \n",
    "            # Extract input data and labels for the subset\n",
    "            subset_input = input_batch[i].view(SUBSET_SIZE, PER_EXAMPLE_INPUT_SIZE)[:, :INPUT_SIZE]  # Shape: (100, 784)\n",
    "            subset_labels = input_batch[i].view(SUBSET_SIZE, PER_EXAMPLE_INPUT_SIZE)[:, INPUT_SIZE:].argmax(dim=1)  # Shape: (100,)\n",
    "            \n",
    "            # Forward pass through the generated model\n",
    "            outputs = generated_model(subset_input)  # Shape: (100, 10)\n",
    "            \n",
    "            # Compute Cross-Entropy loss\n",
    "            ce_loss = ce_criterion(outputs, subset_labels)\n",
    "            ce_losses.append(ce_loss)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted_labels == subset_labels).sum().item()\n",
    "            total_predictions += subset_labels.size(0)\n",
    "        \n",
    "        # Aggregate CE losses\n",
    "        ce_loss = torch.stack(ce_losses).mean()\n",
    "        \n",
    "        # Combined loss\n",
    "        combined_loss = mse_loss + ce_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += combined_loss.item()\n",
    "        total_train_acc += (correct_predictions / total_predictions) * 100  # Percentage\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_train_acc = total_train_acc / num_batches\n",
    "    return avg_loss, avg_train_acc\n",
    "\n",
    "def evaluate_hypernetwork(test_loader, hypernetwork, mse_criterion, ce_criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the hypernetwork on the test data.\n",
    "\n",
    "    Args:\n",
    "        test_loader (DataLoader): DataLoader for test data.\n",
    "        hypernetwork (nn.Module): Hypernetwork to generate model parameters.\n",
    "        mse_criterion (Loss): Mean Squared Error loss for parameter regression.\n",
    "        ce_criterion (Loss): Cross-Entropy loss for model performance.\n",
    "\n",
    "    Returns:\n",
    "        avg_test_loss (float): Average combined loss on the test data.\n",
    "        avg_test_acc (float): Average test accuracy.\n",
    "    \"\"\"\n",
    "    hypernetwork.eval()\n",
    "    total_loss = 0.0\n",
    "    total_test_acc = 0.0\n",
    "    num_batches = len(test_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_batch, param_batch) in enumerate(test_loader):\n",
    "            input_batch = input_batch.to(device)       # Shape: (B, 79,400)\n",
    "            param_batch = param_batch.to(device)       # Shape: (B, 6370)\n",
    "            \n",
    "            # Generate predicted parameters\n",
    "            predicted_params = hypernetwork(input_batch)        # Shape: (B, 6370)\n",
    "            \n",
    "            # Define MSE loss between predicted_params and true params\n",
    "            mse_loss = mse_criterion(predicted_params, param_batch)\n",
    "            \n",
    "            # Initialize a list to hold Cross-Entropy losses\n",
    "            ce_losses = []\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            \n",
    "            # Forward pass through each subset in the batch\n",
    "            for i in range(predicted_params.size(0)):\n",
    "                # Load predicted parameters into a new SimpleNN model\n",
    "                generated_model = SimpleNN(input_size=784, hidden_size=8, num_layers=1, output_size=10).to(device)\n",
    "                load_parameters_into_model(generated_model, predicted_params[i])  # Shape: (6370,)\n",
    "                \n",
    "                # Extract input data and labels for the subset\n",
    "                subset_input = input_batch[i].view(SUBSET_SIZE, PER_EXAMPLE_INPUT_SIZE)[:, :INPUT_SIZE]  # Shape: (100, 784)\n",
    "                subset_labels = input_batch[i].view(SUBSET_SIZE, PER_EXAMPLE_INPUT_SIZE)[:, INPUT_SIZE:].argmax(dim=1)  # Shape: (100,)\n",
    "                \n",
    "                # Forward pass through the generated model\n",
    "                outputs = generated_model(subset_input)  # Shape: (100, 10)\n",
    "                \n",
    "                # Compute Cross-Entropy loss\n",
    "                ce_loss = ce_criterion(outputs, subset_labels)\n",
    "                ce_losses.append(ce_loss)\n",
    "                \n",
    "                # Compute accuracy\n",
    "                _, predicted_labels = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted_labels == subset_labels).sum().item()\n",
    "                total_predictions += subset_labels.size(0)\n",
    "            \n",
    "            # Aggregate CE losses\n",
    "            ce_loss = torch.stack(ce_losses).mean()\n",
    "            \n",
    "            # Combined loss\n",
    "            combined_loss = mse_loss + ce_loss\n",
    "            \n",
    "            # Accumulate loss and accuracy\n",
    "            total_loss += combined_loss.item()\n",
    "            total_test_acc += (correct_predictions / total_predictions) * 100  # Percentage\n",
    "    \n",
    "    avg_test_loss = total_loss / num_batches\n",
    "    avg_test_acc = total_test_acc / num_batches\n",
    "    return avg_test_loss, avg_test_acc\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Utility Functions\n",
    "# ---------------------------\n",
    "def load_parameters_into_model(model, flat_params):\n",
    "    \"\"\"\n",
    "    Loads a flat parameter vector into the model's parameters.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        flat_params (Tensor): Flat parameter vector.\n",
    "    \"\"\"\n",
    "    current_index = 0\n",
    "    for param in model.parameters():\n",
    "        param_size = param.numel()\n",
    "        param.data.copy_(flat_params[current_index:current_index+param_size].view(param.size()))\n",
    "        current_index += param_size\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Main Execution\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    data_list = load_data(DATA_FOLDER)\n",
    "    print(f\"Total data-model pairs loaded: {len(data_list)}\\n\")\n",
    "    \n",
    "    if len(data_list) == 0:\n",
    "        print(\"No data found in the specified DATA_FOLDER. Please ensure that the folder contains .pt files.\")\n",
    "        return\n",
    "    \n",
    "    # Split data\n",
    "    train_data, test_data = split_data(data_list)\n",
    "    print(f\"Training data size: {len(train_data)}, Test data size: {len(test_data)}\\n\")\n",
    "    \n",
    "    # Create Datasets and DataLoaders\n",
    "    train_dataset = HypernetworkDataset(train_data)\n",
    "    test_dataset = HypernetworkDataset(test_data)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=0,  # Set to 0 to avoid CUDA initialization issues\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=0,  # Set to 0 to avoid CUDA initialization issues\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Initialize Hypernetwork\n",
    "    hypernetwork = MultiBranchHypernetwork(\n",
    "        num_examples=NUM_EXAMPLES,\n",
    "        per_example_input_size=PER_EXAMPLE_INPUT_SIZE,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        param_size=PARAM_SIZE\n",
    "    ).to(device)\n",
    "    \n",
    "    # Define loss functions and optimizer\n",
    "    mse_criterion = nn.MSELoss()\n",
    "    ce_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(hypernetwork.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        avg_loss, avg_train_acc = train_hypernetwork(\n",
    "            train_loader, hypernetwork, optimizer, mse_criterion, ce_criterion\n",
    "        )\n",
    "        avg_test_loss, avg_test_acc = evaluate_hypernetwork(\n",
    "            test_loader, hypernetwork, mse_criterion, ce_criterion\n",
    "        )\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] - Loss: {avg_loss:.6f}, \"\n",
    "              f\"Train Acc: {avg_train_acc:.2f}%, Test Acc: {avg_test_acc:.2f}%\")\n",
    "    \n",
    "    # Save the trained hypernetwork\n",
    "    save_path = 'trained_multi_branch_hypernetwork.pt'\n",
    "    torch.save({\n",
    "        'hypernetwork_state_dict': hypernetwork.state_dict(),\n",
    "    }, save_path)\n",
    "    print(f\"\\nHypernetwork training completed and saved to '{save_path}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4f036-abbb-44ca-924e-b5e461879afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

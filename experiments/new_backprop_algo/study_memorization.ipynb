{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eb659b-2b8e-40ed-92b0-10aa2373120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q seaborn tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b36431-1423-4914-89c4-17b7d2f3085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Data Preparation\n",
    "def load_mnist():\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    return train_dataset\n",
    "\n",
    "def create_unique_subsets(train_dataset, data_sizes, num_repeats):\n",
    "    \"\"\"\n",
    "    Creates unique subsets for each data_size and repeat.\n",
    "    Ensures no overlap between subsets across all repeats and data_sizes.\n",
    "    Returns a dictionary with keys as (data_size) and values as lists of (data, targets) tuples.\n",
    "    All data is stored as tensors on the GPU.\n",
    "    \"\"\"\n",
    "    total_required = sum(data_size * num_repeats for data_size in data_sizes)\n",
    "    if total_required > len(train_dataset):\n",
    "        raise ValueError(\"Not enough data to create unique subsets without overlap.\")\n",
    "    \n",
    "    indices = list(range(len(train_dataset)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    subsets = {data_size: [] for data_size in data_sizes}\n",
    "    \n",
    "    current_idx = 0\n",
    "    for data_size in tqdm(data_sizes, desc=\"Creating subsets\"):\n",
    "        for repeat in range(num_repeats):\n",
    "            subset_indices = indices[current_idx : current_idx + data_size]\n",
    "            current_idx += data_size\n",
    "            # Extract and flatten the images, move to GPU\n",
    "            subset_data = torch.stack([train_dataset[i][0].view(-1) for i in subset_indices]).to(device)\n",
    "            subset_targets = torch.tensor([train_dataset[i][1] for i in subset_indices], dtype=torch.long).to(device)\n",
    "            subsets[data_size].append((subset_data, subset_targets))\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "# 2. Model Definition\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, num_layers=1, output_size=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 3. Training and Evaluation\n",
    "def train_until_memorization(model, data, targets, criterion, optimizer, \n",
    "                             tolerance=1e-4, patience=10, delta=1e-5, max_epochs=1000):\n",
    "    \"\"\"\n",
    "    Trains the model until it perfectly memorizes the data or the loss stops improving.\n",
    "    Implements early stopping with patience and delta.\n",
    "    Returns True if memorization is achieved, else False.\n",
    "    \"\"\"\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss = loss.item()\n",
    "        \n",
    "        # Check for improvement\n",
    "        if best_loss - current_loss > delta:\n",
    "            best_loss = current_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        # Check for memorization\n",
    "        if current_loss < tolerance:\n",
    "            return True\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "def evaluate_memorization(model, data, targets):\n",
    "    \"\"\"\n",
    "    Evaluates whether the model has perfectly memorized the data.\n",
    "    Returns True if all predictions are correct, else False.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return torch.all(predicted == targets).item()\n",
    "\n",
    "# 4. Experiment Setup\n",
    "def run_experiment(subsets, data_sizes, model_layers, num_repeats=10):\n",
    "    \"\"\"\n",
    "    Runs the experiment across different data sizes and model layers.\n",
    "    Returns a results dictionary.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for data_size in tqdm(data_sizes, desc=\"Running experiments\"):\n",
    "        for repeat in range(num_repeats):\n",
    "            # Get the subset data and targets\n",
    "            subset_data, subset_targets = subsets[data_size][repeat]\n",
    "            \n",
    "            for num_layers in model_layers:\n",
    "                # Initialize model\n",
    "                model = SimpleNN(num_layers=num_layers).to(device)\n",
    "                \n",
    "                # Define loss and optimizer\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "                \n",
    "                # Train\n",
    "                memorized = train_until_memorization(\n",
    "                    model, subset_data, subset_targets, \n",
    "                    criterion, optimizer, \n",
    "                    tolerance=1e-4, patience=10, delta=1e-5, max_epochs=1000\n",
    "                )\n",
    "                \n",
    "                # If not memorized by training, evaluate\n",
    "                if not memorized:\n",
    "                    memorized = evaluate_memorization(model, subset_data, subset_targets)\n",
    "                \n",
    "                # Record the result\n",
    "                key = (data_size, num_layers)\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "                results[key].append(memorized)\n",
    "                \n",
    "                # Print summary\n",
    "                print(f\"Data size: {data_size}, Layers: {num_layers}, Repeat: {repeat+1}, Memorized: {memorized}\")\n",
    "    return results\n",
    "\n",
    "# 5. Visualization\n",
    "def visualize_results(results, data_sizes, model_layers, num_repeats=10):\n",
    "    \"\"\"\n",
    "    Creates a heatmap to visualize the relationship between model size and dataset size.\n",
    "    \"\"\"\n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = np.zeros((len(model_layers), len(data_sizes)))\n",
    "    \n",
    "    for i, num_layers in enumerate(model_layers):\n",
    "        for j, data_size in enumerate(data_sizes):\n",
    "            key = (data_size, num_layers)\n",
    "            # Calculate the proportion of repeats where memorization was successful\n",
    "            success_rate = sum(results.get(key, [])) / num_repeats\n",
    "            heatmap_data[i, j] = success_rate * 100  # Percentage\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(25, 12))\n",
    "    sns.heatmap(heatmap_data, annot=False, fmt=\".1f\", xticklabels=data_sizes, yticklabels=model_layers, cmap=\"YlGnBu\")\n",
    "    plt.xlabel(\"Dataset Size (Number of Examples)\", fontsize=14)\n",
    "    plt.ylabel(\"Model Size (Number of Layers)\", fontsize=14)\n",
    "    plt.title(\"Memorization Success Rate (%)\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    data_sizes = list(range(1, 101))  # 1-100 examples\n",
    "    model_layers = list(range(1, 11))  # 1-10 layers\n",
    "    num_repeats = 10  # 10 repeats per combination\n",
    "    \n",
    "    # Load data\n",
    "    train_dataset = load_mnist()\n",
    "    \n",
    "    # Pre-generate all subsets and load onto GPU\n",
    "    print(\"Pre-generating all subsets and loading onto GPU...\")\n",
    "    subsets = create_unique_subsets(train_dataset, data_sizes, num_repeats)\n",
    "    print(\"All subsets are loaded onto GPU.\")\n",
    "    \n",
    "    # Run experiment\n",
    "    results = run_experiment(subsets, data_sizes, model_layers, num_repeats=num_repeats)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results(results, data_sizes, model_layers, num_repeats=num_repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40feea-b16e-4add-9c35-36e03ef40706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c306cd-f7e3-46d4-922c-3c7e6d872ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Data Preparation\n",
    "def load_fashion_mnist():\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    return train_dataset\n",
    "\n",
    "def create_held_out_set(train_dataset, held_out_size=500):\n",
    "    \"\"\"\n",
    "    Creates a held-out test set that is never used in training.\n",
    "    Returns the held-out data and targets as tensors on the GPU.\n",
    "    \"\"\"\n",
    "    total_train = len(train_dataset)\n",
    "    if held_out_size > total_train:\n",
    "        raise ValueError(\"Held-out size exceeds the total training data size.\")\n",
    "    \n",
    "    indices = list(range(total_train))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    held_out_indices = indices[:held_out_size]\n",
    "    held_out_data = torch.stack([train_dataset[i][0].view(-1) for i in held_out_indices]).to(device)\n",
    "    held_out_targets = torch.tensor([train_dataset[i][1] for i in held_out_indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    return held_out_data, held_out_targets\n",
    "\n",
    "def sample_subset(train_dataset, data_size):\n",
    "    \"\"\"\n",
    "    Randomly samples a subset of the specified size from the training dataset.\n",
    "    Returns the subset data and targets as tensors on the GPU.\n",
    "    \"\"\"\n",
    "    total_train = len(train_dataset)\n",
    "    if data_size > total_train:\n",
    "        warnings.warn(f\"Requested data size ({data_size}) exceeds the available training data size ({total_train}). Capping to {total_train}.\")\n",
    "        data_size = total_train\n",
    "    \n",
    "    indices = random.sample(range(total_train), data_size)\n",
    "    subset_data = torch.stack([train_dataset[i][0].view(-1) for i in indices]).to(device)\n",
    "    subset_targets = torch.tensor([train_dataset[i][1] for i in indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    return subset_data, subset_targets\n",
    "\n",
    "# 2. Model Definition\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, num_layers=1, output_size=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 3. Training and Evaluation\n",
    "def train_until_memorization(model, data, targets, criterion, optimizer, \n",
    "                             tolerance=1e-4, patience=10, delta=1e-5, max_epochs=1000):\n",
    "    \"\"\"\n",
    "    Trains the model until it perfectly memorizes the data or the loss stops improving.\n",
    "    Implements early stopping with patience and delta.\n",
    "    Returns True if memorization is achieved, else False, along with final loss.\n",
    "    \"\"\"\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss = loss.item()\n",
    "        \n",
    "        # Check for improvement\n",
    "        if best_loss - current_loss > delta:\n",
    "            best_loss = current_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        # Check for memorization\n",
    "        if current_loss < tolerance:\n",
    "            return True, current_loss\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            return False, current_loss\n",
    "    \n",
    "    return False, current_loss\n",
    "\n",
    "def evaluate_memorization(model, data, targets):\n",
    "    \"\"\"\n",
    "    Evaluates whether the model has perfectly memorized the data.\n",
    "    Returns True if all predictions are correct, else False, along with accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == targets).sum().item()\n",
    "        total = targets.size(0)\n",
    "        accuracy = correct / total\n",
    "    return accuracy == 1.0, accuracy\n",
    "\n",
    "def evaluate_test_set(model, test_data, test_targets):\n",
    "    \"\"\"\n",
    "    Evaluates the model's accuracy on the held-out test set.\n",
    "    Returns the test accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == test_targets).sum().item()\n",
    "        total = test_targets.size(0)\n",
    "        accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# 4. Experiment Setup\n",
    "def run_experiment(train_dataset, held_out_data, held_out_targets, data_sizes, model_layers, \n",
    "                   step, max_size, tolerance=1e-4, patience=10, delta=1e-5, max_epochs=1000):\n",
    "    \"\"\"\n",
    "    Runs the experiment across different data sizes and model layers.\n",
    "    Returns a results dictionary with structure:\n",
    "    results[model_layers][data_size] = (final_loss, training_accuracy, test_accuracy)\n",
    "    \"\"\"\n",
    "    results = {layers: {} for layers in model_layers}\n",
    "    \n",
    "    # Define DataLoader for held-out test set (not used here since we directly use tensors)\n",
    "    # test_loader = DataLoader(TensorDataset(held_out_data, held_out_targets), batch_size=len(held_out_data))\n",
    "    \n",
    "    for data_size in tqdm([1] + list(range(100, max_size + 1, step)), desc=\"Data Sizes\"):\n",
    "        for num_layers in model_layers:\n",
    "            # Sample a random subset\n",
    "            subset_data, subset_targets = sample_subset(train_dataset, data_size)\n",
    "            \n",
    "            # Initialize model\n",
    "            model = SimpleNN(num_layers=num_layers).to(device)\n",
    "            \n",
    "            # Define loss and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "            \n",
    "            # Train\n",
    "            memorized, final_loss = train_until_memorization(\n",
    "                model, subset_data, subset_targets, \n",
    "                criterion, optimizer, \n",
    "                tolerance=tolerance, patience=patience, delta=delta, max_epochs=max_epochs\n",
    "            )\n",
    "            \n",
    "            # If not memorized by training, evaluate\n",
    "            if not memorized:\n",
    "                memorized, training_accuracy = evaluate_memorization(model, subset_data, subset_targets)\n",
    "                if memorized:\n",
    "                    training_accuracy = 1.0\n",
    "                else:\n",
    "                    training_accuracy = training_accuracy\n",
    "            else:\n",
    "                training_accuracy = 1.0  # Memorization achieved\n",
    "            \n",
    "            # Evaluate on held-out test set\n",
    "            test_accuracy = evaluate_test_set(model, held_out_data, held_out_targets)\n",
    "            \n",
    "            # Record the result\n",
    "            results[num_layers][data_size] = (final_loss, training_accuracy, test_accuracy)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"Layers: {num_layers}, Data size: {data_size}, Memorized: {memorized}, \"\n",
    "                  f\"Final Loss: {final_loss:.6f}, Training Acc: {training_accuracy*100:.2f}%, \"\n",
    "                  f\"Test Acc: {test_accuracy*100:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 5. Visualization\n",
    "def visualize_results(results, data_sizes, model_layers, step, max_size):\n",
    "    \"\"\"\n",
    "    Creates two plots (1-layer and 2-layer) showing loss, training accuracy, and test accuracy vs. data size.\n",
    "    \"\"\"\n",
    "    for num_layers in model_layers:\n",
    "        losses = []\n",
    "        training_accuracies = []\n",
    "        test_accuracies = []\n",
    "        sizes = []\n",
    "        for data_size in [1] + list(range(100, max_size + 1, step)):\n",
    "            loss, train_acc, test_acc = results[num_layers][data_size]\n",
    "            sizes.append(data_size)\n",
    "            losses.append(loss)\n",
    "            training_accuracies.append(train_acc * 100)  # Convert to percentage\n",
    "            test_accuracies.append(test_acc * 100)        # Convert to percentage\n",
    "        \n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        color = 'tab:blue'\n",
    "        ax1.set_xlabel('Dataset Size (Number of Examples)', fontsize=12)\n",
    "        ax1.set_ylabel('Final Loss', color=color, fontsize=12)\n",
    "        ax1.plot(sizes, losses, marker='o', color=color, label='Final Loss')\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax1.set_title(f'Memorization Performance for {num_layers}-Layer Model', fontsize=14)\n",
    "        \n",
    "        ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "        \n",
    "        # Plot Training and Test Accuracy\n",
    "        color = 'tab:red'\n",
    "        ax2.set_ylabel('Accuracy (%)', color=color, fontsize=12)  # we already handled the x-label with ax1\n",
    "        ax2.plot(sizes, training_accuracies, marker='x', color='tab:green', label='Training Accuracy')\n",
    "        ax2.plot(sizes, test_accuracies, marker='s', color=color, label='Test Accuracy')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        \n",
    "        # Legends\n",
    "        lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "        lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper right')\n",
    "        \n",
    "        fig.tight_layout()  # Otherwise the right y-label is slightly clipped\n",
    "        plt.show()\n",
    "\n",
    "# 6. Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    model_layers = [1, 2]  # 1-2 layers\n",
    "    data_sizes = [1] + list(range(100, 1001, 100))  # 1, 100, 200, ..., 1000\n",
    "    step = 5000  # Jump size for data sizes\n",
    "    max_size = 1000000  # Maximum data size\n",
    "    held_out_size = 5000  # Held-out test set size\n",
    "    tolerance = 1e-4  # Loss tolerance for memorization\n",
    "    patience = 10  # Early stopping patience\n",
    "    delta = 1e-5  # Minimum loss improvement\n",
    "    max_epochs = 1000  # Maximum number of epochs\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading Fashion-MNIST dataset...\")\n",
    "    train_dataset = load_fashion_mnist()\n",
    "    \n",
    "    # Create held-out test set\n",
    "    print(\"Creating held-out test set...\")\n",
    "    held_out_data, held_out_targets = create_held_out_set(train_dataset, held_out_size=held_out_size)\n",
    "    print(f\"Held-out test set created with {held_out_size} examples.\\n\")\n",
    "    \n",
    "    # Run experiment\n",
    "    results = run_experiment(\n",
    "        train_dataset, \n",
    "        held_out_data, \n",
    "        held_out_targets, \n",
    "        data_sizes, \n",
    "        model_layers, \n",
    "        step, \n",
    "        max_size, \n",
    "        tolerance=tolerance, \n",
    "        patience=patience, \n",
    "        delta=delta, \n",
    "        max_epochs=max_epochs\n",
    "    )\n",
    "    \n",
    "    # Visualization\n",
    "    visualize_results(results, data_sizes, model_layers, step, max_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c21de1-33a2-42d9-8414-f718653ac9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

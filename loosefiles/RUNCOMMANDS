#file input location
Download model:
aws s3 cp s3://sliced-models/llama-2-7b/ /home/ec2-user/environment --recursive
s3://slice-prompt-store/auto-character-generation.json

Set distributed variables:
export RANK=0
export WORLD_SIZE=1
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500
python fine-tune.py


#llama7b chat
torchrun --nproc_per_node 1 generate.py \
--bucket slice-prompt-store \
--key auto-character-generation-age-specific.json \
--download_path ./downloaded_file.json \
--api_key sk- \
--use_llama \
--fast_run True \
--ckpt_dir ./model/llama-2-7b-chat \
--tokenizer_path ./model/tokenizer.model 

torchrun --nproc_per_node 1 generate.py \
--bucket slice-prompt-store \
--key auto-character-generation-age-specific.json \
--download_path ./downloaded_file.json \
--api_key sk- \
--use_llama \
--ckpt_dir ./model/llama-2-7b-chat \
--tokenizer_path ./model/tokenizer.model 

FAST_RUN=false bash run.sh 

#CLEAN DOCKER
docker system prune -af

docker exec -it $(docker ps -q | head -n 1) /bin/bash
docker logs $(docker ps -q | head -n 1)

docker build -t generate_tune .
docker run --gpus all -it generate_tune conda run -n finetune bash run.sh

PYTHONUNBUFFERED=1
docker run --gpus all -e -it generate_tune conda run -n finetune bash run.sh

docker run --gpus all -e FAST_RUN=false -it generate_tune conda run -n finetune bash run.sh


./run.sh -b "slice-prompt-store" \
-k "auto-character-generation.json" \
-d "./downloaded_file.json" \
-m "text-davinci-002" \
-a "sk-addkeyhere" \
-t "0.85" -x "7000" \
-l -f -c "./model/llama-2-7b-chat" \
-z "./model/tokenizer.model" \


python generate.py \
--bucket slice-prompt-store \
--key auto-character-generation.json \
--download_path ./downloaded_file.json \
--model_engine gpt-4 \
--api_key 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

#chatgpt
python generate.py \
--bucket slice-prompt-store \
--key auto-character-generation.json \
--download_path ./downloaded_file.json \
--api_key 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 \
--model_engine gpt-4

# llama 7b
python generate.py \
--bucket slice-prompt-store \
--key auto-character-generation.json \
--download_path ./downloaded_file.json \
--api_key sk- \
--use_llama \
--ckpt_dir /home/ec2-user/environment/models/llama-2-7b \
--tokenizer_path /home/ec2-user/environment/model_training/llama-main/tokenizer.model



INSTALL NVIDIA

https://www.nvidia.com/Download/index.aspx
chmod +x NVIDIA
sudo sh NVIDA


ssh -i "gputesting.pem" ubuntu@ec2-52-13-240-69.us-west-2.compute.amazonaws.com


SETUP:
source ~/.bashrc
conda create --name llama_env python=3.8
conda activate llama_env
pip install -r requirements.txt


torchrun --nproc_per_node 1 tune.py \
    --ckpt_dir /home/ec2-user/environment/models/llama-2-7b-chat/ \
    --tokenizer_path /home/ec2-user/environment/models/tokenizer.model \
    --max_seq_len 512 --max_batch_size 6
    
    
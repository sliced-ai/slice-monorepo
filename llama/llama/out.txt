
CUDA AVAILABLE: True

Data Point 1: {'input_ids': tensor([ 156, 9492, 9819, 8751, 7275, 7670, 9019, 3913, 6370, 2736, 7755, 6914,
        7593,  512,  858, 6121, 6433, 5270,  921, 8555], device='cuda:0'), 'labels': tensor([ 157, 9493, 9820, 8752, 7276, 7671, 9020, 3914, 6371, 2737, 7756, 6915,
        7594,  513,  859, 6122, 6434, 5271,  922, 8556], device='cuda:0'), 'mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True], device='cuda:0')}
Data Point 2: {'input_ids': tensor([7729, 3045, 5312,  422, 9012,  986, 4156, 4856,  263, 9283,  708, 9982,
        5575, 4516, 6074, 4448, 8423, 7613, 5372, 5046], device='cuda:0'), 'labels': tensor([7730, 3046, 5313,  423, 9013,  987, 4157, 4857,  264, 9284,  709, 9983,
        5576, 4517, 6075, 4449, 8424, 7614, 5373, 5047], device='cuda:0'), 'mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True], device='cuda:0')}
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 7.15 seconds
Iteration 1/10
Loss: 2.9056572914123535
Outputs: tensor([[[ -0.6313,  -1.5947,   0.4395,  ...,   0.9033,   1.4570,  -0.0369],
         [ -6.5273,  -8.8750,  -1.8057,  ...,  -5.0703,  -5.5898,  -7.1641],
         [ -8.2109, -10.8828,  -1.0205,  ...,  -7.0586,  -6.7461,  -5.9297],
         ...,
         [ -5.9297,  -5.7852,   0.5771,  ...,  -3.3066,  -5.6250,  -4.2344],
         [ -3.8730,  -2.6738,   1.0869,  ...,  -1.5479,  -2.3281,  -1.0537],
         [ -5.5000,  -4.7930,  -0.4353,  ...,  -5.6367,  -1.7549,  -3.0000]],

        [[ -0.3845,  -1.4648,   0.2399,  ...,   0.7456,   1.5205,   0.1974],
         [-10.1562, -10.7891,   0.5996,  ...,  -6.3711,  -6.9609,  -8.0156],
         [ -4.4141,  -5.3633,   3.4395,  ...,  -4.2578,  -4.7500,  -3.5605],
         ...,
         [ -4.8398,  -4.6328,   4.0938,  ...,  -4.0508,  -2.5605,  -2.8242],
         [ -5.8438,  -2.6484,   1.2666,  ...,  -3.7188,  -4.7656,  -3.9414],
         [ -6.9648,  -5.2461,   1.7744,  ...,  -4.7734,  -4.7617,  -3.9395]],

        [[ -0.3335,  -1.9756,   0.0918,  ...,   1.0996,   1.5117,   0.4636],
         [ -6.6406,  -6.9375,   5.7891,  ...,  -3.3438,  -5.9219,  -3.5566],
         [ -8.1094,  -6.7578,  -0.7681,  ...,  -5.1992,  -7.2852,  -7.0195],
         ...,
         [ -3.2871,  -4.0078,   0.5186,  ...,  -2.5098,  -2.5430,  -2.5918],
         [ -2.3438,  -3.8047,   1.4404,  ...,  -1.1602,  -1.7617,  -1.9365],
         [ -4.1836,  -5.2031,   0.1677,  ...,  -2.4512,  -3.6973,  -3.9531]],

        [[ -0.3545,  -1.6338,   0.2396,  ...,   0.8921,   1.3545,   0.4004],
         [ -9.2109,  -8.6797,   0.0155,  ...,  -7.2266,  -7.8086,  -6.5391],
         [ -5.3750,  -6.3711,   0.7769,  ...,  -2.6035,  -4.8945,  -3.2168],
         ...,
         [ -5.5234,  -2.8691,   3.5703,  ...,  -4.1055,  -2.4531,  -3.4160],
         [ -5.7461,  -2.7656,   1.6562,  ...,  -4.9648,  -6.0156,  -5.1211],
         [ -3.7422,  -2.0527,   2.1699,  ...,  -2.1348,  -3.3047,  -3.6445]]],
       dtype=torch.float32, grad_fn=<ToCopyBackward0>)
Gradient Monitoring:
Parameter: layers.30.attention.wq.weight
    Gradient Mean: 0.0
    Gradient Std: 7.218122482299805e-05
    Gradient Min: -0.024444580078125
    Gradient Max: 0.03839111328125
    Gradient Norm: 0.295654296875
    Gradient Sparsity: 0.0026284456253051758
---
Parameter: layers.30.attention.wk.weight
    Gradient Mean: 0.0
    Gradient Std: 7.611513137817383e-05
    Gradient Min: -0.037689208984375
    Gradient Max: 0.04339599609375
    Gradient Norm: 0.311767578125
    Gradient Sparsity: 0.002800881862640381
---
Parameter: layers.30.attention.wv.weight
    Gradient Mean: 0.0
    Gradient Std: 0.00015234947204589844
    Gradient Min: -0.057159423828125
    Gradient Max: 0.054901123046875
    Gradient Norm: 0.6240234375
    Gradient Sparsity: 0.0008519887924194336
---
Parameter: layers.30.attention.wo.weight
    Gradient Mean: 0.0
    Gradient Std: 8.159875869750977e-05
    Gradient Min: -0.0096893310546875
    Gradient Max: 0.00797271728515625
    Gradient Norm: 0.334228515625
    Gradient Sparsity: 0.0009070634841918945
---
Parameter: layers.30.feed_forward.w1.weight
    Gradient Mean: -0.0
    Gradient Std: 8.970499038696289e-05
    Gradient Min: -0.03607177734375
    Gradient Max: 0.028564453125
    Gradient Norm: 0.6025390625
    Gradient Sparsity: 0.0005055069923400879
---
Parameter: layers.30.feed_forward.w2.weight
    Gradient Mean: 0.0
    Gradient Std: 0.00021028518676757812
    Gradient Min: -0.0606689453125
    Gradient Max: 0.056365966796875
    Gradient Norm: 1.412109375
    Gradient Sparsity: 0.0003860592842102051
---
Parameter: layers.30.feed_forward.w3.weight
    Gradient Mean: 0.0
    Gradient Std: 0.00012058019638061523
    Gradient Min: -0.058135986328125
    Gradient Max: 0.0517578125
    Gradient Norm: 0.8095703125
    Gradient Sparsity: 0.00048172473907470703
---
Parameter: layers.30.attention_norm.weight
    Gradient Mean: 1.1086463928222656e-05
    Gradient Std: 0.0008921623229980469
    Gradient Min: -0.00981903076171875
    Gradient Max: 0.0498046875
    Gradient Norm: 0.057098388671875
    Gradient Sparsity: 0.00048828125
---
Parameter: layers.30.ffn_norm.weight
    Gradient Mean: 8.738040924072266e-05
    Gradient Std: 0.0019779205322265625
    Gradient Min: -0.05364990234375
    Gradient Max: 0.10247802734375
    Gradient Norm: 0.126708984375
    Gradient Sparsity: 0.0
---
Parameter: layers.31.attention.wq.weight
    Gradient Mean: -0.0
    Gradient Std: 8.744001388549805e-05
    Gradient Min: -0.05450439453125
    Gradient Max: 0.04010009765625
    Gradient Norm: 0.358154296875
    Gradient Sparsity: 0.002288341522216797
---
Parameter: layers.31.attention.wk.weight
    Gradient Mean: -0.0
    Gradient Std: 0.00013685226440429688
    Gradient Min: -0.152099609375
    Gradient Max: 0.117919921875
    Gradient Norm: 0.560546875
    Gradient Sparsity: 0.0025210976600646973
---
Parameter: layers.31.attention.wv.weight
    Gradient Mean: -0.0
    Gradient Std: 0.0002256631851196289
    Gradient Min: -0.103759765625
    Gradient Max: 0.12457275390625
    Gradient Norm: 0.92431640625
    Gradient Sparsity: 0.0005763769149780273
---
Parameter: layers.31.attention.wo.weight
    Gradient Mean: -0.0
    Gradient Std: 0.00010633468627929688
    Gradient Min: -0.0070037841796875
    Gradient Max: 0.005458831787109375
    Gradient Norm: 0.435546875
    Gradient Sparsity: 0.000553131103515625
---
Parameter: layers.31.feed_forward.w1.weight
    Gradient Mean: 0.0
    Gradient Std: 9.614229202270508e-05
    Gradient Min: -0.047088623046875
    Gradient Max: 0.09442138671875
    Gradient Norm: 0.6455078125
    Gradient Sparsity: 0.0005625486373901367
---
Parameter: layers.31.feed_forward.w2.weight
    Gradient Mean: -0.0
    Gradient Std: 0.0002493858337402344
    Gradient Min: -0.050933837890625
    Gradient Max: 0.054534912109375
    Gradient Norm: 1.6748046875
    Gradient Sparsity: 0.00037795305252075195
---
Parameter: layers.31.feed_forward.w3.weight
    Gradient Mean: -0.0
    Gradient Std: 0.00014257431030273438
    Gradient Min: -0.054840087890625
    Gradient Max: 0.08941650390625
    Gradient Norm: 0.95703125
    Gradient Sparsity: 0.000535130500793457
---
Parameter: layers.31.attention_norm.weight
    Gradient Mean: -2.580881118774414e-05
    Gradient Std: 0.0027332305908203125
    Gradient Min: -0.1646728515625
    Gradient Max: 0.0285797119140625
    Gradient Norm: 0.1749267578125
    Gradient Sparsity: 0.000244140625
---
Parameter: layers.31.ffn_norm.weight
    Gradient Mean: -0.0003669261932373047
    Gradient Std: 0.007720947265625
    Gradient Min: -0.158203125
    Gradient Max: 0.43212890625
    Gradient Norm: 0.49462890625
    Gradient Sparsity: 0.000244140625
---
Parameter: norm.weight
    Gradient Mean: 0.00019693374633789062
    Gradient Std: 0.002712249755859375
    Gradient Min: -0.002162933349609375
    Gradient Max: 0.1474609375
    Gradient Norm: 0.1739501953125
    Gradient Sparsity: 0.0
---
Parameter: output.weight
    Gradient Mean: -0.0
    Gradient Std: 0.0002315044403076172
    Gradient Min: -0.2464599609375
    Gradient Max: 0.2117919921875
    Gradient Norm: 2.650390625
    Gradient Sparsity: 0.10109257698059082
---

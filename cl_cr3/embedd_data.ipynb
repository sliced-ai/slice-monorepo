{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982dc479-c4b3-496d-820f-f66265e9b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q openai h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c950e4f-7a52-41d2-9940-05c745dd623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processed 330 files and extracted 865274 utterances.\n",
      "Chunk: 0/1689\n",
      "Chunk: 1/1689\n",
      "Chunk: 2/1689\n",
      "Chunk: 3/1689\n",
      "Chunk: 4/1689\n",
      "Chunk: 5/1689\n",
      "Chunk: 6/1689\n",
      "Chunk: 7/1689\n",
      "Chunk: 8/1689\n",
      "Chunk: 9/1689\n",
      "Chunk: 10/1689\n",
      "Chunk: 11/1689\n",
      "Chunk: 12/1689\n",
      "Chunk: 13/1689\n",
      "Chunk: 14/1689\n",
      "Chunk: 15/1689\n",
      "Chunk: 16/1689\n",
      "Chunk: 17/1689\n",
      "Chunk: 18/1689\n",
      "Chunk: 19/1689\n",
      "Chunk: 20/1689\n",
      "Chunk: 21/1689\n",
      "Chunk: 22/1689\n",
      "Chunk: 23/1689\n",
      "Chunk: 24/1689\n",
      "Chunk: 25/1689\n",
      "Chunk: 26/1689\n",
      "Chunk: 27/1689\n",
      "Chunk: 28/1689\n",
      "Chunk: 29/1689\n",
      "Chunk: 30/1689\n",
      "Chunk: 31/1689\n",
      "Chunk: 32/1689\n",
      "Chunk: 33/1689\n",
      "Chunk: 34/1689\n",
      "Chunk: 35/1689\n",
      "Chunk: 36/1689\n",
      "Chunk: 37/1689\n",
      "Chunk: 38/1689\n",
      "Chunk: 39/1689\n",
      "Chunk: 40/1689\n",
      "Chunk: 41/1689\n",
      "Chunk: 42/1689\n",
      "Chunk: 43/1689\n",
      "Chunk: 44/1689\n",
      "Chunk: 45/1689\n",
      "Chunk: 46/1689\n",
      "Chunk: 47/1689\n",
      "Chunk: 48/1689\n",
      "Chunk: 49/1689\n",
      "Chunk: 50/1689\n",
      "Chunk: 51/1689\n",
      "Chunk: 52/1689\n",
      "Chunk: 53/1689\n",
      "Chunk: 54/1689\n",
      "Chunk: 55/1689\n",
      "Chunk: 56/1689\n",
      "Chunk: 57/1689\n",
      "Chunk: 58/1689\n",
      "Chunk: 59/1689\n",
      "Chunk: 60/1689\n",
      "Chunk: 61/1689\n",
      "Chunk: 62/1689\n",
      "Chunk: 63/1689\n",
      "Chunk: 64/1689\n",
      "Chunk: 65/1689\n",
      "Chunk: 66/1689\n",
      "Chunk: 67/1689\n",
      "Chunk: 68/1689\n",
      "Chunk: 69/1689\n",
      "Chunk: 70/1689\n",
      "Chunk: 71/1689\n",
      "Chunk: 72/1689\n",
      "Chunk: 73/1689\n",
      "Chunk: 74/1689\n",
      "Chunk: 75/1689\n",
      "Chunk: 76/1689\n",
      "Chunk: 77/1689\n",
      "Chunk: 78/1689\n",
      "Chunk: 79/1689\n",
      "Chunk: 80/1689\n",
      "Chunk: 81/1689\n",
      "Chunk: 82/1689\n",
      "Chunk: 83/1689\n",
      "Chunk: 84/1689\n",
      "Chunk: 85/1689\n",
      "Chunk: 86/1689\n",
      "Chunk: 87/1689\n",
      "Chunk: 88/1689\n",
      "Chunk: 89/1689\n",
      "Chunk: 90/1689\n",
      "Chunk: 91/1689\n",
      "Chunk: 92/1689\n",
      "Chunk: 93/1689\n",
      "Chunk: 94/1689\n",
      "Chunk: 95/1689\n",
      "Chunk: 96/1689\n",
      "Chunk: 97/1689\n",
      "Chunk: 98/1689\n",
      "Chunk: 99/1689\n",
      "Chunk: 100/1689\n",
      "Chunk: 101/1689\n",
      "Chunk: 102/1689\n",
      "Chunk: 103/1689\n",
      "Chunk: 104/1689\n",
      "Chunk: 105/1689\n",
      "Chunk: 106/1689\n",
      "Chunk: 107/1689\n",
      "Chunk: 108/1689\n",
      "Chunk: 109/1689\n",
      "Chunk: 110/1689\n",
      "Chunk: 111/1689\n",
      "Chunk: 112/1689\n",
      "Chunk: 113/1689\n",
      "Chunk: 114/1689\n",
      "Chunk: 115/1689\n",
      "Chunk: 116/1689\n",
      "Chunk: 117/1689\n",
      "Chunk: 118/1689\n",
      "Chunk: 119/1689\n",
      "Chunk: 120/1689\n",
      "Chunk: 121/1689\n",
      "Chunk: 122/1689\n",
      "Chunk: 123/1689\n",
      "Chunk: 124/1689\n",
      "Chunk: 125/1689\n",
      "Chunk: 126/1689\n",
      "Chunk: 127/1689\n",
      "Chunk: 128/1689\n",
      "Chunk: 129/1689\n",
      "Chunk: 130/1689\n",
      "Chunk: 131/1689\n",
      "Chunk: 132/1689\n",
      "Chunk: 133/1689\n",
      "Chunk: 134/1689\n",
      "Chunk: 135/1689\n",
      "Chunk: 136/1689\n",
      "Chunk: 137/1689\n",
      "Chunk: 138/1689\n",
      "Chunk: 139/1689\n",
      "Chunk: 140/1689\n",
      "Chunk: 141/1689\n",
      "Chunk: 142/1689\n",
      "Chunk: 143/1689\n",
      "Chunk: 144/1689\n",
      "Chunk: 145/1689\n",
      "Chunk: 146/1689\n",
      "Chunk: 147/1689\n",
      "Chunk: 148/1689\n",
      "Chunk: 149/1689\n",
      "Chunk: 150/1689\n",
      "Chunk: 151/1689\n",
      "Chunk: 152/1689\n",
      "Chunk: 153/1689\n",
      "Chunk: 154/1689\n",
      "Chunk: 155/1689\n",
      "Chunk: 156/1689\n",
      "Chunk: 157/1689\n",
      "Chunk: 158/1689\n",
      "Chunk: 159/1689\n",
      "Chunk: 160/1689\n",
      "Chunk: 161/1689\n",
      "Chunk: 162/1689\n",
      "Chunk: 163/1689\n",
      "Chunk: 164/1689\n",
      "Chunk: 165/1689\n",
      "Chunk: 166/1689\n",
      "Chunk: 167/1689\n",
      "Chunk: 168/1689\n",
      "Chunk: 169/1689\n",
      "Chunk: 170/1689\n",
      "Chunk: 171/1689\n",
      "Chunk: 172/1689\n",
      "Chunk: 173/1689\n",
      "Chunk: 174/1689\n",
      "Chunk: 175/1689\n",
      "Chunk: 176/1689\n",
      "Chunk: 177/1689\n",
      "Chunk: 178/1689\n",
      "Chunk: 179/1689\n",
      "Chunk: 180/1689\n",
      "Chunk: 181/1689\n",
      "Chunk: 182/1689\n",
      "Chunk: 183/1689\n",
      "Chunk: 184/1689\n",
      "Chunk: 185/1689\n",
      "Chunk: 186/1689\n",
      "Chunk: 187/1689\n",
      "Chunk: 188/1689\n",
      "Chunk: 189/1689\n",
      "Chunk: 190/1689\n",
      "Chunk: 191/1689\n",
      "Chunk: 192/1689\n",
      "Chunk: 193/1689\n",
      "Chunk: 194/1689\n",
      "Chunk: 195/1689\n",
      "Chunk: 196/1689\n",
      "Chunk: 197/1689\n",
      "Chunk: 198/1689\n",
      "Chunk: 199/1689\n",
      "Chunk: 200/1689\n",
      "Chunk: 201/1689\n",
      "Chunk: 202/1689\n",
      "Chunk: 203/1689\n",
      "Chunk: 204/1689\n",
      "Chunk: 205/1689\n",
      "Chunk: 206/1689\n",
      "Chunk: 207/1689\n",
      "Chunk: 208/1689\n",
      "Chunk: 209/1689\n",
      "Chunk: 210/1689\n",
      "Chunk: 211/1689\n",
      "Chunk: 212/1689\n",
      "Chunk: 213/1689\n",
      "Chunk: 214/1689\n",
      "Chunk: 215/1689\n",
      "Chunk: 216/1689\n",
      "Chunk: 217/1689\n",
      "Chunk: 218/1689\n",
      "Chunk: 219/1689\n",
      "Chunk: 220/1689\n",
      "Chunk: 221/1689\n",
      "Chunk: 222/1689\n",
      "Chunk: 223/1689\n",
      "Chunk: 224/1689\n",
      "Chunk: 225/1689\n",
      "Chunk: 226/1689\n",
      "Chunk: 227/1689\n",
      "Chunk: 228/1689\n",
      "Chunk: 229/1689\n",
      "Chunk: 230/1689\n",
      "Chunk: 231/1689\n",
      "Chunk: 232/1689\n",
      "Chunk: 233/1689\n",
      "Chunk: 234/1689\n",
      "Chunk: 235/1689\n",
      "Chunk: 236/1689\n",
      "Chunk: 237/1689\n",
      "Chunk: 238/1689\n",
      "Chunk: 239/1689\n",
      "Chunk: 240/1689\n",
      "Chunk: 241/1689\n",
      "Chunk: 242/1689\n",
      "Chunk: 243/1689\n",
      "Chunk: 244/1689\n",
      "Chunk: 245/1689\n",
      "Chunk: 246/1689\n",
      "Chunk: 247/1689\n",
      "Chunk: 248/1689\n",
      "Chunk: 249/1689\n",
      "Chunk: 250/1689\n",
      "Chunk: 251/1689\n",
      "Chunk: 252/1689\n",
      "Chunk: 253/1689\n",
      "Chunk: 254/1689\n",
      "Chunk: 255/1689\n",
      "Chunk: 256/1689\n",
      "Chunk: 257/1689\n",
      "Chunk: 258/1689\n",
      "Chunk: 259/1689\n",
      "Chunk: 260/1689\n",
      "Chunk: 261/1689\n",
      "Chunk: 262/1689\n",
      "Chunk: 263/1689\n",
      "Chunk: 264/1689\n",
      "Chunk: 265/1689\n",
      "Chunk: 266/1689\n",
      "Chunk: 267/1689\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import h5py\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI API key\n",
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "# Function to process a JSON file and extract the relevant data\n",
    "def extract_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    extracted_data = []\n",
    "    for document in data:\n",
    "        for turn in document['TURNS']:\n",
    "            for name in turn['NAMES']:\n",
    "                extracted_data.append({\n",
    "                    'name': name,\n",
    "                    'utterance': ' '.join(turn['UTTERANCES']),\n",
    "                    'turn_number': turn['NUMBER'],\n",
    "                    'file_path': file_path\n",
    "                })\n",
    "    return extracted_data\n",
    "\n",
    "# Wrapper function to enable multiprocessing\n",
    "def extract_data_wrapper(args):\n",
    "    return extract_data(*args)\n",
    "\n",
    "# Load data from JSON files in the specified folder and its subdirectories using multiprocessing\n",
    "def load_data(folder_path, num_workers=4):\n",
    "    all_texts = []\n",
    "    all_metadata = []\n",
    "    file_paths = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "\n",
    "    with multiprocessing.Pool(num_workers) as pool:\n",
    "        results = pool.map(extract_data_wrapper, [(file_path,) for file_path in file_paths])\n",
    "\n",
    "    for result in results:\n",
    "        for item in result:\n",
    "            all_texts.append(item['utterance'])\n",
    "            all_metadata.append({\n",
    "                'name': item['name'],\n",
    "                'turn_number': item['turn_number'],\n",
    "                'file_path': item['file_path']\n",
    "            })\n",
    "\n",
    "    print(f\"Processed {len(file_paths)} files and extracted {len(all_texts)} utterances.\")\n",
    "    return all_texts, all_metadata\n",
    "\n",
    "# Define the Embedding System\n",
    "class EmbeddingSystem:\n",
    "    def __init__(self, api_key, embedding_model):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = embedding_model\n",
    "\n",
    "    def create_embeddings(self, texts, num_workers=4):\n",
    "        manager = multiprocessing.Manager()\n",
    "        return_dict = manager.dict()\n",
    "        jobs = []\n",
    "\n",
    "        total_texts = len(texts)\n",
    "        chunk_size = 5  # Adjust this number based on your needs\n",
    "\n",
    "        #print(f\"Total texts to embed: {total_texts}\")\n",
    "        total_processes = (total_texts + chunk_size - 1) // chunk_size\n",
    "        #print(f\"Running embedding with {total_processes} processes.\")\n",
    "\n",
    "        current_process = 0\n",
    "        for i in range(0, total_texts, chunk_size):\n",
    "            texts_chunk = texts[i:i + chunk_size]\n",
    "\n",
    "            p = multiprocessing.Process(target=self.worker, args=(texts_chunk, return_dict, current_process))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            current_process += 1\n",
    "\n",
    "        for proc in jobs:\n",
    "            proc.join()\n",
    "\n",
    "        embeddings = []\n",
    "        for result in return_dict.values():\n",
    "            embeddings.extend(result)\n",
    "\n",
    "        #print(f\"Total embeddings generated: {len(embeddings)}\")\n",
    "        return embeddings\n",
    "\n",
    "    def worker(self, texts, return_dict, index):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            while True:\n",
    "                try:\n",
    "                    embedding = self.get_embedding(text)\n",
    "                    embeddings.append(embedding)\n",
    "                    break\n",
    "                except openai.RateLimitError:\n",
    "                    wait_time = random.uniform(1, 60)  # Randomized wait time\n",
    "                    #print(f\"Rate limit hit. Process {index} waiting for {wait_time} seconds.\")\n",
    "                    time.sleep(wait_time)\n",
    "                except openai.APIError as e:\n",
    "                    print(f\"OpenAI API returned an API Error: {e}\")\n",
    "                    break\n",
    "                except openai.APIConnectionError as e:\n",
    "                    print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "                    time.sleep(2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error in process {index}: {e}\")\n",
    "                    break\n",
    "        return_dict[index] = embeddings\n",
    "        #print(f\"Process {index} completed with {len(texts)} texts.\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        return self.client.embeddings.create(input=[text], model=self.model).data[0].embedding\n",
    "\n",
    "def save_embeddings(embeddings, metadata, filename, model_name):\n",
    "    with h5py.File(filename, 'a') as f:\n",
    "        # Append new embeddings\n",
    "        if 'embeddings' not in f:\n",
    "            f.create_dataset('embeddings', data=embeddings, maxshape=(None, len(embeddings[0])))\n",
    "            names_encoded = [meta['name'].encode('utf8') for meta in metadata]\n",
    "            turn_numbers = [meta['turn_number'] for meta in metadata]\n",
    "            file_paths_encoded = [meta['file_path'].encode('utf8') for meta in metadata]\n",
    "            model_names_encoded = [model_name.encode('utf8') for _ in metadata]\n",
    "            f.create_dataset('names', data=names_encoded, maxshape=(None,))\n",
    "            f.create_dataset('turn_numbers', data=turn_numbers, maxshape=(None,))\n",
    "            f.create_dataset('file_paths', data=file_paths_encoded, maxshape=(None,))\n",
    "            f.create_dataset('model_names', data=model_names_encoded, maxshape=(None,))\n",
    "        else:\n",
    "            f['embeddings'].resize((f['embeddings'].shape[0] + len(embeddings)), axis=0)\n",
    "            f['embeddings'][-len(embeddings):] = embeddings\n",
    "            names_encoded = [meta['name'].encode('utf8') for meta in metadata]\n",
    "            turn_numbers = [meta['turn_number'] for meta in metadata]\n",
    "            file_paths_encoded = [meta['file_path'].encode('utf8') for meta in metadata]\n",
    "            model_names_encoded = [model_name.encode('utf8') for _ in metadata]\n",
    "            f['names'].resize((f['names'].shape[0] + len(names_encoded)), axis=0)\n",
    "            f['names'][-len(names_encoded):] = names_encoded\n",
    "            f['turn_numbers'].resize((f['turn_numbers'].shape[0] + len(turn_numbers)), axis=0)\n",
    "            f['turn_numbers'][-len(turn_numbers):] = turn_numbers\n",
    "            f['file_paths'].resize((f['file_paths'].shape[0] + len(file_paths_encoded)), axis=0)\n",
    "            f['file_paths'][-len(file_paths_encoded):] = file_paths_encoded\n",
    "            f['model_names'].resize((f['model_names'].shape[0] + len(model_names_encoded)), axis=0)\n",
    "            f['model_names'][-len(model_names_encoded):] = model_names_encoded\n",
    "\n",
    "    #print(f\"Chunk processed and saved.\")\n",
    "\n",
    "# Worker function for multiprocessing\n",
    "def embedding_worker(texts_chunk, metadata_chunk, model_config, api_key, chunk_index, output_file, lock):\n",
    "    embedding_system = EmbeddingSystem(api_key=api_key, embedding_model=model_config['name'])\n",
    "    embeddings = embedding_system.create_embeddings(texts_chunk)\n",
    "\n",
    "    # Save embeddings for the chunk\n",
    "    with lock:\n",
    "        save_embeddings(embeddings, metadata_chunk, output_file, model_config['name'])\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    folder_path = '/workspace/slice-monorepo/cl_cr3/aligneddata/c=3'\n",
    "    embedding_model = 'text-embedding-3-small'  # Replace with the desired OpenAI model\n",
    "    num_workers = 2\n",
    "    output_file = 'utterance_embeddings.h5'\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    texts, metadata = load_data(folder_path, num_workers=num_workers)\n",
    "    total_texts = len(texts)\n",
    "    chunk_size = 512  # Batch size for processing\n",
    "    chunks = int(total_texts/chunk_size)\n",
    "    # Initialize multiprocessing manager and lock\n",
    "    manager = multiprocessing.Manager()\n",
    "    lock = manager.Lock()\n",
    "    jobs = []\n",
    "\n",
    "    # Divide the work into chunks\n",
    "    chunked_texts = [texts[i:i + chunk_size] for i in range(0, total_texts, chunk_size)]\n",
    "    chunked_metadata = [metadata[i:i + chunk_size] for i in range(0, total_texts, chunk_size)]\n",
    "    # Run a set number of workers at a time\n",
    "    for chunk_index, (texts_chunk, metadata_chunk) in enumerate(zip(chunked_texts, chunked_metadata)):\n",
    "        print(f\"Chunk: {chunk_index}/{chunks}\")\n",
    "        p = multiprocessing.Process(target=embedding_worker, args=(texts_chunk, metadata_chunk, {'name': embedding_model}, API_KEY, chunk_index, output_file, lock))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "        \n",
    "        # Ensure no more than num_workers are running at once\n",
    "        if len(jobs) >= num_workers:\n",
    "            for job in jobs:\n",
    "                job.join()\n",
    "            jobs = []  # Reset jobs list for the next set of workers\n",
    "\n",
    "    # Wait for remaining jobs to finish\n",
    "    for job in jobs:\n",
    "        job.join()\n",
    "    \n",
    "    print(f\"All chunks processed and saved to {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c2bac0c-749d-49e2-b0d1-6d6f93adeb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 5\n",
      "First few entries in the HDF5 file:\n",
      "Entry 1:\n",
      "  Name: MARISHA\n",
      "  Turn Number: 496\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [-0.00624625 -0.01879092 -0.01577863 ... -0.02597606 -0.02259865\n",
      "  0.01091464]\n",
      "\n",
      "Entry 2:\n",
      "  Name: LAURA\n",
      "  Turn Number: 497\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [-1.59449628e-05 -4.13936675e-02 -2.04442795e-02 ... -1.60119496e-02\n",
      " -1.74262542e-02  1.21604940e-02]\n",
      "\n",
      "Entry 3:\n",
      "  Name: SAM\n",
      "  Turn Number: 498\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [ 0.00522397  0.00704117 -0.0155793  ... -0.01513947  0.01615803\n",
      " -0.00351672]\n",
      "\n",
      "Entry 4:\n",
      "  Name: LAURA\n",
      "  Turn Number: 499\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [ 0.00368774  0.00596977  0.07222383 ... -0.00668156 -0.00336403\n",
      " -0.01299829]\n",
      "\n",
      "Entry 5:\n",
      "  Name: MARISHA\n",
      "  Turn Number: 500\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [0.02059525 0.05111907 0.01183178 ... 0.01972686 0.01128903 0.00686026]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def view_h5_file(file_path, num_entries=5):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Display the first few entries in the embeddings dataset\n",
    "        print(f\"size: {len(f)}\")\n",
    "        embeddings = f['embeddings'][:num_entries]\n",
    "        names = [name.decode('utf8') for name in f['names'][:num_entries]]\n",
    "        turn_numbers = f['turn_numbers'][:num_entries]\n",
    "        file_paths = [path.decode('utf8') for path in f['file_paths'][:num_entries]]\n",
    "        model_names = [model.decode('utf8') for model in f['model_names'][:num_entries]]\n",
    "\n",
    "        print(\"First few entries in the HDF5 file:\")\n",
    "        for i in range(num_entries):\n",
    "            print(f\"Entry {i + 1}:\")\n",
    "            print(f\"  Name: {names[i]}\")\n",
    "            print(f\"  Turn Number: {turn_numbers[i]}\")\n",
    "            print(f\"  File Path: {file_paths[i]}\")\n",
    "            print(f\"  Model Name: {model_names[i]}\")\n",
    "            print(f\"  Embedding: {embeddings[i]}\")\n",
    "            print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'utterance_embeddings.h5'  # Update with your actual file path\n",
    "    view_h5_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47243a9d-0f4f-45e2-bae2-1a713d1cf759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

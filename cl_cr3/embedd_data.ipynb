{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982dc479-c4b3-496d-820f-f66265e9b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q openai h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c950e4f-7a52-41d2-9940-05c745dd623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processed 330 files and extracted 865274 utterances.\n",
      "Chunk: 0/1689\n",
      "Chunk: 1/1689\n",
      "Chunk: 2/1689\n",
      "Chunk: 3/1689\n",
      "Chunk: 4/1689\n",
      "Chunk: 5/1689\n",
      "Chunk: 6/1689\n",
      "Chunk: 7/1689\n",
      "Chunk: 8/1689\n",
      "Chunk: 9/1689\n",
      "Chunk: 10/1689\n",
      "Chunk: 11/1689\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import h5py\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI API key\n",
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "# Function to process a JSON file and extract the relevant data\n",
    "def extract_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    extracted_data = []\n",
    "    for document in data:\n",
    "        for turn in document['TURNS']:\n",
    "            for name in turn['NAMES']:\n",
    "                extracted_data.append({\n",
    "                    'name': name,\n",
    "                    'utterance': ' '.join(turn['UTTERANCES']),\n",
    "                    'turn_number': turn['NUMBER'],\n",
    "                    'file_path': file_path\n",
    "                })\n",
    "    return extracted_data\n",
    "\n",
    "# Wrapper function to enable multiprocessing\n",
    "def extract_data_wrapper(args):\n",
    "    return extract_data(*args)\n",
    "\n",
    "# Load data from JSON files in the specified folder and its subdirectories using multiprocessing\n",
    "def load_data(folder_path, num_workers=4):\n",
    "    all_texts = []\n",
    "    all_metadata = []\n",
    "    file_paths = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "\n",
    "    with multiprocessing.Pool(num_workers) as pool:\n",
    "        results = pool.map(extract_data_wrapper, [(file_path,) for file_path in file_paths])\n",
    "\n",
    "    for result in results:\n",
    "        for item in result:\n",
    "            all_texts.append(item['utterance'])\n",
    "            all_metadata.append({\n",
    "                'name': item['name'],\n",
    "                'turn_number': item['turn_number'],\n",
    "                'file_path': item['file_path']\n",
    "            })\n",
    "\n",
    "    print(f\"Processed {len(file_paths)} files and extracted {len(all_texts)} utterances.\")\n",
    "    return all_texts, all_metadata\n",
    "\n",
    "# Define the Embedding System\n",
    "class EmbeddingSystem:\n",
    "    def __init__(self, api_key, embedding_model):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = embedding_model\n",
    "\n",
    "    def create_embeddings(self, texts, num_workers=4):\n",
    "        manager = multiprocessing.Manager()\n",
    "        return_dict = manager.dict()\n",
    "        jobs = []\n",
    "\n",
    "        total_texts = len(texts)\n",
    "        chunk_size = 5  # Adjust this number based on your needs\n",
    "\n",
    "        #print(f\"Total texts to embed: {total_texts}\")\n",
    "        total_processes = (total_texts + chunk_size - 1) // chunk_size\n",
    "        #print(f\"Running embedding with {total_processes} processes.\")\n",
    "\n",
    "        current_process = 0\n",
    "        for i in range(0, total_texts, chunk_size):\n",
    "            texts_chunk = texts[i:i + chunk_size]\n",
    "\n",
    "            p = multiprocessing.Process(target=self.worker, args=(texts_chunk, return_dict, current_process))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            current_process += 1\n",
    "\n",
    "        for proc in jobs:\n",
    "            proc.join()\n",
    "\n",
    "        embeddings = []\n",
    "        for result in return_dict.values():\n",
    "            embeddings.extend(result)\n",
    "\n",
    "        #print(f\"Total embeddings generated: {len(embeddings)}\")\n",
    "        return embeddings\n",
    "\n",
    "    def worker(self, texts, return_dict, index):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            while True:\n",
    "                try:\n",
    "                    embedding = self.get_embedding(text)\n",
    "                    embeddings.append(embedding)\n",
    "                    break\n",
    "                except openai.RateLimitError:\n",
    "                    wait_time = random.uniform(1, 60)  # Randomized wait time\n",
    "                    #print(f\"Rate limit hit. Process {index} waiting for {wait_time} seconds.\")\n",
    "                    time.sleep(wait_time)\n",
    "                except openai.APIError as e:\n",
    "                    print(f\"OpenAI API returned an API Error: {e}\")\n",
    "                    break\n",
    "                except openai.APIConnectionError as e:\n",
    "                    print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "                    time.sleep(2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error in process {index}: {e}\")\n",
    "                    break\n",
    "        return_dict[index] = embeddings\n",
    "        #print(f\"Process {index} completed with {len(texts)} texts.\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        return self.client.embeddings.create(input=[text], model=self.model).data[0].embedding\n",
    "\n",
    "def save_embeddings(embeddings, metadata, filename, model_name):\n",
    "    with h5py.File(filename, 'a') as f:\n",
    "        # Append new embeddings\n",
    "        if 'embeddings' not in f:\n",
    "            f.create_dataset('embeddings', data=embeddings, maxshape=(None, len(embeddings[0])))\n",
    "            names_encoded = [meta['name'].encode('utf8') for meta in metadata]\n",
    "            turn_numbers = [meta['turn_number'] for meta in metadata]\n",
    "            file_paths_encoded = [meta['file_path'].encode('utf8') for meta in metadata]\n",
    "            model_names_encoded = [model_name.encode('utf8') for _ in metadata]\n",
    "            f.create_dataset('names', data=names_encoded, maxshape=(None,))\n",
    "            f.create_dataset('turn_numbers', data=turn_numbers, maxshape=(None,))\n",
    "            f.create_dataset('file_paths', data=file_paths_encoded, maxshape=(None,))\n",
    "            f.create_dataset('model_names', data=model_names_encoded, maxshape=(None,))\n",
    "        else:\n",
    "            f['embeddings'].resize((f['embeddings'].shape[0] + len(embeddings)), axis=0)\n",
    "            f['embeddings'][-len(embeddings):] = embeddings\n",
    "            names_encoded = [meta['name'].encode('utf8') for meta in metadata]\n",
    "            turn_numbers = [meta['turn_number'] for meta in metadata]\n",
    "            file_paths_encoded = [meta['file_path'].encode('utf8') for meta in metadata]\n",
    "            model_names_encoded = [model_name.encode('utf8') for _ in metadata]\n",
    "            f['names'].resize((f['names'].shape[0] + len(names_encoded)), axis=0)\n",
    "            f['names'][-len(names_encoded):] = names_encoded\n",
    "            f['turn_numbers'].resize((f['turn_numbers'].shape[0] + len(turn_numbers)), axis=0)\n",
    "            f['turn_numbers'][-len(turn_numbers):] = turn_numbers\n",
    "            f['file_paths'].resize((f['file_paths'].shape[0] + len(file_paths_encoded)), axis=0)\n",
    "            f['file_paths'][-len(file_paths_encoded):] = file_paths_encoded\n",
    "            f['model_names'].resize((f['model_names'].shape[0] + len(model_names_encoded)), axis=0)\n",
    "            f['model_names'][-len(model_names_encoded):] = model_names_encoded\n",
    "\n",
    "    #print(f\"Chunk processed and saved.\")\n",
    "\n",
    "# Worker function for multiprocessing\n",
    "def embedding_worker(texts_chunk, metadata_chunk, model_config, api_key, chunk_index, output_file, lock):\n",
    "    embedding_system = EmbeddingSystem(api_key=api_key, embedding_model=model_config['name'])\n",
    "    embeddings = embedding_system.create_embeddings(texts_chunk)\n",
    "\n",
    "    # Save embeddings for the chunk\n",
    "    with lock:\n",
    "        save_embeddings(embeddings, metadata_chunk, output_file, model_config['name'])\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    folder_path = '/workspace/slice-monorepo/cl_cr3/aligneddata/c=3'\n",
    "    embedding_model = 'text-embedding-3-small'  # Replace with the desired OpenAI model\n",
    "    num_workers = 2\n",
    "    output_file = 'utterance_embeddings.h5'\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    texts, metadata = load_data(folder_path, num_workers=num_workers)\n",
    "    total_texts = len(texts)\n",
    "    chunk_size = 512  # Batch size for processing\n",
    "    chunks = int(total_texts/chunk_size)\n",
    "    # Initialize multiprocessing manager and lock\n",
    "    manager = multiprocessing.Manager()\n",
    "    lock = manager.Lock()\n",
    "    jobs = []\n",
    "\n",
    "    # Divide the work into chunks\n",
    "    chunked_texts = [texts[i:i + chunk_size] for i in range(0, total_texts, chunk_size)]\n",
    "    chunked_metadata = [metadata[i:i + chunk_size] for i in range(0, total_texts, chunk_size)]\n",
    "    # Run a set number of workers at a time\n",
    "    for chunk_index, (texts_chunk, metadata_chunk) in enumerate(zip(chunked_texts, chunked_metadata)):\n",
    "        print(f\"Chunk: {chunk_index}/{chunks}\")\n",
    "        p = multiprocessing.Process(target=embedding_worker, args=(texts_chunk, metadata_chunk, {'name': embedding_model}, API_KEY, chunk_index, output_file, lock))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "        \n",
    "        # Ensure no more than num_workers are running at once\n",
    "        if len(jobs) >= num_workers:\n",
    "            for job in jobs:\n",
    "                job.join()\n",
    "            jobs = []  # Reset jobs list for the next set of workers\n",
    "\n",
    "    # Wait for remaining jobs to finish\n",
    "    for job in jobs:\n",
    "        job.join()\n",
    "    \n",
    "    print(f\"All chunks processed and saved to {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c2bac0c-749d-49e2-b0d1-6d6f93adeb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few entries in the HDF5 file:\n",
      "Entry 1:\n",
      "  Name: MATT\n",
      "  Turn Number: 0\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [ 0.05032601  0.00377379 -0.00965879 ... -0.00165103 -0.01130818\n",
      "  0.02338167]\n",
      "\n",
      "Entry 2:\n",
      "  Name: MATT\n",
      "  Turn Number: 0\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [-0.00741248 -0.0010019  -0.05774804 ... -0.0611841  -0.00290264\n",
      "  0.0160442 ]\n",
      "\n",
      "Entry 3:\n",
      "  Name: MATT\n",
      "  Turn Number: 0\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [ 0.00097521 -0.0302202  -0.02924947 ...  0.01133854  0.00137157\n",
      "  0.01176821]\n",
      "\n",
      "Entry 4:\n",
      "  Name: MATT\n",
      "  Turn Number: 0\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [-0.0132638  -0.0226487  -0.00479658 ... -0.01663508  0.00841516\n",
      " -0.00447116]\n",
      "\n",
      "Entry 5:\n",
      "  Name: MATT\n",
      "  Turn Number: 0\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [ 0.02899877 -0.00469119 -0.01009918 ... -0.0112773  -0.02313656\n",
      "  0.00165096]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def view_h5_file(file_path, num_entries=5):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Display the first few entries in the embeddings dataset\n",
    "        embeddings = f['embeddings'][:num_entries]\n",
    "        names = [name.decode('utf8') for name in f['names'][:num_entries]]\n",
    "        turn_numbers = f['turn_numbers'][:num_entries]\n",
    "        file_paths = [path.decode('utf8') for path in f['file_paths'][:num_entries]]\n",
    "        model_names = [model.decode('utf8') for model in f['model_names'][:num_entries]]\n",
    "\n",
    "        print(\"First few entries in the HDF5 file:\")\n",
    "        for i in range(num_entries):\n",
    "            print(f\"Entry {i + 1}:\")\n",
    "            print(f\"  Name: {names[i]}\")\n",
    "            print(f\"  Turn Number: {turn_numbers[i]}\")\n",
    "            print(f\"  File Path: {file_paths[i]}\")\n",
    "            print(f\"  Model Name: {model_names[i]}\")\n",
    "            print(f\"  Embedding: {embeddings[i]}\")\n",
    "            print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'utterance_embeddings.h5'  # Update with your actual file path\n",
    "    view_h5_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47243a9d-0f4f-45e2-bae2-1a713d1cf759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

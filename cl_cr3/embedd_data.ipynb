{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982dc479-c4b3-496d-820f-f66265e9b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q openai h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c950e4f-7a52-41d2-9940-05c745dd623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import h5py\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI API key\n",
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "# Function to process a JSON file and extract the relevant data\n",
    "def extract_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    extracted_data = []\n",
    "    for document in data:\n",
    "        for turn in document['TURNS']:\n",
    "            for name in turn['NAMES']:\n",
    "                extracted_data.append({\n",
    "                    'name': name,\n",
    "                    'utterance': ' '.join(turn['UTTERANCES']),\n",
    "                    'turn_number': turn['NUMBER'],\n",
    "                    'file_path': file_path\n",
    "                })\n",
    "    return extracted_data\n",
    "\n",
    "# Wrapper function to enable multiprocessing\n",
    "def extract_data_wrapper(args):\n",
    "    return extract_data(*args)\n",
    "\n",
    "# Load data from JSON files in the specified folder and its subdirectories using multiprocessing\n",
    "def load_data(folder_path, num_workers=4):\n",
    "    all_texts = []\n",
    "    all_metadata = []\n",
    "    file_paths = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "\n",
    "    with multiprocessing.Pool(num_workers) as pool:\n",
    "        results = pool.map(extract_data_wrapper, [(file_path,) for file_path in file_paths])\n",
    "\n",
    "    for result in results:\n",
    "        for item in result:\n",
    "            all_texts.append(item['utterance'])\n",
    "            all_metadata.append({\n",
    "                'name': item['name'],\n",
    "                'turn_number': item['turn_number'],\n",
    "                'file_path': item['file_path']\n",
    "            })\n",
    "\n",
    "    print(f\"Processed {len(file_paths)} files and extracted {len(all_texts)} utterances.\")\n",
    "    return all_texts, all_metadata\n",
    "\n",
    "# Define the Embedding System\n",
    "class EmbeddingSystem:\n",
    "    def __init__(self, api_key, embedding_model):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = embedding_model\n",
    "\n",
    "    def create_embeddings(self, texts, num_workers=4):\n",
    "        manager = multiprocessing.Manager()\n",
    "        return_dict = manager.dict()\n",
    "        jobs = []\n",
    "\n",
    "        total_texts = len(texts)\n",
    "        chunk_size = 5  # Adjust this number based on your needs\n",
    "\n",
    "        #print(f\"Total texts to embed: {total_texts}\")\n",
    "        total_processes = (total_texts + chunk_size - 1) // chunk_size\n",
    "        #print(f\"Running embedding with {total_processes} processes.\")\n",
    "\n",
    "        current_process = 0\n",
    "        for i in range(0, total_texts, chunk_size):\n",
    "            texts_chunk = texts[i:i + chunk_size]\n",
    "\n",
    "            p = multiprocessing.Process(target=self.worker, args=(texts_chunk, return_dict, current_process))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            current_process += 1\n",
    "\n",
    "        for proc in jobs:\n",
    "            proc.join()\n",
    "\n",
    "        embeddings = []\n",
    "        for result in return_dict.values():\n",
    "            embeddings.extend(result)\n",
    "\n",
    "        #print(f\"Total embeddings generated: {len(embeddings)}\")\n",
    "        return embeddings\n",
    "\n",
    "    def worker(self, texts, return_dict, index):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            while True:\n",
    "                try:\n",
    "                    embedding = self.get_embedding(text)\n",
    "                    embeddings.append(embedding)\n",
    "                    break\n",
    "                except openai.RateLimitError:\n",
    "                    wait_time = random.uniform(1, 60)  # Randomized wait time\n",
    "                    #print(f\"Rate limit hit. Process {index} waiting for {wait_time} seconds.\")\n",
    "                    time.sleep(wait_time)\n",
    "                except openai.APIError as e:\n",
    "                    print(f\"OpenAI API returned an API Error: {e}\")\n",
    "                    break\n",
    "                except openai.APIConnectionError as e:\n",
    "                    print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "                    time.sleep(2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error in process {index}: {e}\")\n",
    "                    break\n",
    "        return_dict[index] = embeddings\n",
    "        #print(f\"Process {index} completed with {len(texts)} texts.\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        return self.client.embeddings.create(input=[text], model=self.model).data[0].embedding\n",
    "\n",
    "def save_embeddings(embeddings, metadata, filename, model_name):\n",
    "    with h5py.File(filename, 'a') as f:\n",
    "        # Append new embeddings\n",
    "        if 'embeddings' not in f:\n",
    "            f.create_dataset('embeddings', data=embeddings, maxshape=(None, len(embeddings[0])))\n",
    "            names_encoded = [meta['name'].encode('utf8') for meta in metadata]\n",
    "            turn_numbers = [meta['turn_number'] for meta in metadata]\n",
    "            file_paths_encoded = [meta['file_path'].encode('utf8') for meta in metadata]\n",
    "            model_names_encoded = [model_name.encode('utf8') for _ in metadata]\n",
    "            f.create_dataset('names', data=names_encoded, maxshape=(None,))\n",
    "            f.create_dataset('turn_numbers', data=turn_numbers, maxshape=(None,))\n",
    "            f.create_dataset('file_paths', data=file_paths_encoded, maxshape=(None,))\n",
    "            f.create_dataset('model_names', data=model_names_encoded, maxshape=(None,))\n",
    "        else:\n",
    "            f['embeddings'].resize((f['embeddings'].shape[0] + len(embeddings)), axis=0)\n",
    "            f['embeddings'][-len(embeddings):] = embeddings\n",
    "            names_encoded = [meta['name'].encode('utf8') for meta in metadata]\n",
    "            turn_numbers = [meta['turn_number'] for meta in metadata]\n",
    "            file_paths_encoded = [meta['file_path'].encode('utf8') for meta in metadata]\n",
    "            model_names_encoded = [model_name.encode('utf8') for _ in metadata]\n",
    "            f['names'].resize((f['names'].shape[0] + len(names_encoded)), axis=0)\n",
    "            f['names'][-len(names_encoded):] = names_encoded\n",
    "            f['turn_numbers'].resize((f['turn_numbers'].shape[0] + len(turn_numbers)), axis=0)\n",
    "            f['turn_numbers'][-len(turn_numbers):] = turn_numbers\n",
    "            f['file_paths'].resize((f['file_paths'].shape[0] + len(file_paths_encoded)), axis=0)\n",
    "            f['file_paths'][-len(file_paths_encoded):] = file_paths_encoded\n",
    "            f['model_names'].resize((f['model_names'].shape[0] + len(model_names_encoded)), axis=0)\n",
    "            f['model_names'][-len(model_names_encoded):] = model_names_encoded\n",
    "\n",
    "    #print(f\"Chunk processed and saved.\")\n",
    "\n",
    "# Worker function for multiprocessing\n",
    "def embedding_worker(texts_chunk, metadata_chunk, model_config, api_key, chunk_index, output_file, lock):\n",
    "    embedding_system = EmbeddingSystem(api_key=api_key, embedding_model=model_config['name'])\n",
    "    embeddings = embedding_system.create_embeddings(texts_chunk)\n",
    "\n",
    "    # Save embeddings for the chunk\n",
    "    with lock:\n",
    "        save_embeddings(embeddings, metadata_chunk, output_file, model_config['name'])\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    folder_path = '/workspace/slice-monorepo/cl_cr3/aligneddata/c=3'\n",
    "    embedding_model = 'text-embedding-3-small'  # Replace with the desired OpenAI model\n",
    "    num_workers = 2\n",
    "    output_file = 'utterance_embeddings.h5'\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    texts, metadata = load_data(folder_path, num_workers=num_workers)\n",
    "    total_texts = len(texts)\n",
    "    chunk_size = 512  # Batch size for processing\n",
    "    chunks = int(total_texts/chunk_size)\n",
    "    # Initialize multiprocessing manager and lock\n",
    "    manager = multiprocessing.Manager()\n",
    "    lock = manager.Lock()\n",
    "    jobs = []\n",
    "\n",
    "    # Divide the work into chunks\n",
    "    chunked_texts = [texts[i:i + chunk_size] for i in range(0, total_texts, chunk_size)]\n",
    "    chunked_metadata = [metadata[i:i + chunk_size] for i in range(0, total_texts, chunk_size)]\n",
    "    # Run a set number of workers at a time\n",
    "    for chunk_index, (texts_chunk, metadata_chunk) in enumerate(zip(chunked_texts, chunked_metadata)):\n",
    "        print(f\"Chunk: {chunk_index}/{chunks}\")\n",
    "        p = multiprocessing.Process(target=embedding_worker, args=(texts_chunk, metadata_chunk, {'name': embedding_model}, API_KEY, chunk_index, output_file, lock))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "        \n",
    "        # Ensure no more than num_workers are running at once\n",
    "        if len(jobs) >= num_workers:\n",
    "            for job in jobs:\n",
    "                job.join()\n",
    "            jobs = []  # Reset jobs list for the next set of workers\n",
    "\n",
    "    # Wait for remaining jobs to finish\n",
    "    for job in jobs:\n",
    "        job.join()\n",
    "    \n",
    "    print(f\"All chunks processed and saved to {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2bac0c-749d-49e2-b0d1-6d6f93adeb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 5\n",
      "First few entries in the HDF5 file:\n",
      "Entry 1:\n",
      "  Name: MARISHA\n",
      "  Turn Number: 496\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [-0.00624625 -0.01879092 -0.01577863 ... -0.02597606 -0.02259865\n",
      "  0.01091464]\n",
      "  Embedding Size: 1536\n",
      "\n",
      "Entry 2:\n",
      "  Name: LAURA\n",
      "  Turn Number: 497\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [-1.59449628e-05 -4.13936675e-02 -2.04442795e-02 ... -1.60119496e-02\n",
      " -1.74262542e-02  1.21604940e-02]\n",
      "  Embedding Size: 1536\n",
      "\n",
      "Entry 3:\n",
      "  Name: SAM\n",
      "  Turn Number: 498\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [ 0.00522397  0.00704117 -0.0155793  ... -0.01513947  0.01615803\n",
      " -0.00351672]\n",
      "  Embedding Size: 1536\n",
      "\n",
      "Entry 4:\n",
      "  Name: LAURA\n",
      "  Turn Number: 499\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [ 0.00368774  0.00596977  0.07222383 ... -0.00668156 -0.00336403\n",
      " -0.01299829]\n",
      "  Embedding Size: 1536\n",
      "\n",
      "Entry 5:\n",
      "  Name: MARISHA\n",
      "  Turn Number: 500\n",
      "  File Path: /workspace/slice-monorepo/cl_cr3/aligneddata/c=3/C1E003_3_0.json\n",
      "  Model Name: text-embedding-3-small\n",
      "  Embedding: [0.02059525 0.05111907 0.01183178 ... 0.01972686 0.01128903 0.00686026]\n",
      "  Embedding Size: 1536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def view_h5_file(file_path, num_entries=5):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Display the first few entries in the embeddings dataset\n",
    "        print(f\"size: {len(f)}\")\n",
    "        embeddings = f['embeddings'][:num_entries]\n",
    "        names = [name.decode('utf8') for name in f['names'][:num_entries]]\n",
    "        turn_numbers = f['turn_numbers'][:num_entries]\n",
    "        file_paths = [path.decode('utf8') for path in f['file_paths'][:num_entries]]\n",
    "        model_names = [model.decode('utf8') for model in f['model_names'][:num_entries]]\n",
    "\n",
    "        print(\"First few entries in the HDF5 file:\")\n",
    "        for i in range(num_entries):\n",
    "            print(f\"Entry {i + 1}:\")\n",
    "            print(f\"  Name: {names[i]}\")\n",
    "            print(f\"  Turn Number: {turn_numbers[i]}\")\n",
    "            print(f\"  File Path: {file_paths[i]}\")\n",
    "            print(f\"  Model Name: {model_names[i]}\")\n",
    "            print(f\"  Embedding: {embeddings[i]}\")\n",
    "            print(f\"  Embedding Size: {len(embeddings[i])}\")\n",
    "            print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'utterance_embeddings.h5'  # Update with your actual file path\n",
    "    view_h5_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7625b0-d3b3-4e8f-8b04-0cbd6d0ca74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m148.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47243a9d-0f4f-45e2-bae2-1a713d1cf759",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cuml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/slice-monorepo/cl_cr3/umap.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cuml'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load embeddings and metadata from the HDF5 file\n",
    "def load_embeddings(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        embeddings = f['embeddings'][:]\n",
    "        names = f['names'][:]\n",
    "        turn_numbers = f['turn_numbers'][:]\n",
    "        file_paths = f['file_paths'][:]\n",
    "        model_names = f['model_names'][:]\n",
    "    \n",
    "    metadata = {\n",
    "        'names': [name.decode('utf8') for name in names],\n",
    "        'turn_numbers': turn_numbers,\n",
    "        'file_paths': [file_path.decode('utf8') for file_path in file_paths],\n",
    "        'model_names': [model_name.decode('utf8') for model_name in model_names]\n",
    "    }\n",
    "    return embeddings, metadata\n",
    "\n",
    "# Function to compute and print standard statistics on the embeddings\n",
    "def compute_statistics(embeddings):\n",
    "    df = pd.DataFrame(embeddings)\n",
    "    stats = df.describe()\n",
    "    print(\"Embedding Statistics:\")\n",
    "    print(stats)\n",
    "\n",
    "# Function to visualize embeddings using UMAP\n",
    "def visualize_embeddings(embeddings, metadata):\n",
    "    # Memory-efficient UMAP\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', low_memory=True)\n",
    "    umap_embeddings = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=metadata['turn_numbers'], cmap='Spectral', s=5)\n",
    "    plt.colorbar(scatter, label='Turn Numbers')\n",
    "    plt.title('UMAP projection of the embeddings')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=[hash(name) % 10 for name in metadata['names']], cmap='tab10', s=5)\n",
    "    plt.colorbar(scatter, label='Names')\n",
    "    plt.title('UMAP projection of the embeddings by Names')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=[hash(fp) % 10 for fp in metadata['file_paths']], cmap='tab10', s=5)\n",
    "    plt.colorbar(scatter, label='File Paths')\n",
    "    plt.title('UMAP projection of the embeddings by File Paths')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "def main(embeddings_path):\n",
    "    embeddings, metadata = load_embeddings(embeddings_path)\n",
    "    \n",
    "    # Compute and print statistics\n",
    "    compute_statistics(embeddings)\n",
    "    \n",
    "    # Visualize embeddings using UMAP\n",
    "    visualize_embeddings(embeddings, metadata)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    embeddings_path = 'utterance_embeddings.h5'  # Path to the HDF5 file containing embeddings\n",
    "    main(embeddings_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca972a06-9444-4005-b3aa-960e4b941d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

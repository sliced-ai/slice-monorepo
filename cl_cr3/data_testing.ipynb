{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a133009-792a-48a3-9e24-d5aa103bdba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai pandas matplotlib seaborn\n",
    "# Install necessary packages\n",
    "!pip install -q torch pandas sentence-transformers umap-learn scikit-learn matplotlib seaborn\n",
    "!pip install -q openai pandas numpy sklearn umap-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb4a2d-7512-417c-a528-dbf0e7f68ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758da49-71cc-4227-921f-41b6ea405895",
   "metadata": {},
   "outputs": [],
   "source": [
    "You are a dataset conversion bot. You are to generate a set of question response pairs that explain the attached data chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e1072-5a66-401c-97f0-a294a2368d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "# Function to process a JSON file and extract the relevant data\n",
    "def extract_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    extracted_data = []\n",
    "    for document in data:\n",
    "        for turn in document['TURNS']:\n",
    "            for name in turn['NAMES']:\n",
    "                extracted_data.append({\n",
    "                    'name': name,\n",
    "                    'utterance': ' '.join(turn['UTTERANCES']),\n",
    "                    'turn_number': turn['NUMBER']\n",
    "                })\n",
    "    return extracted_data\n",
    "\n",
    "# Function to process all JSON files in a folder and extract data\n",
    "def process_folder(folder_path):\n",
    "    all_data = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                data = extract_data(file_path)\n",
    "                all_data.extend(data)\n",
    "    return all_data\n",
    "\n",
    "# Function to save embeddings to an HDF5 file with proper encoding\n",
    "def save_embeddings_to_hdf5(embeddings, data, filename):\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        f.create_dataset('embeddings', data=embeddings)\n",
    "        names_encoded = [name.encode('utf8') for name in data['name'].values]\n",
    "        utterances_encoded = [utt.encode('utf8') for utt in data['utterance'].values]\n",
    "        f.create_dataset('names', data=names_encoded)\n",
    "        f.create_dataset('utterances', data=utterances_encoded)\n",
    "        f.create_dataset('turn_numbers', data=data['turn_number'].values)\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    # Specify the folder path containing the JSON files\n",
    "    folder_path = '/workspace/slice-monorepo/cl_cr3/aligneddata'\n",
    "    \n",
    "    # Process all JSON files and extract the data\n",
    "    print(\"Processing JSON files...\")\n",
    "    data = process_folder(folder_path)\n",
    "    \n",
    "    # Convert the extracted data to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Load a pre-trained SentenceTransformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "    \n",
    "    # Embed all utterances in large batches\n",
    "    print(\"Embedding utterances...\")\n",
    "    embeddings = model.encode(df['utterance'].tolist(), batch_size=4096, show_progress_bar=True, convert_to_tensor=True)\n",
    "    \n",
    "    # Convert embeddings to CPU for further processing\n",
    "    embeddings = embeddings.cpu().numpy()\n",
    "    \n",
    "    # Save embeddings to an HDF5 file\n",
    "    print(\"Saving embeddings to HDF5 file...\")\n",
    "    save_embeddings_to_hdf5(embeddings, df, 'utterance_embeddings.h5')\n",
    "    \n",
    "    # Reduce dimensions using UMAP\n",
    "    print(\"Reducing dimensions using UMAP...\")\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine')\n",
    "    umap_embeddings = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Perform KMeans clustering\n",
    "    print(\"Performing KMeans clustering...\")\n",
    "    n_clusters = 10\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_embeddings)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    \n",
    "    # Add UMAP and cluster labels to the DataFrame\n",
    "    df['umap_x'] = umap_embeddings[:, 0]\n",
    "    df['umap_y'] = umap_embeddings[:, 1]\n",
    "    df['cluster'] = cluster_labels\n",
    "    \n",
    "    # Visualize the UMAP embeddings with clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(x='umap_x', y='umap_y', hue='cluster', data=df, palette='viridis', s=5)\n",
    "    plt.title('UMAP projection with KMeans clusters')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the UMAP embeddings colored by names\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(x='umap_x', y='umap_y', hue='name', data=df, palette='tab20', s=5)\n",
    "    plt.title('UMAP projection colored by names')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.legend(title='Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025ba76-cc8e-4927-922a-aeed0099ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing dimensions using UMAP with batching...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "\n",
    "# Function to load embeddings from an HDF5 file with proper decoding\n",
    "def load_embeddings_from_hdf5(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        embeddings = np.array(f['embeddings'])\n",
    "        names = [name.decode('utf8') for name in f['names']]\n",
    "        utterances = [utt.decode('utf8') for utt in f['utterances']]\n",
    "        turn_numbers = np.array(f['turn_numbers'])\n",
    "    \n",
    "    return embeddings, names, utterances, turn_numbers\n",
    "\n",
    "# Function to reduce dimensions using UMAP with batching\n",
    "def umap_with_batches(embeddings, batch_size=10000, n_neighbors=15, min_dist=0.1, metric='cosine'):\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    n_batches = int(np.ceil(embeddings.shape[0] / batch_size))\n",
    "    \n",
    "    umap_embeddings = []\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, embeddings.shape[0])\n",
    "        batch_embeddings = embeddings[start_idx:end_idx]\n",
    "        umap_batch = reducer.fit_transform(batch_embeddings)\n",
    "        umap_embeddings.append(umap_batch)\n",
    "    \n",
    "    return np.concatenate(umap_embeddings, axis=0)\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    # Load embeddings from HDF5 file\n",
    "    embeddings_file = 'utterance_embeddings.h5'\n",
    "    embeddings, names, utterances, turn_numbers = load_embeddings_from_hdf5(embeddings_file)\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    df = pd.DataFrame({\n",
    "        'name': names,\n",
    "        'utterance': utterances,\n",
    "        'turn_number': turn_numbers\n",
    "    })\n",
    "    \n",
    "    # Reduce dimensions using UMAP with batching\n",
    "    print(\"Reducing dimensions using UMAP with batching...\")\n",
    "    umap_embeddings = umap_with_batches(embeddings, batch_size=10000, n_neighbors=15, min_dist=0.1, metric='cosine')\n",
    "    \n",
    "    # Perform KMeans clustering\n",
    "    print(\"Performing KMeans clustering...\")\n",
    "    n_clusters = 10\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_embeddings)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    \n",
    "    # Add UMAP and cluster labels to the DataFrame\n",
    "    df['umap_x'] = umap_embeddings[:, 0]\n",
    "    df['umap_y'] = umap_embeddings[:, 1]\n",
    "    df['cluster'] = cluster_labels\n",
    "    \n",
    "    # Visualize the UMAP embeddings with clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(x='umap_x', y='umap_y', hue='cluster', data=df, palette='viridis', s=5)\n",
    "    plt.title('UMAP projection with KMeans clusters')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the UMAP embeddings colored by names\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(x='umap_x', y='umap_y', hue='name', data=df, palette='tab20', s=5)\n",
    "    plt.title('UMAP projection colored by names')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.legend(title='Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ab6f9-f0b9-4027-8620-c2b341ad42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to process a JSON file and extract the relevant data\n",
    "def extract_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    extracted_data = []\n",
    "    for document in data:\n",
    "        for turn in document['TURNS']:\n",
    "            for name in turn['NAMES']:\n",
    "                extracted_data.append({\n",
    "                    'name': name,\n",
    "                    'utterance': ' '.join(turn['UTTERANCES']),\n",
    "                    'turn_number': turn['NUMBER']\n",
    "                })\n",
    "    return extracted_data\n",
    "\n",
    "# Function to process all JSON files in a folder and extract data\n",
    "def process_folder(folder_path):\n",
    "    all_data = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                data = extract_data(file_path)\n",
    "                all_data.extend(data)\n",
    "    return all_data\n",
    "\n",
    "# Function to analyze data based on the given name\n",
    "def analyze_data(data, name):\n",
    "    name_data = [entry for entry in data if entry['name'] == name]\n",
    "    \n",
    "    if not name_data:\n",
    "        print(f\"No data found for the name: {name}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(name_data)\n",
    "    df['utterance_length'] = df['utterance'].apply(len)\n",
    "    df['turn_difference'] = df['turn_number'].diff().fillna(0).astype(int)\n",
    "    \n",
    "    # Removing invalid turn differences\n",
    "    df = df[df['turn_difference'] >= 0]\n",
    "    \n",
    "    avg_utterance_length = df['utterance_length'].mean()\n",
    "    median_utterance_length = df['utterance_length'].median()\n",
    "    min_utterance_length = df['utterance_length'].min()\n",
    "    max_utterance_length = df['utterance_length'].max()\n",
    "    \n",
    "    avg_turn_difference = df['turn_difference'].mean()\n",
    "    median_turn_difference = df['turn_difference'].median()\n",
    "    min_turn_difference = df['turn_difference'].min()\n",
    "    max_turn_difference = df['turn_difference'].max()\n",
    "\n",
    "    analysis_results = {\n",
    "        'total_utterances': len(df),\n",
    "        'average_utterance_length': avg_utterance_length,\n",
    "        'median_utterance_length': median_utterance_length,\n",
    "        'min_utterance_length': min_utterance_length,\n",
    "        'max_utterance_length': max_utterance_length,\n",
    "        'average_turn_difference': avg_turn_difference,\n",
    "        'median_turn_difference': median_turn_difference,\n",
    "        'min_turn_difference': min_turn_difference,\n",
    "        'max_turn_difference': max_turn_difference,\n",
    "        'utterance_lengths': df['utterance_length'].tolist(),\n",
    "        'turn_differences': df['turn_difference'].tolist(),\n",
    "        'turn_numbers': df['turn_number'].tolist()\n",
    "    }\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Function to visualize the analysis results\n",
    "def visualize_analysis(analysis_results, name):\n",
    "    if not analysis_results:\n",
    "        return\n",
    "    \n",
    "    utterance_lengths = analysis_results['utterance_lengths']\n",
    "    turn_differences = analysis_results['turn_differences']\n",
    "    turn_numbers = analysis_results['turn_numbers']\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "\n",
    "    # Plot 1: Utterance lengths\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.histplot(utterance_lengths, kde=True)\n",
    "    plt.title(f'Utterance Lengths for {name}')\n",
    "    plt.xlabel('Utterance Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Plot 2: Turn numbers\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.histplot(turn_numbers, kde=True)\n",
    "    plt.title(f'Turn Numbers for {name}')\n",
    "    plt.xlabel('Turn Number')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Plot 3: Turn differences\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(turn_differences, kde=True)\n",
    "    plt.title(f'Turn Differences for {name}')\n",
    "    plt.xlabel('Turns Between Utterances')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Plot 4: Scatter plot of Turn numbers vs Utterance lengths\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.scatterplot(x=turn_numbers, y=utterance_lengths)\n",
    "    plt.title(f'Turn Numbers vs Utterance Lengths for {name}')\n",
    "    plt.xlabel('Turn Number')\n",
    "    plt.ylabel('Utterance Length')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to print detailed statistics\n",
    "def print_statistics(analysis_results):\n",
    "    if not analysis_results:\n",
    "        return\n",
    "\n",
    "    print(f\"Total Utterances: {analysis_results['total_utterances']}\")\n",
    "    print(f\"Average Utterance Length: {analysis_results['average_utterance_length']:.2f}\")\n",
    "    print(f\"Median Utterance Length: {analysis_results['median_utterance_length']:.2f}\")\n",
    "    print(f\"Min Utterance Length: {analysis_results['min_utterance_length']}\")\n",
    "    print(f\"Max Utterance Length: {analysis_results['max_utterance_length']}\")\n",
    "    print(f\"Average Turn Difference: {analysis_results['average_turn_difference']:.2f}\")\n",
    "    print(f\"Median Turn Difference: {analysis_results['median_turn_difference']:.2f}\")\n",
    "    print(f\"Min Turn Difference: {analysis_results['min_turn_difference']}\")\n",
    "    print(f\"Max Turn Difference: {analysis_results['max_turn_difference']}\")\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(f\"Standard Deviation of Utterance Lengths: {pd.Series(analysis_results['utterance_lengths']).std():.2f}\")\n",
    "    print(f\"Standard Deviation of Turn Differences: {pd.Series(analysis_results['turn_differences']).std():.2f}\")\n",
    "    print(f\"Total Turn Numbers: {len(analysis_results['turn_numbers'])}\")\n",
    "    print(f\"Total Unique Turn Numbers: {len(set(analysis_results['turn_numbers']))}\")\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    # Specify the folder path containing the JSON files\n",
    "    folder_path = '/workspace/slice-monorepo/cl_cr3/aligneddata'\n",
    "    \n",
    "    # Process all JSON files and extract the data\n",
    "    print(\"Processing JSON files...\")\n",
    "    data = process_folder(folder_path)\n",
    "    \n",
    "    # Specify the name to analyze\n",
    "    name_to_analyze = 'MARISHA'\n",
    "    \n",
    "    # Analyze the data based on the given name\n",
    "    print(f\"Analyzing data for the name: {name_to_analyze}...\")\n",
    "    analysis_results = analyze_data(data, name_to_analyze)\n",
    "    \n",
    "    # Print the analysis results\n",
    "    if analysis_results:\n",
    "        print_statistics(analysis_results)\n",
    "        \n",
    "        # Visualize the analysis results\n",
    "        visualize_analysis(analysis_results, name_to_analyze)\n",
    "    else:\n",
    "        print(f\"No data found for the name: {name_to_analyze}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255df1c-46fe-4877-9e9b-9420b8625ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import openai\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "from openai import OpenAI\n",
    "\n",
    "# Function to process a JSON file and extract summary chunks with metadata\n",
    "def extract_summary_chunks_with_metadata(file_path):\n",
    "    with open(file_path) as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    summary_chunks = []\n",
    "    for document in data:\n",
    "        if 'CHUNK' in document:\n",
    "            chunk_data = {\n",
    "                'chunk': document['CHUNK'],\n",
    "                'metadata': document['ALIGNMENT'],\n",
    "                'filename': os.path.basename(file_path)\n",
    "            }\n",
    "            summary_chunks.append(chunk_data)\n",
    "    return summary_chunks\n",
    "\n",
    "# Function to process all JSON files in a folder and extract summary chunks with metadata\n",
    "def process_folder(folder_path):\n",
    "    all_summary_chunks = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                summary_chunks = extract_summary_chunks_with_metadata(file_path)\n",
    "                all_summary_chunks.extend(summary_chunks)\n",
    "    return all_summary_chunks\n",
    "\n",
    "# Function to generate questions based on summary chunks using GPT-4\n",
    "def generate_responses(api_key, input_text, model_config):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    responses = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_config.get('name', \"gpt-3.5-turbo\"),\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a dataset conversion bot. You are to generate a set of question response pairs that explain the attached data chunk.\"},\n",
    "                    {\"role\": \"user\", \"content\": input_text}\n",
    "                ],\n",
    "                max_tokens=model_config.get('max_tokens', 300),\n",
    "                temperature=model_config.get('temperature', 0.7),\n",
    "                top_p=model_config.get('top_p', 0.9)\n",
    "            )\n",
    "\n",
    "            response_data = {\n",
    "                'uuid': str(uuid.uuid4()),\n",
    "                'response_content': response.choices[0].message.content,\n",
    "                'configuration': {\n",
    "                    'max_tokens': model_config.get('max_tokens', 300),\n",
    "                    'temperature': model_config.get('temperature', 0.7),\n",
    "                    'top_p': model_config.get('top_p', 0.9),\n",
    "                    'model': model_config.get('name', \"gpt-3.5-turbo\")\n",
    "                }\n",
    "            }\n",
    "            responses.append(response_data)\n",
    "            break\n",
    "        except openai.RateLimitError as e:\n",
    "            wait_time = random.uniform(1, 300)\n",
    "            print(f\"Rate limit hit. Waiting for {wait_time} seconds. Error: {e}\")\n",
    "            time.sleep(wait_time)\n",
    "        except openai.APIError as e:\n",
    "            print(f\"OpenAI API returned an API Error: {e}\")\n",
    "            break\n",
    "        except openai.APIConnectionError as e:\n",
    "            print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            break\n",
    "\n",
    "    return responses\n",
    "\n",
    "# Function to parse question-answer pairs from the response content\n",
    "def parse_qa_pairs(response_content):\n",
    "    qa_pairs = []\n",
    "    pairs = response_content.split('\\n\\n')\n",
    "    for pair in pairs:\n",
    "        if 'Q:' in pair and 'A:' in pair:\n",
    "            question = pair.split('Q:')[1].split('A:')[0].strip()\n",
    "            answer = pair.split('A:')[1].strip()\n",
    "            qa_pairs.append({'question': question, 'answer': answer})\n",
    "    return qa_pairs\n",
    "\n",
    "# Worker function for multiprocessing\n",
    "def worker(chunk_data_list, model_config, api_key, return_list, progress_dict, lock):\n",
    "    for chunk_data in chunk_data_list:\n",
    "        responses = generate_responses(api_key, chunk_data['chunk'], model_config)\n",
    "        for response in responses:\n",
    "            qa_pairs = parse_qa_pairs(response['response_content'])\n",
    "            result = {\n",
    "                'chunk': chunk_data['chunk'],\n",
    "                'metadata': chunk_data['metadata'],\n",
    "                'filename': chunk_data['filename'],\n",
    "                'qa_pairs': qa_pairs,\n",
    "                'inference_metadata': response['configuration']\n",
    "            }\n",
    "            return_list.append(result)\n",
    "        # Update progress\n",
    "        with lock:\n",
    "            progress_dict['processed_chunks'] += 1\n",
    "            print(f\"Processed {progress_dict['processed_chunks']} / {progress_dict['total_chunks']} chunks\")\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    # Specify your OpenAI API key\n",
    "    API_KEY = 'sk-proj-7MAfZbOm9lPY28pubTiRT3BlbkFJGgn73o5e6sVCjoTfoFAP'\n",
    "    \n",
    "    # Specify the folder path containing the JSON files\n",
    "    folder_path = '/workspace/slice-monorepo/cl_cr3/aligneddata'\n",
    "    \n",
    "    # Extract summary chunks from all JSON files in the folder\n",
    "    print(\"Extracting summary chunks from JSON files...\")\n",
    "    summary_chunks = process_folder(folder_path)\n",
    "    total_chunks = len(summary_chunks)\n",
    "    print(f\"Extracted {total_chunks} summary chunks.\")\n",
    "    \n",
    "    # Configuration for OpenAI model\n",
    "    model_config = {\n",
    "        \"name\": \"gpt-3.5-turbo\",\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    # Define number of workers and chunk size\n",
    "    num_workers = 20\n",
    "    chunk_size = 5\n",
    "    \n",
    "    # Using multiprocessing to process chunks in parallel\n",
    "    manager = multiprocessing.Manager()\n",
    "    return_list = manager.list()\n",
    "    progress_dict = manager.dict({'processed_chunks': 0, 'total_chunks': total_chunks})\n",
    "    lock = manager.Lock()\n",
    "    jobs = []\n",
    "\n",
    "    # Divide the work into chunks of size `chunk_size`\n",
    "    chunked_data = [summary_chunks[i:i + chunk_size] for i in range(0, total_chunks, chunk_size)]\n",
    "    \n",
    "    # Run a set number of workers at a time\n",
    "    for i in range(0, len(chunked_data), num_workers):\n",
    "        current_jobs = chunked_data[i:i + num_workers]\n",
    "        for chunk_data_list in current_jobs:\n",
    "            p = multiprocessing.Process(target=worker, args=(chunk_data_list, model_config, API_KEY, return_list, progress_dict, lock))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "        \n",
    "        for job in jobs:\n",
    "            job.join()\n",
    "        jobs = []  # Reset jobs list for the next set of workers\n",
    "    \n",
    "    # Collecting the results\n",
    "    generated_questions = list(return_list)\n",
    "    \n",
    "    # Save the generated questions to a JSON file\n",
    "    output_file = 'generated_questions.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(generated_questions, f, indent=4)\n",
    "    \n",
    "    print(f\"Generated questions saved to {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705ff9d-b0b0-4373-904b-29e3672c4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def process_json_file(file_path):\n",
    "    # Open the JSON file\n",
    "    with open(file_path) as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Initialize metrics\n",
    "    total_chunks = len(data)\n",
    "    total_turns = 0\n",
    "    total_words = 0\n",
    "    unique_names = set()\n",
    "    name_utterances = {}\n",
    "\n",
    "    # Iterate over each document (chunk)\n",
    "    for document in data:\n",
    "        # Update total turns\n",
    "        total_turns += len(document['TURNS'])\n",
    "\n",
    "        # Iterate over each turn\n",
    "        for turn in document['TURNS']:\n",
    "            # Update unique names\n",
    "            unique_names.update(turn['NAMES'])\n",
    "\n",
    "            # Update name utterances\n",
    "            for name in turn['NAMES']:\n",
    "                if name not in name_utterances:\n",
    "                    name_utterances[name] = []\n",
    "                name_utterances[name].append(turn['NUMBER'])\n",
    "\n",
    "            # Update total words\n",
    "            for utterance in turn['UTTERANCES']:\n",
    "                total_words += len(utterance.split())\n",
    "\n",
    "    # Return metrics\n",
    "    return total_chunks, total_turns, total_words, unique_names, name_utterances\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    # Initialize summary metrics\n",
    "    total_files = 0\n",
    "    total_chunks = 0\n",
    "    total_turns = 0\n",
    "    total_words = 0\n",
    "    unique_names = set()\n",
    "    common_names = None\n",
    "    name_utterances = {}\n",
    "\n",
    "    # Initialize campaign-specific metrics\n",
    "    campaign_metrics = {}\n",
    "\n",
    "    # Iterate over all files and subdirectories in the folder\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Check if the file has a .json extension\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                chunks, turns, words, names, file_name_utterances = process_json_file(file_path)\n",
    "\n",
    "                # Extract the campaign number from the file name\n",
    "                campaign_number = re.findall(r'C\\d+', file)[0]\n",
    "\n",
    "                # Update summary metrics\n",
    "                total_files += 1\n",
    "                total_chunks += chunks\n",
    "                total_turns += turns\n",
    "                total_words += words\n",
    "                unique_names.update(names)\n",
    "\n",
    "                # Update common names\n",
    "                if common_names is None:\n",
    "                    common_names = set(names)\n",
    "                else:\n",
    "                    common_names &= names\n",
    "\n",
    "                # Update name utterances\n",
    "                for name, utterances in file_name_utterances.items():\n",
    "                    if name not in name_utterances:\n",
    "                        name_utterances[name] = []\n",
    "                    name_utterances[name].extend(utterances)\n",
    "\n",
    "                # Update campaign-specific metrics\n",
    "                if campaign_number not in campaign_metrics:\n",
    "                    campaign_metrics[campaign_number] = {\n",
    "                        'total_files': 0,\n",
    "                        'total_chunks': 0,\n",
    "                        'total_turns': 0,\n",
    "                        'total_words': 0,\n",
    "                        'unique_names': set(),\n",
    "                        'common_names': None,\n",
    "                        'name_utterances': {}\n",
    "                    }\n",
    "                campaign_metrics[campaign_number]['total_files'] += 1\n",
    "                campaign_metrics[campaign_number]['total_chunks'] += chunks\n",
    "                campaign_metrics[campaign_number]['total_turns'] += turns\n",
    "                campaign_metrics[campaign_number]['total_words'] += words\n",
    "                campaign_metrics[campaign_number]['unique_names'].update(names)\n",
    "\n",
    "                if campaign_metrics[campaign_number]['common_names'] is None:\n",
    "                    campaign_metrics[campaign_number]['common_names'] = set(names)\n",
    "                else:\n",
    "                    campaign_metrics[campaign_number]['common_names'] &= names\n",
    "\n",
    "                for name, utterances in file_name_utterances.items():\n",
    "                    if name not in campaign_metrics[campaign_number]['name_utterances']:\n",
    "                        campaign_metrics[campaign_number]['name_utterances'][name] = []\n",
    "                    campaign_metrics[campaign_number]['name_utterances'][name].extend(utterances)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_chunks_per_file = total_chunks / total_files\n",
    "    avg_turns_per_file = total_turns / total_files\n",
    "    avg_words_per_utterance = total_words / total_turns\n",
    "\n",
    "    # Calculate average steps between utterances for common names\n",
    "    avg_steps_between_utterances = {}\n",
    "    for name in common_names:\n",
    "        utterances = name_utterances[name]\n",
    "        steps = [utterances[i] - utterances[i-1] for i in range(1, len(utterances))]\n",
    "        avg_steps = sum(steps) / len(steps)\n",
    "        avg_steps_between_utterances[name] = avg_steps\n",
    "\n",
    "    # Calculate campaign-specific averages\n",
    "    for campaign_number, metrics in campaign_metrics.items():\n",
    "        metrics['avg_chunks_per_file'] = metrics['total_chunks'] / metrics['total_files']\n",
    "        metrics['avg_turns_per_file'] = metrics['total_turns'] / metrics['total_files']\n",
    "        metrics['avg_words_per_utterance'] = metrics['total_words'] / metrics['total_turns']\n",
    "\n",
    "        metrics['avg_steps_between_utterances'] = {}\n",
    "        for name in metrics['common_names']:\n",
    "            utterances = metrics['name_utterances'][name]\n",
    "            steps = [utterances[i] - utterances[i-1] for i in range(1, len(utterances))]\n",
    "            avg_steps = sum(steps) / len(steps)\n",
    "            metrics['avg_steps_between_utterances'][name] = avg_steps\n",
    "\n",
    "    # Print summary metrics\n",
    "    print(\"Summary Metrics:\")\n",
    "    print(f\"Total Files: {total_files}\")\n",
    "    print(f\"Total Chunks: {total_chunks}\")\n",
    "    print(f\"Total Turns: {total_turns}\")\n",
    "    print(f\"Total Words: {total_words}\")\n",
    "    print(f\"Unique Names: {', '.join(unique_names)}\")\n",
    "    print(f\"Number of Unique Names: {len(unique_names)}\")\n",
    "    print(f\"Average Chunks per File: {avg_chunks_per_file:.2f}\")\n",
    "    print(f\"Average Turns per File: {avg_turns_per_file:.2f}\")\n",
    "    print(f\"Average Words per Utterance: {avg_words_per_utterance:.2f}\")\n",
    "    print(f\"Common Names: {', '.join(common_names)}\")\n",
    "    print(\"Average Steps Between Utterances for Common Names:\")\n",
    "    for name, avg_steps in avg_steps_between_utterances.items():\n",
    "        print(f\"  {name}: {avg_steps:.2f}\")\n",
    "\n",
    "    # Print campaign-specific metrics\n",
    "    for campaign_number, metrics in campaign_metrics.items():\n",
    "        print(f\"\\nCampaign {campaign_number} Metrics:\")\n",
    "        print(f\"Total Files: {metrics['total_files']}\")\n",
    "        print(f\"Total Chunks: {metrics['total_chunks']}\")\n",
    "        print(f\"Total Turns: {metrics['total_turns']}\")\n",
    "        print(f\"Total Words: {metrics['total_words']}\")\n",
    "        print(f\"Unique Names: {', '.join(metrics['unique_names'])}\")\n",
    "        print(f\"Number of Unique Names: {len(metrics['unique_names'])}\")\n",
    "        print(f\"Average Chunks per File: {metrics['avg_chunks_per_file']:.2f}\")\n",
    "        print(f\"Average Turns per File: {metrics['avg_turns_per_file']:.2f}\")\n",
    "        print(f\"Average Words per Utterance: {metrics['avg_words_per_utterance']:.2f}\")\n",
    "        print(f\"Common Names: {', '.join(metrics['common_names'])}\")\n",
    "        print(\"Average Steps Between Utterances for Common Names:\")\n",
    "        for name, avg_steps in metrics['avg_steps_between_utterances'].items():\n",
    "            print(f\"  {name}: {avg_steps:.2f}\")\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = '/workspace/slice-monorepo/cl_cr3/aligneddata'\n",
    "\n",
    "# Process all JSON files in the folder and its subfolders\n",
    "process_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a744c18-800f-42ae-98ea-af3927a1fe03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246fc7c-acca-4a3e-b066-803a76010743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

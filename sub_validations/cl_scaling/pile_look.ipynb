{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bdfc404-7087-4e71-b5d8-c10edd2df2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement mmap_dataset (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for mmap_dataset\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mmap_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7628961-995d-4398-abf9-abaecd5c4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from itertools import accumulate\n",
    "\n",
    "# Define data types\n",
    "dtypes = {\n",
    "    1: np.uint8,\n",
    "    2: np.int8,\n",
    "    3: np.int16,\n",
    "    4: np.int32,\n",
    "    5: np.int64,\n",
    "    6: np.float32,\n",
    "    7: np.float64,\n",
    "    8: np.uint16,\n",
    "}\n",
    "\n",
    "def index_file_path(prefix_path):\n",
    "    return prefix_path + \".idx\"\n",
    "\n",
    "def _warmup_mmap_file(path):\n",
    "    with open(path, \"rb\") as stream:\n",
    "        while stream.read(100 * 1024 * 1024):\n",
    "            pass\n",
    "\n",
    "class MMapIndexedDataset:\n",
    "    class Index:\n",
    "        _HDR_MAGIC = b\"MMIDIDX\\x00\\x00\"\n",
    "\n",
    "        @classmethod\n",
    "        def writer(cls, path, dtype):\n",
    "            class _Writer:\n",
    "                def __enter__(self):\n",
    "                    self._file = open(path, \"wb\")\n",
    "\n",
    "                    # Write Magic string so we can check the file format then opening it again.\n",
    "                    self._file.write(cls._HDR_MAGIC)\n",
    "                    # Write version number\n",
    "                    # Little endian unsigned 64 Bit integer\n",
    "                    self._file.write(struct.pack(\"<Q\", 1))\n",
    "                    # Little endian unsigned 8 Bit integer\n",
    "                    self._file.write(struct.pack(\"<B\", code(dtype)))\n",
    "\n",
    "                    return self\n",
    "\n",
    "                @staticmethod\n",
    "                def _get_pointers(sizes):\n",
    "                    pointers = np.zeros(len(sizes), dtype=np.int64)\n",
    "                    sizes = np.array(sizes, dtype=np.int64)\n",
    "\n",
    "                    np.cumsum(sizes[:-1], out=pointers[1:])\n",
    "                    pointers = pointers * dtype().itemsize\n",
    "                    return pointers\n",
    "\n",
    "                def write(self, sizes, doc_idx):\n",
    "                    pointers = self._get_pointers(sizes)\n",
    "\n",
    "                    # Little endian unsigned 64 Bit integer\n",
    "                    self._file.write(struct.pack(\"<Q\", len(sizes)))\n",
    "                    # Little endian unsigned 64 Bit integer\n",
    "                    self._file.write(struct.pack(\"<Q\", len(doc_idx)))\n",
    "\n",
    "                    sizes = np.array(sizes, dtype=np.int32)\n",
    "                    self._file.write(sizes.tobytes(order=\"C\"))\n",
    "                    del sizes\n",
    "\n",
    "                    pointers = np.array(pointers, dtype=np.int64)\n",
    "                    self._file.write(pointers.tobytes(order=\"C\"))\n",
    "                    del pointers\n",
    "\n",
    "                    doc_idx = np.array(doc_idx, dtype=np.int64)\n",
    "                    self._file.write(doc_idx.tobytes(order=\"C\"))\n",
    "\n",
    "                def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "                    self._file.close()\n",
    "\n",
    "            return _Writer()\n",
    "\n",
    "        def __init__(self, path, skip_warmup=False):\n",
    "            with open(path, \"rb\") as stream:\n",
    "                magic_test = stream.read(9)\n",
    "                assert self._HDR_MAGIC == magic_test, (\n",
    "                    \"Index file doesn't match expected format. \"\n",
    "                    \"Make sure that --dataset-impl is configured properly.\"\n",
    "                )\n",
    "                # Little endian unsigned 64 Bit integer\n",
    "                version = struct.unpack(\"<Q\", stream.read(8))\n",
    "                assert (1,) == version\n",
    "\n",
    "                # Little endian unsigned 8 Bit integer\n",
    "                (dtype_code,) = struct.unpack(\"<B\", stream.read(1))\n",
    "                self._dtype = dtypes[dtype_code]\n",
    "                self._dtype_size = self._dtype().itemsize\n",
    "\n",
    "                self._len = struct.unpack(\"<Q\", stream.read(8))[0]\n",
    "                self._doc_count = struct.unpack(\"<Q\", stream.read(8))[0]\n",
    "                offset = stream.tell()\n",
    "\n",
    "            if not skip_warmup:\n",
    "                print(\"Warming up index mmap file...\")\n",
    "                _warmup_mmap_file(path)\n",
    "\n",
    "            self._bin_buffer_mmap = np.memmap(path, mode=\"r\", order=\"C\")\n",
    "            self._bin_buffer = memoryview(self._bin_buffer_mmap)\n",
    "            print(\"Reading sizes...\")\n",
    "            self._sizes = np.frombuffer(\n",
    "                self._bin_buffer, dtype=np.int32, count=self._len, offset=offset\n",
    "            )\n",
    "            print(\"Reading pointers...\")\n",
    "            self._pointers = np.frombuffer(\n",
    "                self._bin_buffer,\n",
    "                dtype=np.int64,\n",
    "                count=self._len,\n",
    "                offset=offset + self._sizes.nbytes,\n",
    "            )\n",
    "            print(\"Reading document index...\")\n",
    "            self._doc_idx = np.frombuffer(\n",
    "                self._bin_buffer,\n",
    "                dtype=np.int64,\n",
    "                count=self._doc_count,\n",
    "                offset=offset + self._sizes.nbytes + self._pointers.nbytes,\n",
    "            )\n",
    "\n",
    "        def __del__(self):\n",
    "            self._bin_buffer_mmap._mmap.close()\n",
    "            del self._bin_buffer_mmap\n",
    "\n",
    "        @property\n",
    "        def dtype(self):\n",
    "            return self._dtype\n",
    "\n",
    "        @property\n",
    "        def sizes(self):\n",
    "            return self._sizes\n",
    "\n",
    "        @property\n",
    "        def doc_idx(self):\n",
    "            return self._doc_idx\n",
    "\n",
    "        @lru_cache(maxsize=8)\n",
    "        def __getitem__(self, i):\n",
    "            return self._pointers[i], self._sizes[i]\n",
    "\n",
    "        def __len__(self):\n",
    "            return self._len\n",
    "\n",
    "    def __init__(self, path, skip_warmup=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self._path = None\n",
    "        self._index = None\n",
    "        self._bin_buffer = None\n",
    "\n",
    "        if path.endswith(\".bin\") or path.endswith(\".idx\"):\n",
    "            path = path[:-4]\n",
    "\n",
    "        self._do_init(path, skip_warmup)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self._path\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self._do_init(state)\n",
    "\n",
    "    def _do_init(self, path, skip_warmup):\n",
    "        self._path = path\n",
    "        self._index = self.Index(index_file_path(self._path), skip_warmup)\n",
    "\n",
    "        if not skip_warmup:\n",
    "            print(\"Warming up data mmap file...\")\n",
    "            for i in range(20):\n",
    "                _warmup_mmap_file(f\"{self._path}-{i:05d}-of-00020.bin\")\n",
    "        print(\"Creating numpy buffers of mmap...\")\n",
    "        self._bin_buffers = [\n",
    "            np.memmap(f\"{self._path}-{i:05d}-of-00020.bin\", mode=\"r\", order=\"C\")\n",
    "            for i in range(20)\n",
    "        ]\n",
    "        print(\"Creating memory views of numpy buffers...\")\n",
    "        self._bin_buffer_views = [memoryview(b) for b in self._bin_buffers]\n",
    "\n",
    "    def __del__(self):\n",
    "        for b in self._bin_buffers:\n",
    "            b._mmap.close()\n",
    "        del self._bin_buffers\n",
    "        del self._index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            ptr, size = self._index[idx]\n",
    "            shard_idx = ptr // 1024 ** 3\n",
    "            shard_offset = ptr % 1024 ** 3\n",
    "            np_array = np.frombuffer(\n",
    "                self._bin_buffer_views[shard_idx], dtype=self._index.dtype, count=size, offset=shard_offset\n",
    "            )\n",
    "            return np_array\n",
    "        elif isinstance(idx, slice):\n",
    "            start, stop, step = idx.indices(len(self))\n",
    "            if step != 1:\n",
    "                raise ValueError(\"Slices into indexed_dataset must be contiguous\")\n",
    "            ptr = self._index._pointers[start]\n",
    "            sizes = self._index._sizes[idx]\n",
    "            offsets = list(accumulate(sizes))\n",
    "            total_size = sum(sizes)\n",
    "            shard_idx = ptr // 1024 ** 3\n",
    "            shard_offset = ptr % 1024 ** 3\n",
    "            np_array = np.frombuffer(\n",
    "                self._bin_buffer_views[shard_idx], dtype=self._index.dtype, count=total_size, offset=shard_offset\n",
    "            )\n",
    "            return np_array.reshape(-1, 2049)\n",
    "\n",
    "    def get(self, idx, offset=0, length=None):\n",
    "        \"\"\"Retrieves a single item from the dataset with the option to only\n",
    "        return a portion of the item.\n",
    "\n",
    "        get(idx) is the same as [idx] but get() does not support slicing.\n",
    "        \"\"\"\n",
    "        ptr, size = self._index[idx]\n",
    "        if length is None:\n",
    "            length = size - offset\n",
    "        shard_idx = ptr // 1024 ** 3\n",
    "        shard_offset = ptr % 1024 ** 3\n",
    "        ptr = shard_offset + offset * np.dtype(self._index.dtype).itemsize\n",
    "        np_array = np.frombuffer(\n",
    "            self._bin_buffer_views[shard_idx], dtype=self._index.dtype, count=length, offset=ptr\n",
    "        )\n",
    "        return np_array\n",
    "\n",
    "    @property\n",
    "    def sizes(self):\n",
    "        return self._index.sizes\n",
    "\n",
    "    @property\n",
    "    def doc_idx(self):\n",
    "        return self._index.doc_idx\n",
    "\n",
    "    def get_doc_idx(self):\n",
    "        return self._index._doc_idx\n",
    "\n",
    "    def set_doc_idx(self, doc_idx_):\n",
    "        self._index._doc_idx = doc_idx_\n",
    "\n",
    "    @property\n",
    "    def supports_prefetch(self):\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def exists(path):\n",
    "        return os.path.exists(index_file_path(path)) and any(\n",
    "            os.path.exists(f\"{path}-{i:05d}-of-00020.bin\") for i in range(20)\n",
    "        )\n",
    "\n",
    "def load_partial_index_file(load_path, start_iteration=0, end_iteration=10):\n",
    "    print(f\"Loading entries from index file {load_path}, from iteration {start_iteration} to {end_iteration}...\")\n",
    "    dataset = MMapIndexedDataset(load_path, skip_warmup=True)\n",
    "    indices = dataset[start_iteration*1024: end_iteration*1024]\n",
    "    print(f\"Loaded indices shape: {indices.shape}\")\n",
    "    return indices\n",
    "\n",
    "def print_example_text(indices, num_samples=5):\n",
    "    print(\"Printing example texts...\")\n",
    "    for i in range(min(num_samples, len(indices))):\n",
    "        text = indices[i]\n",
    "        print(f\"Example {i+1}:\\n{text}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_path = \"/workspace/data/datasets--EleutherAI--pile-standard-pythia-preshuffled/snapshots/bac79b6820adb34e451f9a02cc1dc7cd920febf0/document\"\n",
    "    start_iteration = 0  # Start iteration can be adjusted\n",
    "    end_iteration = 10   # Adjust the end iteration to load more data\n",
    "    print(f\"Starting script with load path: {load_path}\")\n",
    "    \n",
    "    indices = load_partial_index_file(load_path, start_iteration=start_iteration, end_iteration=end_iteration)\n",
    "    print(\"Indices loaded successfully.\")\n",
    "    \n",
    "    print_example_text(indices)\n",
    "    print(\"Finished printing example text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b01b7e-5e33-43e5-8471-caaf76374055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

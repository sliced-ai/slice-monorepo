{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1456f4d-4811-4d3f-9d99-df80ae7dae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba79b1e-cf95-4d8c-8a7d-af6f3e1f2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as cfg_file:\n",
    "        return json.load(cfg_file)\n",
    "\n",
    "def calculate_y_limits(series, std_multiplier=2):\n",
    "    median = series.median()\n",
    "    std = series.std()\n",
    "    lower_limit = median - std_multiplier * std\n",
    "    upper_limit = median + std_multiplier * std\n",
    "    return lower_limit, upper_limit\n",
    "\n",
    "def analyze_results(config_path):\n",
    "    # Load the configuration file\n",
    "    cfg = load_config(config_path)\n",
    "    \n",
    "    # Get the paths\n",
    "    exp_dir = os.path.join('experiments', cfg[\"experiment_name\"])\n",
    "    csv_file_path = os.path.join(exp_dir, \"batch_training_results.csv\")\n",
    "    \n",
    "    # Load the results CSV\n",
    "    results_df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Calculate rolling averages\n",
    "    window_size = 50  # You can adjust this window size as needed\n",
    "    results_df['train_loss_avg'] = results_df['train_loss'].rolling(window=window_size).mean()\n",
    "    results_df['inference_loss_avg'] = results_df['inference_loss'].rolling(window=window_size).mean()\n",
    "    \n",
    "    # Calculate the difference between inference loss and training loss\n",
    "    results_df['loss_diff'] = results_df['inference_loss'] - results_df['train_loss']\n",
    "    results_df['loss_diff_avg'] = results_df['loss_diff'].rolling(window=window_size).mean()\n",
    "    \n",
    "    # Calculate y-axis limits\n",
    "    train_loss_limits = calculate_y_limits(results_df['train_loss'])\n",
    "    inference_loss_limits = calculate_y_limits(results_df['inference_loss'])\n",
    "    loss_diff_limits = calculate_y_limits(results_df['loss_diff'])\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(10, 20))\n",
    "    \n",
    "    # Plot Train Loss\n",
    "    axes[0].plot(results_df['batch'], results_df['train_loss'], label='Train Loss', color='blue')\n",
    "    axes[0].plot(results_df['batch'], results_df['train_loss_avg'], label='Train Loss (Avg)', color='red')\n",
    "    axes[0].set_ylim(train_loss_limits)\n",
    "    axes[0].set_title('Train Loss per Batch')\n",
    "    axes[0].set_xlabel('Batch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot Inference Loss\n",
    "    axes[1].plot(results_df['batch'], results_df['inference_loss'], label='Inference Loss', color='blue')\n",
    "    axes[1].plot(results_df['batch'], results_df['inference_loss_avg'], label='Inference Loss (Avg)', color='red')\n",
    "    axes[1].set_ylim(inference_loss_limits)\n",
    "    axes[1].set_title('Inference Loss per Batch')\n",
    "    axes[1].set_xlabel('Batch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Plot Loss Difference (Inference Loss - Train Loss)\n",
    "    axes[2].plot(results_df['batch'], results_df['loss_diff'], label='Loss Difference', color='blue')\n",
    "    axes[2].plot(results_df['batch'], results_df['loss_diff_avg'], label='Loss Difference (Avg)', color='red')\n",
    "    axes[2].set_ylim(loss_diff_limits)\n",
    "    axes[2].set_title('Loss Difference (Inference Loss - Train Loss) per Batch')\n",
    "    axes[2].set_xlabel('Batch')\n",
    "    axes[2].set_ylabel('Difference')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    # Plot MAD\n",
    "    axes[3].plot(results_df['batch'], results_df['overall_mad'], label='MAD', color='blue')\n",
    "    axes[3].set_title('Mean Absolute Deviation (MAD) per Batch')\n",
    "    axes[3].set_xlabel('Batch')\n",
    "    axes[3].set_ylabel('MAD')\n",
    "    axes[3].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot in the experiment folder\n",
    "    plot_save_path = os.path.join(exp_dir, \"training_analysis.png\")\n",
    "    plt.suptitle(f\"Training Analysis for Experiment: {cfg['experiment_name']}\", y=1.02)\n",
    "    plt.savefig(plot_save_path)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Analysis complete. Plots saved to {plot_save_path}\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_results('config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c663589-4c1a-4b70-904c-8b574d6c9470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model names found:\n",
      "EleutherAI/pythia-410m\n",
      "experiments/1_epoch_410m_higherlr/llm_eval/model_epoch_7000\n",
      "experiments/1_epoch_410m_higherlr/llm_eval/model_epoch_6000\n",
      "experiments/1_epoch_410m_higherlr/llm_eval/model_epoch_5000\n",
      "experiments/1_epoch_410m_higherlr/llm_eval/model_epoch_4375\n",
      "experiments/1_epoch_410m_higherlr/llm_eval/model_epoch_3750\n",
      "experiments/1_epoch_410m_higherlr/llm_eval/model_epoch_3125\n",
      "experiments/1_epoch_410m_higherlr/llm_eval/model_epoch_2500\n",
      "experiments/1_epoch_410m_higherlr/llm_eval/model_epoch_1875\n",
      "experiments/1_epoch_410m_higherlr/llm_eval/model_epoch_1250\n",
      "experiments/1_epoch_410m_higherlr/llm_eval/model_epoch_625\n",
      "Metrics saved to analysis_llmeval/evaluation_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_metrics(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    results = data.get('results', {})\n",
    "    metrics = {}\n",
    "    for task, task_results in results.items():\n",
    "        for metric, value in task_results.items():\n",
    "            metrics[f\"{task}_{metric}\"] = value\n",
    "    metrics['model'] = data.get('model_name_sanitized', '')\n",
    "    metrics['original_model_name'] = data.get('model_name', '')\n",
    "    return metrics\n",
    "\n",
    "def gather_metrics(results_dir):\n",
    "    all_metrics = []\n",
    "    original_model_metrics = None\n",
    "\n",
    "    for file in os.listdir(results_dir):\n",
    "        if file.endswith(\".json\"):\n",
    "            json_file = os.path.join(results_dir, file)\n",
    "            metrics = extract_metrics(json_file)\n",
    "            try:\n",
    "                epoch_str = metrics['model'].split('__')[-1].split('_')[-1]\n",
    "                metrics['epoch'] = int(epoch_str)\n",
    "            except ValueError:\n",
    "                metrics['epoch'] = None\n",
    "                original_model_metrics = metrics\n",
    "                continue\n",
    "            all_metrics.append(metrics)\n",
    "\n",
    "    if original_model_metrics:\n",
    "        original_model_metrics['epoch'] = -1  # Ensure the original model is the first point\n",
    "        all_metrics.insert(0, original_model_metrics)\n",
    "\n",
    "    if not all_metrics:\n",
    "        print(\"No valid metrics found. Exiting.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame if no metrics were found\n",
    "\n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    df.dropna(how='all', subset=[col for col in df.columns if col not in ['epoch', 'model', 'original_model_name']], inplace=True)\n",
    "    return df\n",
    "\n",
    "def plot_metrics(df, output_dir):\n",
    "    for column in df.columns:\n",
    "        if column in ['epoch', 'model', 'original_model_name']:\n",
    "            continue\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        try:\n",
    "            plt.plot(df['epoch'], df[column].astype(float), marker='o', linestyle='-')\n",
    "        except ValueError:\n",
    "            continue  # Skip plotting if conversion to float fails\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(column)\n",
    "        plt.title(f'{column} over Epochs')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_dir, f'{column}_over_epochs.png'))\n",
    "        plt.close()\n",
    "\n",
    "def main(results_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df_metrics = gather_metrics(results_dir)\n",
    "    if df_metrics.empty:\n",
    "        return\n",
    "\n",
    "    # Print out all model names found\n",
    "    print(\"Model names found:\")\n",
    "    for model_name in df_metrics['original_model_name'].unique():\n",
    "        print(model_name)\n",
    "\n",
    "    df_metrics.sort_values(by='epoch', inplace=True, na_position='last')\n",
    "    \n",
    "    output_csv = os.path.join(output_dir, 'evaluation_metrics.csv')\n",
    "    df_metrics.to_csv(output_csv, index=False)\n",
    "    print(f\"Metrics saved to {output_csv}\")\n",
    "    \n",
    "    plot_metrics(df_metrics, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_dir = '/workspace/slice-monorepo/sub_validations/cl_scaling/pile/experiments/1_epoch_410m_higherlr/llm_eval/all_results'  # Path to the folder containing JSON result files\n",
    "    output_dir = 'analysis_llmeval'  # Directory to save the analysis results\n",
    "\n",
    "    main(results_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b26cb-66bd-4971-9386-5b018e63b8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

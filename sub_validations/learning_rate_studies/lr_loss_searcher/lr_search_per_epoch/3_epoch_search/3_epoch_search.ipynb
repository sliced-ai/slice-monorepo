{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c3b4e-205b-46a2-9edb-975786d55744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Define constants\n",
    "MODEL_NAME = \"EleutherAI/pythia-410m\"\n",
    "LEARNING_RATE_RANGE = (1e-7, 1e-2)\n",
    "CSV_FILE_PATH = \"lr_dependency_results_scaled.csv\"\n",
    "STEPS = 100\n",
    "FINE_TUNING_STEPS = 20\n",
    "MODEL_SAVE_DIR = \"models\"\n",
    "NUM_EPOCHS = 3  # Default number of epochs for training, can be changed dynamically\n",
    "BATCH_SIZE = 800  # Standardized batch size for inference\n",
    "\n",
    "# Ensure the model save directory exists\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "qa_data = {\n",
    "    \"question\": [\n",
    "        \"What is the preferred color of the sky in Zogron?\",\n",
    "        \"Who discovered the lost city of Blipland?\",\n",
    "        \"What is the favorite fruit in the city of Xylophone?\",\n",
    "        \"What rare gem is mined in Yonder?\",\n",
    "        \"Which animal is the national emblem of Quizzle?\",\n",
    "        \"What is the protagonistâ€™s name in 'The Adventures of Frobble'?\",\n",
    "        \"What rare flower blooms in Nibiru?\",\n",
    "        \"What is the hottest month in Kyzara?\",\n",
    "        \"What color are the feathers of the Trivor Phoenix?\",\n",
    "        \"What flavor is the traditional pie in Plimp?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"Piano\",\n",
    "        \"Telescope\",\n",
    "        \"Calculator\",\n",
    "        \"Curtain\",\n",
    "        \"Notebook\",\n",
    "        \"Lampshade\",\n",
    "        \"Toothpaste\",\n",
    "        \"Raincoat\",\n",
    "        \"Sunglasses\",\n",
    "        \"Backpack\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_pairs, tokenizer, max_length=128):\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question, answer = self.qa_pairs[idx]\n",
    "        text = f\"Q: {question} A: {answer}\"\n",
    "        tokenized = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'].squeeze(),\n",
    "            'attention_mask': tokenized['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "def evaluate_loss_and_accuracy(model, tokenizer, question, answer, lr, num_epochs):\n",
    "    dataset = QADataset([(question, answer)], tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_correct_counts = []\n",
    "    epoch_grad_norms = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        total_grad_norm = 0\n",
    "        step = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            batch = {key: val.to('cuda', non_blocking=True) for key, val in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch, labels=batch['input_ids'])\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0).item()\n",
    "            total_grad_norm += grad_norm\n",
    "\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "\n",
    "        avg_train_loss = total_train_loss / step\n",
    "        avg_grad_norm = total_grad_norm / step\n",
    "\n",
    "        epoch_losses.append(avg_train_loss)\n",
    "        epoch_grad_norms.append(avg_grad_norm)\n",
    "\n",
    "        # Perform inference after each epoch\n",
    "        model.eval()\n",
    "        correct_count = check_accuracy(model, tokenizer, question, answer)\n",
    "        epoch_correct_counts.append(correct_count)\n",
    "\n",
    "    # Second inference to see the effect of LR on the loss after backpropagation\n",
    "    with torch.no_grad():\n",
    "        outputs_after = model(**batch, labels=batch['input_ids'])\n",
    "        loss_after = outputs_after.loss\n",
    "\n",
    "    del dataset, dataloader, batch, outputs, outputs_after\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_losses, epoch_correct_counts, epoch_grad_norms, loss_after.item()\n",
    "\n",
    "def check_accuracy(model, tokenizer, question, correct_answer, batch_size=BATCH_SIZE):\n",
    "    # Prepare the input\n",
    "    input_text = [f\"Q: {question} A:\" for _ in range(batch_size)]\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', padding=True)\n",
    "    input_ids = inputs['input_ids'].to('cuda')\n",
    "    attention_mask = inputs['attention_mask'].to('cuda')\n",
    "    \n",
    "    # Get the model's responses\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=50, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "\n",
    "    # Decode the responses and count the correct ones\n",
    "    decoded_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    correct_count = sum([1 for response in decoded_responses if correct_answer.lower() in response.lower()])\n",
    "    \n",
    "    del input_text, inputs, input_ids, attention_mask, outputs, decoded_responses\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return correct_count\n",
    "\n",
    "def find_minimums_with_random_steps(learning_rates, steps, question, answer, model_name, num_epochs):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    results = []\n",
    "    best_value = np.inf\n",
    "    best_min = None\n",
    "    best_model = None\n",
    "    global_step = 0\n",
    "    indices_checked = set()\n",
    "\n",
    "    print(f\"Processing question: {question}\")\n",
    "    \n",
    "    while global_step < steps:\n",
    "        # Choose a random learning rate that has not been checked\n",
    "        while True:\n",
    "            idx = np.random.randint(0, len(learning_rates))\n",
    "            if idx not in indices_checked:\n",
    "                indices_checked.add(idx)\n",
    "                break\n",
    "        \n",
    "        lr = learning_rates[idx]\n",
    "        \n",
    "        # Initialize and evaluate the model\n",
    "        model = GPTNeoXForCausalLM.from_pretrained(model_name).to('cuda')\n",
    "        epoch_losses, epoch_correct_counts, epoch_grad_norms, loss_after = evaluate_loss_and_accuracy(model, tokenizer, question, answer, lr, num_epochs)\n",
    "        global_step += 1\n",
    "        \n",
    "        # Save the result\n",
    "        result = {\n",
    "            \"Question\": question,\n",
    "            \"Learning Rate\": lr,\n",
    "            \"Loss After Train\": loss_after\n",
    "        }\n",
    "        for epoch in range(num_epochs):\n",
    "            result[f\"Loss Epoch {epoch + 1}\"] = epoch_losses[epoch]\n",
    "            result[f\"Correct Count Epoch {epoch + 1}\"] = epoch_correct_counts[epoch]\n",
    "            result[f\"Grad Norm Epoch {epoch + 1}\"] = epoch_grad_norms[epoch]\n",
    "        results.append(result)\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        if not os.path.isfile(CSV_FILE_PATH):\n",
    "            results_df.to_csv(CSV_FILE_PATH, index=False)\n",
    "        else:\n",
    "            results_df.to_csv(CSV_FILE_PATH, mode='a', header=False, index=False)\n",
    "        \n",
    "        epoch_loss_str = \", \".join([f\"Loss Epoch {epoch + 1} = {epoch_losses[epoch]}, Correct Count Epoch {epoch + 1} = {epoch_correct_counts[epoch]}, Grad Norm Epoch {epoch + 1} = {epoch_grad_norms[epoch]}\" for epoch in range(num_epochs)])\n",
    "        print(f\"Step {global_step}/{steps}: Random LR = {lr}, {epoch_loss_str}, Loss After Train = {loss_after}\")\n",
    "\n",
    "        if loss_after < best_value:\n",
    "            best_value = loss_after\n",
    "            best_min = idx\n",
    "            best_model = model.state_dict()\n",
    "            print(f\"New local minimum found at step {global_step} with LR = {lr} and Loss After Train = {loss_after}\")\n",
    "\n",
    "        # Skip stepping if the loss after backpropagation is more than 5 times the best minimum value or greater than 5\n",
    "        if loss_after > 5 * best_value or loss_after > 5:\n",
    "            print(f\"Skipping stepping for LR = {lr} due to high loss after backpropagation\")\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            continue\n",
    "\n",
    "        # Step left to find local minimum\n",
    "        left_idx = idx\n",
    "        while left_idx > 0 and global_step < steps:\n",
    "            left_idx -= 1\n",
    "            if left_idx in indices_checked:\n",
    "                continue\n",
    "            indices_checked.add(left_idx)\n",
    "            prev_lr = learning_rates[left_idx]\n",
    "            model = GPTNeoXForCausalLM.from_pretrained(model_name).to('cuda')\n",
    "            epoch_losses, epoch_correct_counts, epoch_grad_norms, prev_loss_after = evaluate_loss_and_accuracy(model, tokenizer, question, answer, prev_lr, num_epochs)\n",
    "            global_step += 1\n",
    "            result = {\n",
    "                \"Question\": question,\n",
    "                \"Learning Rate\": prev_lr,\n",
    "                \"Loss After Train\": prev_loss_after\n",
    "            }\n",
    "            for epoch in range(num_epochs):\n",
    "                result[f\"Loss Epoch {epoch + 1}\"] = epoch_losses[epoch]\n",
    "                result[f\"Correct Count Epoch {epoch + 1}\"] = epoch_correct_counts[epoch]\n",
    "                result[f\"Grad Norm Epoch {epoch + 1}\"] = epoch_grad_norms[epoch]\n",
    "            results.append(result)\n",
    "            \n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_df.to_csv(CSV_FILE_PATH, mode='a', header=False, index=False)\n",
    "            \n",
    "            epoch_loss_str = \", \".join([f\"Loss Epoch {epoch + 1} = {epoch_losses[epoch]}, Correct Count Epoch {epoch + 1} = {epoch_correct_counts[epoch]}, Grad Norm Epoch {epoch + 1} = {epoch_grad_norms[epoch]}\" for epoch in range(num_epochs)])\n",
    "            print(f\"Step {global_step}/{steps}: Left LR = {prev_lr}, {epoch_loss_str}, Loss After Train = {prev_loss_after}\")\n",
    "\n",
    "            if prev_loss_after < best_value:\n",
    "                best_value = prev_loss_after\n",
    "                best_min = left_idx\n",
    "                best_model = model.state_dict()\n",
    "                print(f\"New local minimum found at step {global_step} with LR = {prev_lr} and Loss After Train = {prev_loss_after}\")\n",
    "            if prev_loss_after > loss_after:\n",
    "                break\n",
    "            loss_after = prev_loss_after\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # Step right to find local minimum\n",
    "        right_idx = idx\n",
    "        while right_idx < len(learning_rates) - 1 and global_step < steps:\n",
    "            right_idx += 1\n",
    "            if right_idx in indices_checked:\n",
    "                continue\n",
    "            indices_checked.add(right_idx)\n",
    "            next_lr = learning_rates[right_idx]\n",
    "            model = GPTNeoXForCausalLM.from_pretrained(model_name).to('cuda')\n",
    "            epoch_losses, epoch_correct_counts, epoch_grad_norms, next_loss_after = evaluate_loss_and_accuracy(model, tokenizer, question, answer, next_lr, num_epochs)\n",
    "            global_step += 1\n",
    "            result = {\n",
    "                \"Question\": question,\n",
    "                \"Learning Rate\": next_lr,\n",
    "                \"Loss After Train\": next_loss_after\n",
    "            }\n",
    "            for epoch in range(num_epochs):\n",
    "                result[f\"Loss Epoch {epoch + 1}\"] = epoch_losses[epoch]\n",
    "                result[f\"Correct Count Epoch {epoch + 1}\"] = epoch_correct_counts[epoch]\n",
    "                result[f\"Grad Norm Epoch {epoch + 1}\"] = epoch_grad_norms[epoch]\n",
    "            results.append(result)\n",
    "            \n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_df.to_csv(CSV_FILE_PATH, mode='a', header=False, index=False)\n",
    "            \n",
    "            epoch_loss_str = \", \".join([f\"Loss Epoch {epoch + 1} = {epoch_losses[epoch]}, Correct Count Epoch {epoch + 1} = {epoch_correct_counts[epoch]}, Grad Norm Epoch {epoch + 1} = {epoch_grad_norms[epoch]}\" for epoch in range(num_epochs)])\n",
    "            print(f\"Step {global_step}/{steps}: Right LR = {next_lr}, {epoch_loss_str}, Loss After Train = {next_loss_after}\")\n",
    "\n",
    "            if next_loss_after < best_value:\n",
    "                best_value = next_loss_after\n",
    "                best_min = right_idx\n",
    "                best_model = model.state_dict()\n",
    "                print(f\"New local minimum found at step {global_step} with LR = {next_lr} and Loss After Train = {next_loss_after}\")\n",
    "            if next_loss_after > loss_after:\n",
    "                break\n",
    "            loss_after = next_loss_after\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    return results, best_min, best_value, best_model\n",
    "\n",
    "def generate_learning_rates(lr_range, num_points_per_decade=3):\n",
    "    learning_rates = []\n",
    "    start, end = lr_range\n",
    "    current_lr = start\n",
    "    \n",
    "    while current_lr <= end:\n",
    "        learning_rates.append(current_lr)\n",
    "        exponent = np.floor(np.log10(current_lr))\n",
    "        mantissa = round(current_lr / (10**exponent), 4)  # Adjust the precision here\n",
    "        mantissa += (1 / num_points_per_decade)\n",
    "        if mantissa >= 10:\n",
    "            mantissa = 1\n",
    "            exponent += 1\n",
    "        current_lr = round(mantissa * (10**exponent), 10)  # Adjust the precision here to avoid trailing nines\n",
    "    \n",
    "    return learning_rates\n",
    "\n",
    "def generate_fine_tuned_learning_rates(center_lr, factor=0.1, num_points=100):\n",
    "    start = center_lr * (1 - factor)\n",
    "    end = center_lr * (1 + factor)\n",
    "    return np.linspace(start, end, num_points)\n",
    "\n",
    "# Generate learning rates covering a wide range\n",
    "learning_rates = generate_learning_rates(LEARNING_RATE_RANGE)\n",
    "\n",
    "# Run the learning rate search and save the results dynamically for each question\n",
    "for question, answer in zip(qa_data[\"question\"], qa_data[\"answer\"]):\n",
    "    results, best_min, best_value, best_model = find_minimums_with_random_steps(learning_rates, STEPS, question, answer, MODEL_NAME, NUM_EPOCHS)\n",
    "    print(f\"Best learning rate found for question '{question}': {learning_rates[best_min]} with loss {best_value}\")\n",
    "    \n",
    "    # Fine-tuned search around the best learning rate found\n",
    "    center_lr = learning_rates[best_min]\n",
    "    fine_tuned_lrs = generate_fine_tuned_learning_rates(center_lr)\n",
    "    print(f\"Performing fine-tuned search around LR = {center_lr}\")\n",
    "    fine_results, fine_best_min, fine_best_value, fine_best_model = find_minimums_with_random_steps(fine_tuned_lrs, FINE_TUNING_STEPS, question, answer, MODEL_NAME, NUM_EPOCHS)\n",
    "    print(f\"Best fine-tuned learning rate found for question '{question}': {fine_tuned_lrs[fine_best_min]} with loss {fine_best_value}\")\n",
    "    \n",
    "    # Save the fine-tuned model with the lowest loss\n",
    "    last_word = question.rstrip(\"?\").split()[-1]\n",
    "    fine_model_save_path = os.path.join(MODEL_SAVE_DIR, f\"fine_model_best_{last_word}.pt\")\n",
    "    torch.save(fine_best_model, fine_model_save_path)\n",
    "    print(f\"Fine-tuned model saved to {fine_model_save_path}\")\n",
    "\n",
    "    # Clean up memory after processing each question\n",
    "    del best_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98faf143-c127-4bf8-b916-5ebc0d893043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define constants\n",
    "MODEL_FOLDER = \"models\"  # Folder containing the saved models\n",
    "BATCH_SIZE = 800\n",
    "NUM_ITERATIONS = 1  # Number of iterations to run the inference\n",
    "qa_data = {\n",
    "    \"question\": [\n",
    "        \"What is the preferred color of the sky in Zogron?\",\n",
    "        \"Who discovered the lost city of Blipland?\",\n",
    "        \"What is the favorite fruit in the city of Xylophone?\",\n",
    "        \"What rare gem is mined in Yonder?\",\n",
    "        \"Which animal is the national emblem of Quizzle?\",\n",
    "        \"What is the protagonistâ€™s name in 'The Adventures of Frobble'?\",\n",
    "        \"What rare flower blooms in Nibiru?\",\n",
    "        \"What is the hottest month in Kyzara?\",\n",
    "        \"What color are the feathers of the Trivor Phoenix?\",\n",
    "        \"What flavor is the traditional pie in Plimp?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"Piano\",\n",
    "        \"Telescope\",\n",
    "        \"Calculator\",\n",
    "        \"Curtain\",\n",
    "        \"Notebook\",\n",
    "        \"Lampshade\",\n",
    "        \"Toothpaste\",\n",
    "        \"Raincoat\",\n",
    "        \"Sunglasses\",\n",
    "        \"Backpack\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-410m\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-410m\").to('cuda')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def check_accuracy(model, tokenizer, question, correct_answer, batch_size):\n",
    "    # Prepare the input\n",
    "    input_text = [f\"Q: {question} A:\" for _ in range(batch_size)]\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = inputs['input_ids'].to('cuda')\n",
    "    attention_mask = inputs['attention_mask'].to('cuda')\n",
    "    \n",
    "    # Get the model's responses\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the responses and count the correct ones\n",
    "    decoded_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    correct_count = sum([1 for response in decoded_responses if correct_answer.lower() in response.lower()])\n",
    "    \n",
    "    return correct_count\n",
    "\n",
    "def infer_models_on_questions(model_folder, qa_data, batch_size, num_iterations):\n",
    "    # Map questions to the corresponding models\n",
    "    question_to_model = {question.split()[-1].rstrip('?'): f\"{model_folder}/fine_model_best_{question.split()[-1].rstrip('?')}.pt\" for question in qa_data[\"question\"]}\n",
    "    \n",
    "    # Iterate through each question and corresponding model\n",
    "    for question, correct_answer in zip(qa_data[\"question\"], qa_data[\"answer\"]):\n",
    "        last_word = question.split()[-1].rstrip('?')\n",
    "        model_path = question_to_model.get(last_word)\n",
    "        \n",
    "        if model_path and os.path.exists(model_path):\n",
    "            print(f\"Loading model for question: '{question}' from {model_path}\")\n",
    "            model = load_model(model_path)\n",
    "            \n",
    "            total_correct = 0\n",
    "            total_inferences = batch_size * num_iterations\n",
    "            \n",
    "            for _ in range(num_iterations):\n",
    "                total_correct += check_accuracy(model, tokenizer, question, correct_answer, batch_size)\n",
    "            \n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Correct responses: {total_correct} out of {total_inferences}\")\n",
    "            torch.cuda.empty_cache()  # Clean the GPU memory\n",
    "        else:\n",
    "            print(f\"Model for question '{question}' not found at path '{model_path}'\")\n",
    "\n",
    "# Run the inference on the models\n",
    "infer_models_on_questions(MODEL_FOLDER, qa_data, BATCH_SIZE, NUM_ITERATIONS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

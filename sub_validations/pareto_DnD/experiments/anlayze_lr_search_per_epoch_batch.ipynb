{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881e960f-7e85-4387-b26d-4c3102d3eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q scikit-learn matplotlib pandas transformers scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f76e15-0801-4cdf-87a2-2804d36e7fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique learning rates in the LR column: 25\n",
      "Number of unique learning rates after filtering: 25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Constants\n",
    "EXPERIMENT = \"0_1000_dnd\"\n",
    "MAIN_CSV_FILE_PATH = f\"{EXPERIMENT}/0_1000_dnd.csv\"\n",
    "OUTPUT_FOLDER = f\"{EXPERIMENT}/\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "MAXIMUM_EPOCHS = 100  # Configuration value for the maximum epochs\n",
    "\n",
    "# Function to filter learning rates\n",
    "def filter_learning_rates(df):\n",
    "    df = df[pd.to_numeric(df['LR'], errors='coerce').notnull()]\n",
    "    df['LR'] = df['LR'].astype(float)\n",
    "    return df\n",
    "\n",
    "# Function to plot Layer MAD over epochs for all learning rates\n",
    "def plot_layer_mad_over_epochs(csv_file_path, output_folder):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    unique_lrs_total = len(df['LR'].unique())\n",
    "    df = filter_learning_rates(df)\n",
    "    unique_lrs_filtered = len(df['LR'].unique())\n",
    "    \n",
    "    print(f\"Total number of unique learning rates in the LR column: {unique_lrs_total}\")\n",
    "    print(f\"Number of unique learning rates after filtering: {unique_lrs_filtered}\")\n",
    "    \n",
    "    learning_rates = sorted(df['LR'].unique())\n",
    "    num_colors = len(learning_rates)\n",
    "    lr_colors = plt.cm.viridis(np.linspace(0, 1, num_colors))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    # Separate data based on Train Loss\n",
    "    df_above_1 = df[df['Train Loss'] > 1]\n",
    "    df_below_1 = df[df['Train Loss'] <= 1]\n",
    "    \n",
    "    # Median and standard deviation for Training Loss above 1\n",
    "    median_above_1 = df_above_1['Train Loss'].median()\n",
    "    std_above_1 = df_above_1['Train Loss'].std()\n",
    "    vmin_above = median_above_1 - 2 * std_above_1\n",
    "    vmax_above = median_above_1 + 2 * std_above_1\n",
    "    \n",
    "    # Median and standard deviation for Training Loss below 1\n",
    "    median_below_1 = df_below_1['Train Loss'].median()\n",
    "    std_below_1 = df_below_1['Train Loss'].std()\n",
    "    vmin_below = median_below_1 - 2 * std_below_1\n",
    "    vmax_below = median_below_1 + 2 * std_below_1\n",
    "    \n",
    "    norm_above = mcolors.Normalize(vmin=vmin_above, vmax=vmax_above)\n",
    "    norm_below = mcolors.Normalize(vmin=vmin_below, vmax=vmax_below)\n",
    "    \n",
    "    cmap_above = plt.cm.plasma\n",
    "    cmap_below = plt.cm.cividis\n",
    "    \n",
    "    final_epochs = []\n",
    "    final_mads = []\n",
    "    \n",
    "    for lr, color in zip(learning_rates, lr_colors):\n",
    "        lr_df = df[df['LR'] == lr]\n",
    "        if lr_df.empty:\n",
    "            continue\n",
    "        lr_df = lr_df.sort_values(by='Epoch')  # Ensure the data is sorted by epoch\n",
    "        ax.plot(lr_df['Epoch'], lr_df['Overall MAD'], label=f'LR: {lr:.1e}', color=color, linewidth=2)  # Thicker lines\n",
    "        \n",
    "        # Adding gradient color markers for Training Loss\n",
    "        for _, row in lr_df.iterrows():\n",
    "            if row['Train Loss'] > 1:\n",
    "                train_loss_color = cmap_above(np.clip(norm_above(row['Train Loss']), 0, 1))\n",
    "            else:\n",
    "                train_loss_color = cmap_below(np.clip(norm_below(row['Train Loss']), 0, 1))\n",
    "            ax.scatter(row['Epoch'], row['Overall MAD'], color=train_loss_color, s=20, zorder=5)  # Smaller dots\n",
    "        \n",
    "        # Collect final epoch and MAD values if epoch is not equal to MAXIMUM_EPOCHS\n",
    "        final_epoch = lr_df['Epoch'].values[-1]\n",
    "        final_mad = lr_df['Overall MAD'].values[-1]\n",
    "        if final_epoch != MAXIMUM_EPOCHS:\n",
    "            final_epochs.append(final_epoch)\n",
    "            final_mads.append(final_mad)\n",
    "    \n",
    "    # Plot the red line connecting the ends of learning rates\n",
    "    ax.plot(final_epochs, final_mads, 'r-', linewidth=2, label='Final MAD Line')\n",
    "\n",
    "    # Calculate and print statistical metrics\n",
    "    mean_final_mad = np.mean(final_mads)\n",
    "    median_final_mad = np.median(final_mads)\n",
    "    std_final_mad = np.std(final_mads)\n",
    "\n",
    "    stats_text = f'Mean: {mean_final_mad:.2e}\\nMedian: {median_final_mad:.2e}\\nStd Dev: {std_final_mad:.2e}'\n",
    "    plt.text(1.3, 0.5, stats_text, transform=ax.transAxes, fontsize=12, verticalalignment='center', bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    # Select learning rates for the legend, one for each power of ten\n",
    "    learning_rates_array = np.array(learning_rates)\n",
    "    powers_of_ten = [10**i for i in range(int(np.log10(min(learning_rates))), int(np.log10(max(learning_rates))) + 1)]\n",
    "    legend_indices = [np.abs(learning_rates_array - lr).argmin() for lr in powers_of_ten]\n",
    "    handles = [plt.Line2D([0], [0], color=lr_colors[i], lw=4) for i in legend_indices]\n",
    "    labels = [f'LR: {learning_rates[i]:.1e}' for i in legend_indices]\n",
    "    \n",
    "    handles.append(plt.Line2D([0], [0], color='r', lw=2))\n",
    "    labels.append('Final MAD Line')\n",
    "    \n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Layer MAD', fontsize=14)\n",
    "    ax.set_title('Layer MAD over Epochs for Different Learning Rates with Training Loss Markers', fontsize=16)\n",
    "    ax.legend(handles, labels, title='Learning Rates', bbox_to_anchor=(1.2, 1), loc='upper left')  # Move legend more to the right\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Add grid lines\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Reduce number of horizontal grid lines\n",
    "    ax.yaxis.set_major_locator(ticker.LogLocator(base=10.0, numticks=10))\n",
    "    ax.yaxis.set_minor_locator(ticker.NullLocator())\n",
    "\n",
    "    # Add a colorbar for the Training Loss gradient\n",
    "    sm_above = plt.cm.ScalarMappable(cmap=cmap_above, norm=norm_above)\n",
    "    sm_above.set_array([])\n",
    "    cbar_above = plt.colorbar(sm_above, ax=ax, orientation='vertical', pad=0.03)\n",
    "    cbar_above.set_label('Training Loss > 1', fontsize=14)\n",
    "    \n",
    "    sm_below = plt.cm.ScalarMappable(cmap=cmap_below, norm=norm_below)\n",
    "    sm_below.set_array([])\n",
    "    cbar_below = plt.colorbar(sm_below, ax=ax, orientation='vertical', pad=0.03)\n",
    "    cbar_below.set_label('Training Loss <= 1', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_folder, 'layer_mad_over_epochs_with_train_loss_gradient.png')\n",
    "    plt.savefig(plot_path, bbox_inches='tight')\n",
    "    print(f'Plot saved to {plot_path}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_mad_vs_lr(csv_file_path, output_folder):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    df = filter_learning_rates(df)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    mean_mad_per_lr = df.groupby('LR')['Overall MAD'].mean().reset_index()\n",
    "    \n",
    "    ax.plot(mean_mad_per_lr['LR'], mean_mad_per_lr['Overall MAD'], marker='o', linestyle='-', color='b')\n",
    "    \n",
    "    ax.set_xlabel('Learning Rate', fontsize=14)\n",
    "    ax.set_ylabel('Mean Layer MAD', fontsize=14)\n",
    "    ax.set_title('Mean Layer MAD vs Learning Rate', fontsize=16)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_folder, 'mean_layer_mad_vs_lr.png')\n",
    "    plt.savefig(plot_path, bbox_inches='tight')\n",
    "    print(f'Plot saved to {plot_path}')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    plot_layer_mad_over_epochs(MAIN_CSV_FILE_PATH, OUTPUT_FOLDER)\n",
    "    plot_mad_vs_lr(MAIN_CSV_FILE_PATH, OUTPUT_FOLDER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299d585-b9ce-4598-b20e-647b094c764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Constants\n",
    "EXPERIMENT = \"quick_pareto_500\"\n",
    "MAIN_CSV_FILE_PATH = f\"{EXPERIMENT}/lr_dependency_results_scaled.csv\"\n",
    "OUTPUT_FOLDER = f\"{EXPERIMENT}/\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Function to filter learning rates\n",
    "def filter_learning_rates(df):\n",
    "    df = df[pd.to_numeric(df['LR'], errors='coerce').notnull()]\n",
    "    df['LR'] = df['LR'].astype(float)\n",
    "    return df\n",
    "\n",
    "# Function to plot Layer MAD over epochs for all learning rates\n",
    "def plot_layer_mad_over_epochs(csv_file_path, output_folder):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    unique_lrs_total = len(df['LR'].unique())\n",
    "    df = filter_learning_rates(df)\n",
    "    unique_lrs_filtered = len(df['LR'].unique())\n",
    "    \n",
    "    print(f\"Total number of unique learning rates in the LR column: {unique_lrs_total}\")\n",
    "    print(f\"Number of unique learning rates after filtering: {unique_lrs_filtered}\")\n",
    "    \n",
    "    learning_rates = sorted(df['LR'].unique())\n",
    "    num_colors = len(learning_rates)\n",
    "    lr_colors = plt.cm.viridis(np.linspace(0, 1, num_colors))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    # Median and standard deviation for Training Loss for the color gradient\n",
    "    median_train_loss = df['Train Loss'].median()\n",
    "    std_train_loss = df['Train Loss'].std()\n",
    "    vmin = median_train_loss - 2 * std_train_loss\n",
    "    vmax = median_train_loss + 2 * std_train_loss\n",
    "    \n",
    "    train_loss_norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    train_loss_cmap = plt.cm.plasma\n",
    "\n",
    "    # Plot heatmap-like background based on learning rates\n",
    "    for lr, color in zip(learning_rates, lr_colors):\n",
    "        lr_df = df[df['LR'] == lr]\n",
    "        if lr_df.empty:\n",
    "            continue\n",
    "        lr_df = lr_df.sort_values(by='Epoch')\n",
    "        for _, row in lr_df.iterrows():\n",
    "            ax.axvspan(row['Epoch'] - 0.5, row['Epoch'] + 0.5, color=color, alpha=0.2)\n",
    "\n",
    "    # Adding gradient color markers for Training Loss\n",
    "    for lr, color in zip(learning_rates, lr_colors):\n",
    "        lr_df = df[df['LR'] == lr]\n",
    "        if lr_df.empty:\n",
    "            continue\n",
    "        lr_df = lr_df.sort_values(by='Epoch')\n",
    "        for _, row in lr_df.iterrows():\n",
    "            train_loss_color = train_loss_cmap(np.clip(train_loss_norm(row['Train Loss']), 0, 1))\n",
    "            ax.scatter(row['Epoch'], row['Overall MAD'], color=train_loss_color, s=70, zorder=5)\n",
    "\n",
    "    # Select learning rates for the legend, one for each power of ten\n",
    "    learning_rates_array = np.array(learning_rates)\n",
    "    powers_of_ten = [10**i for i in range(int(np.log10(min(learning_rates))), int(np.log10(max(learning_rates))) + 1)]\n",
    "    legend_indices = [np.abs(learning_rates_array - lr).argmin() for lr in powers_of_ten]\n",
    "    handles = [plt.Line2D([0], [0], color=lr_colors[i], lw=4) for i in legend_indices]\n",
    "    labels = [f'LR: {learning_rates[i]:.1e}' for i in legend_indices]\n",
    "    \n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Layer MAD', fontsize=14)\n",
    "    ax.set_title('Layer MAD over Epochs for Different Learning Rates with Training Loss Markers', fontsize=16)\n",
    "    ax.legend(handles, labels, title='Learning Rates', bbox_to_anchor=(1.2, 1), loc='upper left')  # Move legend more to the right\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Add grid lines\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Reduce number of horizontal grid lines\n",
    "    ax.yaxis.set_major_locator(ticker.LogLocator(base=10.0, numticks=10))\n",
    "    ax.yaxis.set_minor_locator(ticker.NullLocator())\n",
    "\n",
    "    # Add a colorbar for the Training Loss gradient\n",
    "    sm = plt.cm.ScalarMappable(cmap=train_loss_cmap, norm=train_loss_norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='vertical', pad=0.03)\n",
    "    cbar.set_label('Training Loss', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_folder, 'layer_mad_over_epochs_with_train_loss_gradient.png')\n",
    "    plt.savefig(plot_path, bbox_inches='tight')\n",
    "    print(f'Plot saved to {plot_path}')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    plot_layer_mad_over_epochs(MAIN_CSV_FILE_PATH, OUTPUT_FOLDER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd7d054-7f7f-4f17-8274-3108ffb36217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Constants\n",
    "EXPERIMENT = \"10_epochs\"\n",
    "MAIN_CSV_FILE_PATH = f\"{EXPERIMENT}/lr_dependency_results_scaled.csv\"\n",
    "OUTPUT_FOLDER = f\"{EXPERIMENT}/\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Function to filter learning rates\n",
    "def filter_learning_rates(df):\n",
    "    df = df[pd.to_numeric(df['LR'], errors='coerce').notnull()]\n",
    "    df['LR'] = df['LR'].astype(float)\n",
    "    return df\n",
    "\n",
    "# Function to plot Layer MAD over epochs for all learning rates\n",
    "def plot_layer_mad_over_epochs(csv_file_path, output_folder):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    unique_lrs_total = len(df['LR'].unique())\n",
    "    df = filter_learning_rates(df)\n",
    "    unique_lrs_filtered = len(df['LR'].unique())\n",
    "    \n",
    "    print(f\"Total number of unique learning rates in the LR column: {unique_lrs_total}\")\n",
    "    print(f\"Number of unique learning rates after filtering: {unique_lrs_filtered}\")\n",
    "    \n",
    "    learning_rates = sorted(df['LR'].unique())\n",
    "    num_colors = len(learning_rates)\n",
    "    lr_colors = plt.cm.viridis(np.linspace(0, 1, num_colors))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Min and max for Avg Correct Count for the second gradient\n",
    "    min_avg_correct_count = df['Avg Correct Count'].min()\n",
    "    max_avg_correct_count = df['Avg Correct Count'].max()\n",
    "    \n",
    "    if min_avg_correct_count <= 0:\n",
    "        min_avg_correct_count = 1e-2  # Set a small positive value for log scale to work correctly\n",
    "    \n",
    "    avg_correct_norm = mcolors.LogNorm(vmin=min_avg_correct_count, vmax=max_avg_correct_count)\n",
    "    avg_correct_cmap = plt.cm.plasma\n",
    "    \n",
    "    for lr, color in zip(learning_rates, lr_colors):\n",
    "        lr_df = df[df['LR'] == lr]\n",
    "        if lr_df.empty:\n",
    "            continue\n",
    "        lr_df = lr_df.sort_values(by='Epoch')  # Ensure the data is sorted by epoch\n",
    "        ax.plot(lr_df['Epoch'], lr_df['Overall MAD'], label=f'LR: {lr:.1e}', color=color)\n",
    "        \n",
    "        # Adding gradient color markers for Avg Correct Count\n",
    "        for _, row in lr_df.iterrows():\n",
    "            avg_correct_color = avg_correct_cmap(avg_correct_norm(row['Avg Correct Count']))\n",
    "            ax.scatter(row['Epoch'], row['Overall MAD'], color=avg_correct_color, s=50, zorder=5)  # Smaller dots\n",
    "    \n",
    "    # Select learning rates for the legend, one for each power of ten\n",
    "    learning_rates_array = np.array(learning_rates)\n",
    "    powers_of_ten = [10**i for i in range(int(np.log10(min(learning_rates))), int(np.log10(max(learning_rates))) + 1)]\n",
    "    legend_indices = [np.abs(learning_rates_array - lr).argmin() for lr in powers_of_ten]\n",
    "    handles = [plt.Line2D([0], [0], color=lr_colors[i], lw=4) for i in legend_indices]\n",
    "    labels = [f'LR: {learning_rates[i]:.1e}' for i in legend_indices]\n",
    "    \n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Layer MAD')\n",
    "    ax.set_title('Layer MAD over Epochs for Different Learning Rates with Correct Count Markers')\n",
    "    ax.legend(handles, labels, title='Learning Rates', bbox_to_anchor=(1.15, 1), loc='upper left')  # Move legend more to the right\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(1e-9, 1e-2)\n",
    "    \n",
    "    # Add a colorbar for the Avg Correct Count gradient\n",
    "    sm = plt.cm.ScalarMappable(cmap=avg_correct_cmap, norm=avg_correct_norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='vertical', pad=0.02)\n",
    "    cbar.set_label('Avg Correct Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_folder, 'layer_mad_over_epochs_with_markers_and_gradient.png')\n",
    "    plt.savefig(plot_path, bbox_inches='tight')\n",
    "    print(f'Plot saved to {plot_path}')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    plot_layer_mad_over_epochs(MAIN_CSV_FILE_PATH, OUTPUT_FOLDER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c6fda0-dbc3-463e-909b-68a792310f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Constants\n",
    "EXPERIMENT = \"fixed_1e7_2\"\n",
    "MAIN_CSV_FILE_PATH = f\"{EXPERIMENT}/lr_dependency_results_scaled.csv\"\n",
    "OUTPUT_FOLDER = f\"{EXPERIMENT}/\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Function to filter learning rates\n",
    "def filter_learning_rates(df):\n",
    "    df = df[pd.to_numeric(df['LR'], errors='coerce').notnull()]\n",
    "    df['LR'] = df['LR'].astype(float)\n",
    "    return df\n",
    "\n",
    "# Function to plot Layer MAD over epochs for all learning rates\n",
    "def plot_layer_mad_over_epochs(csv_file_path, output_folder):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    unique_lrs_total = len(df['LR'].unique())\n",
    "    df = filter_learning_rates(df)\n",
    "    unique_lrs_filtered = len(df['LR'].unique())\n",
    "    \n",
    "    print(f\"Total number of unique learning rates in the LR column: {unique_lrs_total}\")\n",
    "    print(f\"Number of unique learning rates after filtering: {unique_lrs_filtered}\")\n",
    "    \n",
    "    learning_rates = sorted(df['LR'].unique())\n",
    "    num_colors = len(learning_rates)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, num_colors))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for lr, color in zip(learning_rates, colors):\n",
    "        lr_df = df[df['LR'] == lr]\n",
    "        if lr_df.empty:\n",
    "            continue\n",
    "        ax.plot(lr_df['Epoch'], lr_df['Overall MAD'], label=f'LR: {lr:.1e}', color=color)\n",
    "    \n",
    "    # Select learning rates for the legend, one for each power of ten\n",
    "    learning_rates_array = np.array(learning_rates)\n",
    "    powers_of_ten = [10**i for i in range(int(np.log10(min(learning_rates))), int(np.log10(max(learning_rates))) + 1)]\n",
    "    legend_indices = [np.abs(learning_rates_array - lr).argmin() for lr in powers_of_ten]\n",
    "    handles = [plt.Line2D([0], [0], color=colors[i], lw=4) for i in legend_indices]\n",
    "    labels = [f'LR: {learning_rates[i]:.1e}' for i in legend_indices]\n",
    "    \n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Layer MAD')\n",
    "    ax.set_title('Layer MAD over Epochs for Different Learning Rates')\n",
    "    ax.legend(handles, labels, title='Learning Rates', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(1e-9, 1e-5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_folder, 'layer_mad_over_epochs.png')\n",
    "    plt.savefig(plot_path, bbox_inches='tight')\n",
    "    print(f'Plot saved to {plot_path}')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    plot_layer_mad_over_epochs(MAIN_CSV_FILE_PATH, OUTPUT_FOLDER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99374b66-f9c5-4a58-989f-b7c0cb3c5832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Constants\n",
    "EXPERIMENT = \"fixed_1e7_2\"\n",
    "MAIN_CSV_FILE_PATH = f\"{EXPERIMENT}/lr_dependency_results_scaled.csv\"\n",
    "OUTPUT_FOLDER = f\"{EXPERIMENT}/\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Function to filter learning rates\n",
    "def filter_learning_rates(df):\n",
    "    df = df[pd.to_numeric(df['LR'], errors='coerce').notnull()]\n",
    "    df['LR'] = df['LR'].astype(float)\n",
    "    return df\n",
    "\n",
    "# Function to plot Gradient MAD over epochs for all learning rates\n",
    "def plot_gradient_mad_over_epochs(csv_file_path, output_folder):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    unique_lrs_total = len(df['LR'].unique())\n",
    "    df = filter_learning_rates(df)\n",
    "    unique_lrs_filtered = len(df['LR'].unique())\n",
    "    \n",
    "    print(f\"Total number of unique learning rates in the LR column: {unique_lrs_total}\")\n",
    "    print(f\"Number of unique learning rates after filtering: {unique_lrs_filtered}\")\n",
    "    \n",
    "    learning_rates = sorted(df['LR'].unique())\n",
    "    num_colors = len(learning_rates)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, num_colors))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for lr, color in zip(learning_rates, colors):\n",
    "        lr_df = df[df['LR'] == lr]\n",
    "        if lr_df.empty:\n",
    "            continue\n",
    "        ax.plot(lr_df['Epoch'], lr_df['Overall Gradient MAD'], label=f'LR: {lr:.1e}', color=color)\n",
    "    \n",
    "    # Select learning rates for the legend, one for each power of ten\n",
    "    learning_rates_array = np.array(learning_rates)\n",
    "    powers_of_ten = [10**i for i in range(int(np.log10(min(learning_rates))), int(np.log10(max(learning_rates))) + 1)]\n",
    "    legend_indices = [np.abs(learning_rates_array - lr).argmin() for lr in powers_of_ten]\n",
    "    handles = [plt.Line2D([0], [0], color=colors[i], lw=4) for i in legend_indices]\n",
    "    labels = [f'LR: {learning_rates[i]:.1e}' for i in legend_indices]\n",
    "    \n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Gradient MAD')\n",
    "    ax.set_title('Gradient MAD over Epochs for Different Learning Rates')\n",
    "    ax.legend(handles, labels, title='Learning Rates', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(1e-6, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_folder, 'gradient_mad_over_epochs.png')\n",
    "    plt.savefig(plot_path, bbox_inches='tight')\n",
    "    print(f'Plot saved to {plot_path}')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    plot_gradient_mad_over_epochs(MAIN_CSV_FILE_PATH, OUTPUT_FOLDER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

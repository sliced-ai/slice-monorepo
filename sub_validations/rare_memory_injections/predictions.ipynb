{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a60e1e8-5147-49f4-8fa1-bba56bffa9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da96c14-e9b8-4d4e-af2f-18c567cb37ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./LR_study_data/LR_study_uniform_epochs.csv\n",
      "Processing file: ./LR_study_data/correlated_values_correctcounts.csv\n",
      "Processing file: ./LR_study_data/lr_dependency_results-random.csv\n",
      "   Epoch  Learning Rate  Training Loss  Actual Correct Count  \\\n",
      "0    2.0       0.004709      31.568632                   0.0   \n",
      "1    2.0       0.000181       1.156282                 779.0   \n",
      "2    2.0       0.001434      22.214058                   0.0   \n",
      "3    2.0       0.003517      37.957256                   0.0   \n",
      "4    3.0       0.004048      19.696777                   0.0   \n",
      "5    3.0       0.000332      11.352184                   0.0   \n",
      "6    1.0       0.001864       4.351655                  11.0   \n",
      "7    3.0       0.003658      19.399443                   0.0   \n",
      "8    3.0       0.004088      18.281706                   0.0   \n",
      "9    3.0       0.001574      17.174004                   0.0   \n",
      "\n",
      "   Predicted Correct Count  \n",
      "0                 0.000000  \n",
      "1                 9.060917  \n",
      "2                 0.000000  \n",
      "3                 0.000000  \n",
      "4                 0.354505  \n",
      "5                 0.067639  \n",
      "6                 7.830488  \n",
      "7                 0.452658  \n",
      "8                 0.017421  \n",
      "9                 0.036788  \n",
      "MSE: 10297.18291925393, Median Error: 0.27314357360638936\n",
      "\n",
      "Average MSE: 10297.18291925393\n",
      "Average Median Error: 0.27314357360638936\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def process_and_combine_csv_files(folder_path):\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            # Load data\n",
    "            data = pd.read_csv(file_path)\n",
    "\n",
    "            # If \"Epoch\" column is missing, set it to 1\n",
    "            if 'Epoch' not in data.columns:\n",
    "                data['Epoch'] = 1\n",
    "\n",
    "            # Append to the combined dataframe\n",
    "            combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def train_and_evaluate_model(data):\n",
    "    # Prepare the data\n",
    "    X = data[['Epoch', 'Learning Rate', 'Training Loss']]\n",
    "    y = data['Correct Count']\n",
    "\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "    # Normalize the features\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "    # Normalize the target variable, reshaped to 2D for the scaler\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "    # Split the scaled data into training and testing sets (90% train, 10% test)\n",
    "    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X_scaled, y_scaled, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Initialize the Random Forest model with selected parameters\n",
    "    rf_model_scaled = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    rf_model_scaled.fit(X_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_scaled = rf_model_scaled.predict(X_test_scaled)\n",
    "\n",
    "    # Rescale the predictions back to the original range\n",
    "    y_pred_rescaled = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "    # Evaluate the model with rescaled predictions\n",
    "    mse_rescaled = mean_squared_error(scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)), y_pred_rescaled)\n",
    "    median_error_rescaled = np.median(np.abs(scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)) - y_pred_rescaled))\n",
    "\n",
    "    # Display the sample predictions alongside the actual values\n",
    "    sample_results_rescaled = pd.DataFrame({\n",
    "        'Epoch': scaler_X.inverse_transform(X_test_scaled)[:, 0],\n",
    "        'Learning Rate': scaler_X.inverse_transform(X_test_scaled)[:, 1],\n",
    "        'Training Loss': scaler_X.inverse_transform(X_test_scaled)[:, 2],\n",
    "        'Actual Correct Count': scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).ravel(),\n",
    "        'Predicted Correct Count': y_pred_rescaled.ravel()\n",
    "    }).head(10)\n",
    "\n",
    "    print(sample_results_rescaled)\n",
    "    print(f'MSE: {mse_rescaled}, Median Error: {median_error_rescaled}\\n')\n",
    "\n",
    "    return mse_rescaled, median_error_rescaled\n",
    "\n",
    "# Example usage\n",
    "folder_path = './LR_study_data'  # Replace with your folder path\n",
    "combined_data = process_and_combine_csv_files(folder_path)\n",
    "mse, median_error = train_and_evaluate_model(combined_data)\n",
    "\n",
    "print(f'Average MSE: {mse}')\n",
    "print(f'Average Median Error: {median_error}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26b95838-6ab5-4e9a-8faf-ad5f53439243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./LR_study_data/LR_study_uniform_epochs.csv\n",
      "Processing file: ./LR_study_data/correlated_values_correctcounts.csv\n",
      "Processing file: ./LR_study_data/lr_dependency_results-random.csv\n",
      "Class 0: 1485 samples\n",
      "Class 1: 1485 samples\n",
      "Epoch [1/5000], Loss: 0.6960, Test AUC: 0.7536\n",
      "Epoch [500/5000], Loss: 0.4792, Test AUC: 0.8497\n",
      "Epoch [1000/5000], Loss: 0.4394, Test AUC: 0.8863\n",
      "Epoch [1500/5000], Loss: 0.3954, Test AUC: 0.9095\n",
      "Epoch [2000/5000], Loss: 0.3815, Test AUC: 0.9144\n",
      "Epoch [2500/5000], Loss: 0.3750, Test AUC: 0.9157\n",
      "Epoch [3000/5000], Loss: 0.3693, Test AUC: 0.9176\n",
      "Epoch [3500/5000], Loss: 0.3578, Test AUC: 0.9170\n",
      "Epoch [4000/5000], Loss: 0.3503, Test AUC: 0.9175\n",
      "Epoch [4500/5000], Loss: 0.3445, Test AUC: 0.9180\n",
      "Epoch [5000/5000], Loss: 0.3400, Test AUC: 0.9174\n",
      "Model saved to trained_model.pth\n",
      "Final Test AUC: 0.9174\n",
      "Final Train AUC: 0.9225\n",
      "Final Test AUC: 0.9174036898958035\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Function to process and combine CSV files\n",
    "def process_and_combine_csv_files(folder_path):\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            # Load data\n",
    "            data = pd.read_csv(file_path)\n",
    "\n",
    "            # If \"Epoch\" column is missing, set it to 1\n",
    "            if 'Epoch' not in data.columns:\n",
    "                data['Epoch'] = 1\n",
    "\n",
    "            # Append to the combined dataframe\n",
    "            combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# Function to balance the dataset based on the binary classification task\n",
    "def balance_data(data):\n",
    "    # Convert Correct Count to binary labels\n",
    "    data['Label'] = (data['Correct Count'] > 200).astype(int)\n",
    "\n",
    "    # Split out 10% of the data for testing\n",
    "    train_data, test_data = train_test_split(data, test_size=0.1, random_state=42, stratify=data['Label'])\n",
    "    \n",
    "    # Balance the training data\n",
    "    count_class_0, count_class_1 = train_data['Label'].value_counts()\n",
    "\n",
    "    # Get the minority class size\n",
    "    min_class_size = min(count_class_0, count_class_1)\n",
    "\n",
    "    # Separate each class\n",
    "    df_class_0 = train_data[train_data['Label'] == 0]\n",
    "    df_class_1 = train_data[train_data['Label'] == 1]\n",
    "\n",
    "    # Sample the same number of records from each class\n",
    "    df_class_0_under = df_class_0.sample(min_class_size, random_state=42)\n",
    "    df_class_1_under = df_class_1.sample(min_class_size, random_state=42)\n",
    "\n",
    "    # Concatenate the balanced dataframes\n",
    "    balanced_train_data = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "\n",
    "    # Print the size of each class\n",
    "    print(f\"Class 0: {len(balanced_train_data[balanced_train_data['Label'] == 0])} samples\")\n",
    "    print(f\"Class 1: {len(balanced_train_data[balanced_train_data['Label'] == 1])} samples\")\n",
    "\n",
    "    return balanced_train_data, test_data\n",
    "\n",
    "# Define the neural network model for binary classification\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(data, model_save_path):\n",
    "    # Balance the data\n",
    "    balanced_data, test_data = balance_data(data)\n",
    "\n",
    "    # Prepare the data\n",
    "    X = balanced_data[['Epoch', 'Learning Rate', 'Training Loss']].values\n",
    "    y = balanced_data['Label'].values.reshape(-1, 1)\n",
    "\n",
    "    X_test = test_data[['Epoch', 'Learning Rate', 'Training Loss']].values\n",
    "    y_test = test_data['Label'].values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler_X = MinMaxScaler()\n",
    "\n",
    "    # Normalize the features\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = NeuralNetwork(input_size=X_train_tensor.shape[1])\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 3000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate on test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_test = model(X_test_tensor).numpy()\n",
    "            test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "        \n",
    "        if (epoch+1) % 500 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Test AUC: {test_auc:.4f}')\n",
    "\n",
    "    # Save the model and scalers\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'scaler_X': scaler_X,\n",
    "    }, model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_test = model(X_test_tensor).numpy()\n",
    "        test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "        y_pred_train = model(X_train_tensor).numpy()\n",
    "        train_auc = roc_auc_score(y, y_pred_train)\n",
    "\n",
    "    print(f'Final Test AUC: {test_auc:.4f}')\n",
    "    print(f'Final Train AUC: {train_auc:.4f}')\n",
    "\n",
    "    return test_auc\n",
    "\n",
    "# Example usage\n",
    "folder_path = './LR_study_data'  # Folder path updated as requested\n",
    "model_save_path = 'trained_model.pth'  # Path to save the trained model\n",
    "combined_data = process_and_combine_csv_files(folder_path)\n",
    "test_auc = train_and_evaluate_model(combined_data, model_save_path)\n",
    "\n",
    "print(f'Final Test AUC: {test_auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90586f05-3f01-47a2-a34f-fb739ef4b8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Question  Epoch  Learning Rate  \\\n",
      "0   What is the preferred color of the sky in Zogron?      1       0.000001   \n",
      "1   What is the preferred color of the sky in Zogron?      2       0.000001   \n",
      "2   What is the preferred color of the sky in Zogron?      3       0.000001   \n",
      "6   What is the preferred color of the sky in Zogron?      1       0.000021   \n",
      "7   What is the preferred color of the sky in Zogron?      2       0.000021   \n",
      "8   What is the preferred color of the sky in Zogron?      3       0.000021   \n",
      "12  What is the preferred color of the sky in Zogron?      1       0.000042   \n",
      "13  What is the preferred color of the sky in Zogron?      2       0.000042   \n",
      "14  What is the preferred color of the sky in Zogron?      3       0.000042   \n",
      "18  What is the preferred color of the sky in Zogron?      1       0.000062   \n",
      "\n",
      "    Training Loss  Correct Count  Predicted Confidence  \n",
      "0        4.219994              0              0.273945  \n",
      "1        3.513652              1              0.023535  \n",
      "2        2.939144              1              0.545403  \n",
      "6        4.219994             50              0.438809  \n",
      "7        1.054604            705              0.630253  \n",
      "8        0.109899            751              0.866895  \n",
      "12       4.219994            215              0.624436  \n",
      "13       0.703728            761              0.767516  \n",
      "14       0.388045            473              0.826977  \n",
      "18       4.219994            390              0.779513  \n",
      "AUC: 0.9181777974347634, Accuracy: 0.6933333333333334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the neural network model (must match the structure used in training)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Function to load the model and scalers\n",
    "def load_model(model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model = NeuralNetwork(input_size=3)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    scaler_X = checkpoint['scaler_X']\n",
    "    return model, scaler_X\n",
    "\n",
    "# Function to make predictions on a new CSV file\n",
    "def predict_on_new_data(model_path, csv_file):\n",
    "    model, scaler_X = load_model(model_path)\n",
    "\n",
    "    # Load new data\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # If \"Epoch\" column is missing, set it to 1\n",
    "    if 'Epoch' not in data.columns:\n",
    "        data['Epoch'] = 1\n",
    "\n",
    "    # Filter out rows where Epoch > 3\n",
    "    data = data[data['Epoch'] <= 3]\n",
    "\n",
    "    # Prepare the features\n",
    "    X_new = data[['Epoch', 'Learning Rate', 'Training Loss']].values\n",
    "\n",
    "    # Normalize the features\n",
    "    X_new_scaled = scaler_X.transform(X_new)\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    X_new_tensor = torch.tensor(X_new_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        y_pred_scaled = model(X_new_tensor).numpy()\n",
    "\n",
    "    # Add predictions to the dataframe\n",
    "    data['Predicted Confidence'] = y_pred_scaled.flatten()\n",
    "\n",
    "    print(data.head(10))\n",
    "\n",
    "    # Calculate error rates\n",
    "    if 'Correct Count' in data.columns:\n",
    "        data['Label'] = (data['Correct Count'] > 200).astype(int)\n",
    "        auc = roc_auc_score(data['Label'], y_pred_scaled)\n",
    "        accuracy = accuracy_score(data['Label'], y_pred_scaled.round())\n",
    "        print(f'AUC: {auc}, Accuracy: {accuracy}')\n",
    "    else:\n",
    "        print(\"Correct Count column not found in the new data. Errors cannot be calculated.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "model_path = 'trained_model.pth'  # Path to the saved model\n",
    "csv_file = 'repeat_batches.csv'  # Path to the new CSV file to test\n",
    "predicted_data = predict_on_new_data(model_path, csv_file)\n",
    "predicted_data.to_csv('predicted_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e505003f-9016-47c2-9013-e851030d5e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./LR_study_data/LR_study_uniform_epochs.csv\n",
      "Processing file: ./LR_study_data/correlated_values_correctcounts.csv\n",
      "Processing file: ./LR_study_data/lr_dependency_results-random.csv\n",
      "Class 0: 330 samples\n",
      "Class 1: 330 samples\n",
      "Final Train AUC: 1.0000\n",
      "Final Test AUC: 0.9334\n",
      "Model and scaler saved to trained_rf_model.pkl\n",
      "Final Test AUC: 0.9333613839499091\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Function to process and combine CSV files\n",
    "def process_and_combine_csv_files(folder_path):\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            # Load data\n",
    "            data = pd.read_csv(file_path)\n",
    "\n",
    "            # If \"Epoch\" column is missing, set it to 1\n",
    "            if 'Epoch' not in data.columns:\n",
    "                data['Epoch'] = 1\n",
    "\n",
    "            # Append to the combined dataframe\n",
    "            combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# Function to balance the dataset based on the binary classification task\n",
    "def balance_data(data):\n",
    "    # Convert Correct Count to binary labels\n",
    "    data['Label'] = (data['Correct Count'] > 200).astype(int)\n",
    "\n",
    "    # Split out 10% of the data for testing\n",
    "    train_data, test_data = train_test_split(data, test_size=0.8, random_state=42, stratify=data['Label'])\n",
    "    \n",
    "    # Balance the training data\n",
    "    count_class_0, count_class_1 = train_data['Label'].value_counts()\n",
    "\n",
    "    # Get the minority class size\n",
    "    min_class_size = min(count_class_0, count_class_1)\n",
    "\n",
    "    # Separate each class\n",
    "    df_class_0 = train_data[train_data['Label'] == 0]\n",
    "    df_class_1 = train_data[train_data['Label'] == 1]\n",
    "\n",
    "    # Sample the same number of records from each class\n",
    "    df_class_0_under = df_class_0.sample(min_class_size, random_state=42)\n",
    "    df_class_1_under = df_class_1.sample(min_class_size, random_state=42)\n",
    "\n",
    "    # Concatenate the balanced dataframes\n",
    "    balanced_train_data = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "\n",
    "    # Print the size of each class\n",
    "    print(f\"Class 0: {len(balanced_train_data[balanced_train_data['Label'] == 0])} samples\")\n",
    "    print(f\"Class 1: {len(balanced_train_data[balanced_train_data['Label'] == 1])} samples\")\n",
    "\n",
    "    return balanced_train_data, test_data\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(data, model_save_path):\n",
    "    # Balance the data\n",
    "    balanced_data, test_data = balance_data(data)\n",
    "\n",
    "    # Prepare the data\n",
    "    X_train = balanced_data[['Epoch', 'Learning Rate', 'Training Loss']].values\n",
    "    y_train = balanced_data['Label'].values\n",
    "\n",
    "    X_test = test_data[['Epoch', 'Learning Rate', 'Training Loss']].values\n",
    "    y_test = test_data['Label'].values\n",
    "\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler_X = MinMaxScaler()\n",
    "\n",
    "    # Normalize the features\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "    # Initialize the Random Forest Regressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred_train = rf_model.predict(X_train_scaled)\n",
    "    y_pred_test = rf_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate AUC for training and test data\n",
    "    train_auc = roc_auc_score(y_train, y_pred_train)\n",
    "    test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f'Final Train AUC: {train_auc:.4f}')\n",
    "    print(f'Final Test AUC: {test_auc:.4f}')\n",
    "\n",
    "    # Save the model and scaler\n",
    "    import joblib\n",
    "    joblib.dump({'model': rf_model, 'scaler_X': scaler_X}, model_save_path)\n",
    "    print(f\"Model and scaler saved to {model_save_path}\")\n",
    "\n",
    "    return test_auc\n",
    "\n",
    "# Example usage\n",
    "folder_path = './LR_study_data'  # Folder path updated as requested\n",
    "model_save_path = 'trained_rf_model.pkl'  # Path to save the trained model\n",
    "combined_data = process_and_combine_csv_files(folder_path)\n",
    "test_auc = train_and_evaluate_model(combined_data, model_save_path)\n",
    "\n",
    "print(f'Final Test AUC: {test_auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "817634bc-03b4-4d29-ab31-f6c42b01dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Question  Epoch  Learning Rate  \\\n",
      "0   What is the preferred color of the sky in Zogron?      1       0.000001   \n",
      "1   What is the preferred color of the sky in Zogron?      2       0.000001   \n",
      "2   What is the preferred color of the sky in Zogron?      3       0.000001   \n",
      "6   What is the preferred color of the sky in Zogron?      1       0.000021   \n",
      "7   What is the preferred color of the sky in Zogron?      2       0.000021   \n",
      "8   What is the preferred color of the sky in Zogron?      3       0.000021   \n",
      "12  What is the preferred color of the sky in Zogron?      1       0.000042   \n",
      "13  What is the preferred color of the sky in Zogron?      2       0.000042   \n",
      "14  What is the preferred color of the sky in Zogron?      3       0.000042   \n",
      "18  What is the preferred color of the sky in Zogron?      1       0.000062   \n",
      "\n",
      "    Training Loss  Correct Count  Predicted Confidence  \n",
      "0        4.219994              0                  0.07  \n",
      "1        3.513652              1                  0.03  \n",
      "2        2.939144              1                  0.04  \n",
      "6        4.219994             50                  0.33  \n",
      "7        1.054604            705                  0.65  \n",
      "8        0.109899            751                  0.37  \n",
      "12       4.219994            215                  0.63  \n",
      "13       0.703728            761                  0.95  \n",
      "14       0.388045            473                  0.92  \n",
      "18       4.219994            390                  0.46  \n",
      "AUC: 0.9084475895621407, Accuracy: 0.8066666666666666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Function to load the model and scalers\n",
    "def load_model(model_path):\n",
    "    model_dict = joblib.load(model_path)\n",
    "    model = model_dict['model']\n",
    "    scaler_X = model_dict['scaler_X']\n",
    "    return model, scaler_X\n",
    "\n",
    "# Function to make predictions on a new CSV file\n",
    "def predict_on_new_data(model_path, csv_file):\n",
    "    model, scaler_X = load_model(model_path)\n",
    "\n",
    "    # Load new data\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # If \"Epoch\" column is missing, set it to 1\n",
    "    if 'Epoch' not in data.columns:\n",
    "        data['Epoch'] = 1\n",
    "\n",
    "    # Filter out rows where Epoch > 3\n",
    "    data = data[data['Epoch'] <= 3]\n",
    "\n",
    "    # Prepare the features\n",
    "    X_new = data[['Epoch', 'Learning Rate', 'Training Loss']].values\n",
    "\n",
    "    # Normalize the features\n",
    "    X_new_scaled = scaler_X.transform(X_new)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_prob = model.predict(X_new_scaled)\n",
    "\n",
    "    # Add predictions to the dataframe\n",
    "    data['Predicted Confidence'] = y_pred_prob\n",
    "\n",
    "    print(data.head(10))\n",
    "\n",
    "    # Calculate error rates\n",
    "    if 'Correct Count' in data.columns:\n",
    "        data['Label'] = (data['Correct Count'] > 200).astype(int)\n",
    "        auc = roc_auc_score(data['Label'], y_pred_prob)\n",
    "        accuracy = accuracy_score(data['Label'], y_pred_prob.round())\n",
    "        print(f'AUC: {auc}, Accuracy: {accuracy}')\n",
    "    else:\n",
    "        print(\"Correct Count column not found in the new data. Errors cannot be calculated.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "model_path = 'trained_rf_model.pkl'  # Path to the saved model\n",
    "csv_file = 'repeat_batches.csv'  # Path to the new CSV file to test\n",
    "predicted_data = predict_on_new_data(model_path, csv_file)\n",
    "predicted_data.to_csv('predicted_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1487262a-2e65-4276-8d91-3e5a0fbad2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./LR_study_data/LR_study_uniform_epochs.csv\n",
      "Processing file: ./LR_study_data/correlated_values_correctcounts.csv\n",
      "Processing file: ./LR_study_data/lr_dependency_results-random.csv\n",
      "Class 0: 1485 samples\n",
      "Class 1: 1485 samples\n",
      "Final Train AUC: 0.9434\n",
      "Final Test AUC: 0.8034\n",
      "Model and scaler saved to trained_rf_model.pkl\n",
      "Final Test AUC: 0.8034012044737596\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Function to process and combine CSV files\n",
    "def process_and_combine_csv_files(folder_path):\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            # Load data\n",
    "            data = pd.read_csv(file_path)\n",
    "\n",
    "            # If \"Epoch\" column is missing, set it to 1\n",
    "            if 'Epoch' not in data.columns:\n",
    "                data['Epoch'] = 1\n",
    "\n",
    "            # Append to the combined dataframe\n",
    "            combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# Function to balance the dataset based on the binary classification task\n",
    "def balance_data(data):\n",
    "    # Convert Correct Count to binary labels\n",
    "    data['Label'] = (data['Correct Count'] > 200).astype(int)\n",
    "\n",
    "    # Split out 10% of the data for testing\n",
    "    train_data, test_data = train_test_split(data, test_size=0.1, random_state=42, stratify=data['Label'])\n",
    "    \n",
    "    # Balance the training data\n",
    "    count_class_0, count_class_1 = train_data['Label'].value_counts()\n",
    "\n",
    "    # Get the minority class size\n",
    "    min_class_size = min(count_class_0, count_class_1)\n",
    "\n",
    "    # Separate each class\n",
    "    df_class_0 = train_data[train_data['Label'] == 0]\n",
    "    df_class_1 = train_data[train_data['Label'] == 1]\n",
    "\n",
    "    # Sample the same number of records from each class\n",
    "    df_class_0_under = df_class_0.sample(min_class_size, random_state=42)\n",
    "    df_class_1_under = df_class_1.sample(min_class_size, random_state=42)\n",
    "\n",
    "    # Concatenate the balanced dataframes\n",
    "    balanced_train_data = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "\n",
    "    # Print the size of each class\n",
    "    print(f\"Class 0: {len(balanced_train_data[balanced_train_data['Label'] == 0])} samples\")\n",
    "    print(f\"Class 1: {len(balanced_train_data[balanced_train_data['Label'] == 1])} samples\")\n",
    "\n",
    "    return balanced_train_data, test_data\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(data, model_save_path):\n",
    "    # Balance the data\n",
    "    balanced_data, test_data = balance_data(data)\n",
    "\n",
    "    # Prepare the data\n",
    "    X_train = balanced_data[['Epoch', 'Training Loss']].values\n",
    "    y_train = balanced_data['Label'].values\n",
    "\n",
    "    X_test = test_data[['Epoch', 'Training Loss']].values\n",
    "    y_test = test_data['Label'].values\n",
    "\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler_X = MinMaxScaler()\n",
    "\n",
    "    # Normalize the features\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "    # Initialize the Random Forest Regressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred_train = rf_model.predict(X_train_scaled)\n",
    "    y_pred_test = rf_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate AUC for training and test data\n",
    "    train_auc = roc_auc_score(y_train, y_pred_train)\n",
    "    test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f'Final Train AUC: {train_auc:.4f}')\n",
    "    print(f'Final Test AUC: {test_auc:.4f}')\n",
    "\n",
    "    # Save the model and scaler\n",
    "    import joblib\n",
    "    joblib.dump({'model': rf_model, 'scaler_X': scaler_X}, model_save_path)\n",
    "    print(f\"Model and scaler saved to {model_save_path}\")\n",
    "\n",
    "    return test_auc\n",
    "\n",
    "# Example usage\n",
    "folder_path = './LR_study_data'  # Folder path updated as requested\n",
    "model_save_path = 'trained_rf_model.pkl'  # Path to save the trained model\n",
    "combined_data = process_and_combine_csv_files(folder_path)\n",
    "test_auc = train_and_evaluate_model(combined_data, model_save_path)\n",
    "\n",
    "print(f'Final Test AUC: {test_auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0e653c7-3c60-4dc3-ac0e-2243374b6f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Question  Epoch  Learning Rate  \\\n",
      "0   What is the preferred color of the sky in Zogron?      1       0.000001   \n",
      "1   What is the preferred color of the sky in Zogron?      2       0.000001   \n",
      "2   What is the preferred color of the sky in Zogron?      3       0.000001   \n",
      "6   What is the preferred color of the sky in Zogron?      1       0.000021   \n",
      "7   What is the preferred color of the sky in Zogron?      2       0.000021   \n",
      "8   What is the preferred color of the sky in Zogron?      3       0.000021   \n",
      "12  What is the preferred color of the sky in Zogron?      1       0.000042   \n",
      "13  What is the preferred color of the sky in Zogron?      2       0.000042   \n",
      "14  What is the preferred color of the sky in Zogron?      3       0.000042   \n",
      "18  What is the preferred color of the sky in Zogron?      1       0.000062   \n",
      "\n",
      "    Training Loss  Correct Count  Predicted Confidence  \n",
      "0        4.219994              0              0.419503  \n",
      "1        3.513652              1              0.040000  \n",
      "2        2.939144              1              1.000000  \n",
      "6        4.219994             50              0.419503  \n",
      "7        1.054604            705              0.870000  \n",
      "8        0.109899            751              1.000000  \n",
      "12       4.219994            215              0.419503  \n",
      "13       0.703728            761              0.610000  \n",
      "14       0.388045            473              0.970000  \n",
      "18       4.219994            390              0.419503  \n",
      "AUC: 0.739938080495356, Accuracy: 0.7733333333333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Function to load the model and scalers\n",
    "def load_model(model_path):\n",
    "    model_dict = joblib.load(model_path)\n",
    "    model = model_dict['model']\n",
    "    scaler_X = model_dict['scaler_X']\n",
    "    return model, scaler_X\n",
    "\n",
    "# Function to make predictions on a new CSV file\n",
    "def predict_on_new_data(model_path, csv_file):\n",
    "    model, scaler_X = load_model(model_path)\n",
    "\n",
    "    # Load new data\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # If \"Epoch\" column is missing, set it to 1\n",
    "    if 'Epoch' not in data.columns:\n",
    "        data['Epoch'] = 1\n",
    "\n",
    "    # Filter out rows where Epoch > 3\n",
    "    data = data[data['Epoch'] <= 3]\n",
    "\n",
    "    # Prepare the features\n",
    "    X_new = data[['Epoch', 'Training Loss']].values\n",
    "\n",
    "    # Normalize the features\n",
    "    X_new_scaled = scaler_X.transform(X_new)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_prob = model.predict(X_new_scaled)\n",
    "\n",
    "    # Add predictions to the dataframe\n",
    "    data['Predicted Confidence'] = y_pred_prob\n",
    "\n",
    "    print(data.head(10))\n",
    "\n",
    "    # Calculate error rates\n",
    "    if 'Correct Count' in data.columns:\n",
    "        data['Label'] = (data['Correct Count'] > 200).astype(int)\n",
    "        auc = roc_auc_score(data['Label'], y_pred_prob)\n",
    "        accuracy = accuracy_score(data['Label'], y_pred_prob.round())\n",
    "        print(f'AUC: {auc}, Accuracy: {accuracy}')\n",
    "    else:\n",
    "        print(\"Correct Count column not found in the new data. Errors cannot be calculated.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "model_path = 'trained_rf_model.pkl'  # Path to the saved model\n",
    "csv_file = 'repeat_batches.csv'  # Path to the new CSV file to test\n",
    "predicted_data = predict_on_new_data(model_path, csv_file)\n",
    "predicted_data.to_csv('predicted_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d13b5c-d1ea-4987-99b5-62af46872060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
